<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2024-11-05-insights</title>
      <link href="/84c61bd9.html"/>
      <url>/84c61bd9.html</url>
      
        <content type="html"><![CDATA[<p>现在刷俩track以后……周二竟然有500篇了？？？</p><h2 id="autoglm-autonomous-foundation-agents-for-guis"><ahref="**https://arxiv.org/pdf/2411.00820**">AutoGLM: AutonomousFoundation Agents for GUIs</a></h2><p>前两天刚出一个Android-Lab，今天唐杰老师又搞了个模型的工作AutoGLM。总体的观感有点像上半年的AutoWebBench工作，延拓了一些安卓场景。claude一出，GUIAgent方向又火起来了呀</p><blockquote><p>这个把前几天那个公众号pr稿翻译成论文了吗……</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/autoglm.png"></p><h2id="vision-language-models-can-self-improve-reasoning-via-reflection"><ahref="https://arxiv.org/pdf/2411.00855">Vision-Language Models CanSelf-Improve Reasoning via Reflection</a></h2><p>刘洋老师的工作，目标是VLM中的self-reflection。这个领域火了一年了，但是好像一直没人能真搞出来的。作者这次提的方案是让模型生成一大堆cot，然后互相比较，去写一个更好的，再把这一大堆测试时计算塌缩成一个dual-cot-trace.发现效果还挺好的</p><blockquote><p>之前kumar有一篇training models toself-correct,感觉和这篇是两个方法论，不知道谁的更合理一些……</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/self-reflection.png" style="zoom:33%;" ></p><h2id="hunyuan-large-an-open-source-moe-model-with-52-billion-activated-parameters-by-tencent"><ahref="https://arxiv.org/pdf/2411.02265">Hunyuan-Large: An Open-SourceMoE Model with 52 Billion Activated Parameters by Tencent</a></h2><h2id="hunyuan3d-1.0-a-unified-framework-for-text-to-3d-and-image-to-3d-generation"><ahref="https://arxiv.org/pdf/2411.02293">Hunyuan3D-1.0: A UnifiedFramework for Text-to-3D and Image-to-3D Generation</a></h2><p>混元MoE的技术报告，激活参数就有52B。讲真的，这真挺大的。作者讲到，这个模型和之前的主要区别是，合成数据的占比更大，大一个数量级。</p><p>另外，除了LLM，还在cvtrack偷跑了一个text-to-3D模型，他是真关心游戏场景的公司。</p><blockquote><p>想起来之前tencent搞过一个personal-hub，绝对的合成数据大队。</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/hunyuan-moe.png" style="zoom:33%;" ></p><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/text23d.png" style="zoom:33%;" ></p><h2id="foundations-and-recent-trends-in-multimodal-mobile-agents-a-survey"><ahref="https://arxiv.org/pdf/2411.02006">Foundations and Recent Trends inMultimodal Mobile Agents: A Survey</a></h2><p>如标题，一篇survey，整理最近对于modelagent领域的各种工作，对比了训练数据等问题。想了解GUIAgent的话，这篇工作还挺好的</p><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/model-agent-survey.png"></p>]]></content>
      
      
      <categories>
          
          <category> Arxiv-Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024-11-04-insights</title>
      <link href="/6b0470e7.html"/>
      <url>/6b0470e7.html</url>
      
        <content type="html"><![CDATA[<h2 id="randomized-autoregressive-visual-generation"><ahref="https://arxiv.org/pdf/2411.00776">Randomized Autoregressive VisualGeneration</a></h2><p>字节的工作，作者发现，在auto-regressive生成过程中，简单地做一下数据增强，把不同的imagepatch打乱顺序，竟然可以提升生成的效果？</p><blockquote><p>why</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/rar.png"></p><h2id="beyond-accuracy-ensuring-correct-predictions-with-correct-rationales"><ahref="https://arxiv.org/pdf/2411.00132">Beyond Accuracy: EnsuringCorrect Predictions With Correct Rationales</a></h2><p>作者发现，已有的工作基本只会把CoT中的结果做评价，而不关注rational的正确性。作者提了一个概念叫做double-correct。即过程和结果都正确。作者构造了一个数据集，可以自动化地给出正确的mutl-hoprational，由此训了个模型。</p><blockquote><p>感觉这个观点和openAI那个"don't supervisethought"有点冲突，不知道哪边更有道理……</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/double-correct.png" style="zoom:33%;" ></p><h2id="right-this-way-can-vlms-guide-us-to-see-more-to-answer-questions"><ahref="https://arxiv.org/pdf/2411.00394"><em>Right</em> this way: CanVLMs Guide Us to See More to Answer Questions?</a></h2><p>一篇挺好玩的小品工作：作者考虑到，在vlm中，模型能不能在信息不够的情况下主动对人的照片提出一些建议，进而更好的获取信息。作者设计了一套pipeline，发现训练的模型确实效果不错</p><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/see-more.png"></p><h2 id="gamegen-x-interactive-open-world-game-video-generation"><ahref="https://arxiv.org/pdf/2411.00769"><strong>GameGen-X: InteractiveOpen-world Game Video Generation</strong></a></h2><p>前两天有个生成Minecraft游戏的初创公司，今天挂出来一篇一样的。作者找了150个游戏的视频，用4o标了caption，训了一个模型可以玩"假游戏"。</p><blockquote><p>感觉GameGen真是个好方向啊，感觉有点变成scaling的比拼了</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-11-04-11-08/gamegen-x.png"></p><h2 id="gpt-for-games-an-updated-scoping-review-2020-2024"><ahref="https://arxiv.org/pdf/2411.00308">GPT for Games: An UpdatedScoping Review (2020-2024)</a></h2><p>一篇综述文章，瞄准了AI+Game，作者调研了已有的GPTgame的工作，发现大致分为以下几种：</p><ol type="1"><li>辅助游戏内容生成</li><li>参与游戏设计</li><li>GPT玩游戏</li><li>GPT模拟用户画像，帮助更好地迭代游戏</li></ol><blockquote><p>感觉还挺好玩的……claude computeruse出来以后，有没有来一波claude玩游戏的</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Arxiv-Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024-11-01-insights</title>
      <link href="/8d2dbba3.html"/>
      <url>/8d2dbba3.html</url>
      
        <content type="html"><![CDATA[<h2 id="tract-making-first-layer-pre-activations-trainable"><ahref="https://arxiv.org/pdf/2410.23970">TrAct: Making First-layerPre-Activations Trainable</a></h2><p>很有NeurIPS风格的工作：作者发现，Vison model中第一层visionlayer的gradient正比于图片本身的像素均值，所以越亮的图片，不管是什么内容，对模型的效果都影响更大。设计了一套方法把这个bias去掉，在第一层添加梯度衰减以后，发现训练速度直接提升了四倍。</p><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/tract.png"></p><h2 id="egomimic-scaling-imitation-learning-via-egocentric-video"><ahref="https://arxiv.org/pdf/2410.24221">EgoMimic: Scaling ImitationLearning via Egocentric Video</a></h2><p>作者软硬件联合开发，搞了一套可以用一套方法论收集机器人和人类的第一人称视角、手姿势的框架，由此就可以让具身和人类有一样的视野和数据建模方式了。由此，作者构建了同时包含人和机器人的sft数据，发现训完以后的机器人效果很不错</p><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/ego.png"></p><h2 id="scaling-concept-with-text-guided-diffusion-models"><ahref="https://arxiv.org/pdf/2410.24151"><strong>Scaling Concept WithText-Guided Diffusion Models</strong></a></h2><p>作者探索了text2image场景中concept的概念。所谓concept，就是说图片里对于某种"元素"的占比，比如一张飞机的图片，加重”plane“的概念，飞机可能会变得更大。如果可以操控模型对于concept的权重，就可以实现很多修图的效果。作者设计了一个pipeline，使得训出来的模型可以按照concept去修改图片</p><blockquote><p>这等于是显示地建模了concept，主要原因是diffusionmodel没有语言理解的能力。能不能让text2imagemodel本身具有语言理解的能力，进而去隐式地学习不同图片的区别呢？</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/concept.png"></p><h2id="androidlab-training-and-systematic-benchmarking-of-android-autonomous-agents"><ahref="https://arxiv.org/pdf/2410.24024">AndroidLab: Training andSystematic Benchmarking of Android Autonomous Agents</a></h2><p>唐杰老师的新工作，出了一个安卓的benchmark和SFT数据，有点类似之前google的androidControl。</p><blockquote><p>这是准备布局手机场景了，之前出了AutoWebGLM布局网页</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/android.png"></p>]]></content>
      
      
      <categories>
          
          <category> Arxiv-Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024-10-31-insights</title>
      <link href="/c9590427.html"/>
      <url>/c9590427.html</url>
      
        <content type="html"><![CDATA[<p>最近几天，NeruIPS接收的工作陆陆续续都放出来了</p><h2 id="os-atlas-a-foundation-action-model-for-generalist-gui-agents"><ahref="https://arxiv.org/pdf/2410.23218">OS-ATLAS: A Foundation ActionModel for Generalist GUI Agents</a></h2><p>之前ICLR的一篇投稿论文，今天挂上arxiv了。有点类似之前那个MultiUI，作者设计了一个爬取引擎，爬了2M图片，13Melement，做了grounding训练。在下游任务上发现，提升了grounding能力以后，GUIAgent能力得到了飞跃。</p><blockquote><p>这两天怎么噌噌出这类论文：昨天一个EDGE，前几天一个MultiUI，在前几天的UGround、AGUVIS</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/os-atlas.png"></p><h2id="refereverything-towards-segmenting-everything-we-can-speak-of-in-videos"><ahref="https://arxiv.org/pdf/2410.23287"><strong>ReferEverything: TowardsSegmenting Everything We Can Speak of in Videos</strong></a></h2><p>作者心很大，想要做video领域的SegmentAnything。作者提到，生成模型的representations已经具有通用的表示能力，能不能在尽可能保留向量本身泛化性的基础上，小小地适配到refer任务呢？作者设计了一套pipeline，做到了这件事情。而且事实证明，模型可以效果非常好地refer到通用世界的各种神奇的、开放域的物体上。</p><blockquote><p>不过有一说一啊，给出文字描述，输出位置信息的。这个任务叫做grounding……refer任务是给出坐标，描述内容</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/refer-everything.png" style="zoom:33%;" ></p><h2id="image2struct-benchmarking-structure-extraction-for-vision-language-models"><ahref="https://arxiv.org/pdf/2410.22456">Image2Struct: BenchmarkingStructure Extraction for Vision-Language Models</a></h2><p>PercyLiang的工作，最近好像挺少见到他了。作者提了个很简单的方法：很多图片，比如网页、latex、乐谱，都是从结构化数据推导出来的。模型能不能从图片反向渲染源码，再通过把源码重新渲染，和原图做reconstruction，来对VLM给出评测呢？</p><blockquote><p>这个思路其实在tableunderstanding领域已经有一些工作了。他们会把table的源码丢给Text-LLM，然后用codeintepreter集成的方式生成答案，再把答案作为信号蒸馏会VLM里。这里其实是把renderengine的规则持续地蒸馏进模型参数里</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/image2struct.png"></p><h2id="slowfast-vgen-slow-fast-learning-for-action-driven-long-video-generation"><ahref="https://arxiv.org/pdf/2410.23277">SLOWFAST-VGEN: SLOW-FASTLEARNING FOR ACTION-DRIVEN LONG VIDEO GENERATION</a></h2><p>一篇视频生成的工作：作者讲了一件事情，在视频生成中，对于action的预测是困难的，能不能用fastslowthinking那套理论来独立地学习对于action的预测，和action发生后世界变化的信息呢？作者构造了200k带有actionlanguage标注的video数据，由此训了模型，发现效果很好</p><blockquote><p>所以video generation又开始找video LM的逆任务了吗？video LM里有一个action recognition，抽取视频中所有的action。videogeneration这边就会有一个"action putting"，把所有action给出来再生成……</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-28-11-01/fastslow-vgen.png"></p>]]></content>
      
      
      <categories>
          
          <category> Arxiv-Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Manyshot-ICL: 在context中重现传统AI的可能性</title>
      <link href="/f992cb6b.html"/>
      <url>/f992cb6b.html</url>
      
        <content type="html"><![CDATA[<p>今天来讲讲<a href="https://arxiv.org/abs/2404.11018">Many-ShotIn-Context Learning</a>，大概是deepmind一个月前的文章，读下来和之前JasonWei那篇"Large Models do In-Context LearningDifferently"的阅读体验有点像，是一篇"暗合scaling天意"的文章。</p><p>看完了我把他和另外两篇论文联系了起来，想到了未来LLM在context重建AI的可能性。最后，推荐大家读一下原文，deepmind论文就像乐高，阅读(拼搭)体验一直很好……</p><p>参考资料：</p><blockquote><p>Many-Shot In-Context Learning</p><p>Many-Shot In-Context Learning in Multimodal Foundation Models</p><p>In-Context Reinforcement Learning with Algorithm Distillation</p></blockquote><span id="more"></span><p>作者团队来自Google Deepmind</p><p><img src="../../files/images/manyshot_icl/authors.png"></p><h2 id="introduction">introduction</h2><p>这篇论文方法上没什么好说的，大家都知道in-context learning:把一个任务的很多input-outputpairs放在prompt里，然后模型就可以在不更新自身参数的情况下，"现场学会"一个任务，并对最后给出的input预测出来结果。</p><p>从high-level的角度讲，我觉得这个能力是因为模型学会了所谓的”worldmodel“。传统的AI领域，大家一般会建模出来一个任务，作为输入空间到输出空间的映射(比如情感分类)，<spanclass="math inline">\(f: \mathcal{X} \rightarrow\mathcal{Y}\)</span>，接下来考虑如何训练一个模型<spanclass="math inline">\(f_\theta\)</span>可以做好一个任务。对于LLM来说，从instructiontuning开始，大家开始认为整个世界就是一个<spanclass="math inline">\(\mathcal{X}\)</span>，所有的所谓任务都只是从<spanclass="math inline">\(\mathcal{X}\)</span>​里面的一个采样，因此只需要学会一个<spanclass="math inline">\(f_\theta(\mathcal{X})\)</span>就可以表征所有的任务，in-contextlearning正是从这个情况下涌现出来的能力。</p><p>作者考虑了一个现实的问题：之前的in-contextlearning评测几乎都是3-shot, 5-shot,8-shot。但是今天的LLM已经可以把自己的context拓展到128k，甚至10M(gemini)。那么，有人试过用更多的样本放在prompt里，效果会更好吗？作者把这个setting就叫做manyshot场景</p><p><img src="../../files/images/manyshot_icl/perf.png"></p><p>作者测试了gemini在不同场景下的manyshot表现，发现几乎都比few-shot场景效果好很多。</p><p>为了解释这个看起来有点神奇的现象，作者又定义了两个阴性对照和阳性对照的setting：</p><ul><li>reinforcedICL：先自己生成一堆input-output，然后根据output正确性筛选出好的样本作为shot</li><li>unsupervised ICL：模型生成一堆input，不拼output，看看能不能提升</li></ul><h2 id="performance">Performance</h2><p><img src="../../files/images/manyshot_icl/trans.png"></p><p><img src="../../files/images/manyshot_icl/sum.png"></p><p><img src="../../files/images/manyshot_icl/verify.png"></p><p>作者在各种场景下尝试了many-shot，然后报告了效果随着shot增加的变化情况，可以看到，几乎在所有场景下，提升shot的数量都会让效果变得更好。</p><p>作者为了进一步探索这个现象，尝试了上面提到的reinforcedICL和unsupervised ICL</p><p><img src="../../files/images/manyshot_icl/math.png"></p><p>并且发现比起用groundtruth样本作为ICL样本，模型自己生成的样本甚至效果要更好。而且，这种样本是可以迁移到别的任务上的，右边的图是用MATH数据集生成的样本来作为GSM8K的manyshot样本。</p><p>为什么unsupervisedICL效果很好，难道只需要看到一些query？作者类似于之前那篇weak-to-strong的思路，给了一个基于直觉的解释：如果模型本来就会做目标任务，可能只需要用一些query帮助他”联想“到预训练数据中的一些知识作为锚点，来让他在做现在的input时发散更多的知识。</p><p>从这个思路出发，对于数学这样的场景，预训练见过很多了，可能非常需要这种”联想“。</p><p>还有一个最有意思的实验设计，和大家分享一下啊：作者想要证明，manyshot的效果来源于去掉了预训练数据中的bias。如果大家想证明这个结论，该如何设计实验？</p><p>作者类似于之前那个"large model performs ICL differently",找到了一个情感分类任务，设计了对照组：</p><ul><li>flip：把标签反过来，即positive变成negative，negative变成positive。这个和预训练知识相反，模型不得不在context中学习</li><li>abstract: 把所有的标签变成A、B、C这种没有语义的东西</li></ul><p>通过这两个对照，作者就能勘测出预训练bias对效果的影响，作者发现：最开始，两个对照组的准确率都不太行，但随着shot增加，三种方法的效果最终收敛到了同一水平。这说明：manyshot场景可以逐步削减模型对于预训练和下游任务的理解偏差，进而提升任务的效果。</p><p><img src="../../files/images/manyshot_icl/bias.png"></p><p>最后，作者报告了一个解释不了的现象：随着shot增加，作者看了groundtruth的ppl，发现越来越低。但是，如果统计acc的话，实际上250-shot场景的acc是不如125的。在predict-scaling这个领域，大家往往喜欢用更弱的模型预测更强模型的效果。从scaling曲线上讲，随着几乎<spanclass="math inline">\(ppl \propto \log(N-shot)\)</span>，预测ppl似乎是可行的。然而，更低的ppl却不能带来更高的得分，这和传统benchmark场景的结论相反。</p><blockquote><p>为什么会这样？我想起来之前在读<ahref="https://arxiv.org/abs/2205.14334">Teaching models to express theiruncertainty inwords</a>中作者提到了ppl和1)模型对答案的信心值2)模型表达这个解答过程的信心值都有关。我们可以思考一下many-shot场景，当前面拼了非常多的样本时，模型对于1)和2)的信心值会倾向于更高还是更低呢？同样的，如果模型对于任何答案的信心值都变得更高了，那么可能就更难以区分出好的答案和坏的答案了</p></blockquote><p><img src="../../files/images/manyshot_icl/nll.png"></p><p>对于上面的问题，作者在附录中还给出模型对于正样本和负样本的NLL。可以观察到，总体而言，似乎样本越多，模型越没法使用NLL区分正/负样本</p><p><img src="../../files/images/manyshot_icl/contrastive.png"></p><h2 id="几个问题和我的思考">几个问题和我的思考</h2><p>看完这篇论文确实收益良多，不过我似乎产生了更多的问题，不知道大家有没有类似的感受。</p><h3 id="最像样本相似度">最像样本相似度</h3><p>首先，我有另一个视角去理解这个现象：我们如果统计manyshot样本中和当前queryembedding最像的top1 similarity。然后画个散点图，其中横坐标是top1similarity，纵坐标是正确与否，然后给每个横坐标区间统计平均正确率，变成柱状图。即样本越像，acc越高。而且，我感觉对于所有shot的场景下，这个柱状图可能会遵循同一个分布……这符合大家对预训练模型的认知："LLMcan't perform zeroshot, performance depends on shots in training data"对偶的说法就是"LLM’s manyshot performance depends on similarity of thein-contextexamples"。不过这个视角解释不了另外一个现象，就是few-shot场景下模型表现对于few-shot样本顺序的敏感性</p><h3 id="和finetune的关系">和finetune的关系</h3><p>再有，如果我们有1024个样本，传统的视角下，我们肯定会想要finetune，但这篇论文可能是只能访问API，没有对比finetune的baseline……如果我们相信了前文对于”概念和联想“的直觉解释，那么finetuneGemini对于Gemini到底意味着什么？</p><blockquote><p>传统AI中，会任务finetune是在学习知识，或者说找到更泛化的概念。但是，如果LLM本身就具有泛化的概念，是否finetune这个过程只是帮助模型建立几个"思维的锚"呢？</p></blockquote><p>这让我想起来曾经在delta tuning看到有个领域叫”Intrinsicdimension“，他们试图找到模型最少可以在更新多少个参数的情况下，达到全参数微调的效果的90%，并把这个数字叫做LLM的Intrinsicdimension(ID)，他们发现基础能力却强，ID就越小。这是不是从某种程度上支撑了上面的猜想：越强的模型，本身的retrieve能力越强，就越不再需要很多所谓的"联想锚点"，可以直接从当前instruction里做联想。</p><p>另一个领域是之前的weak-to-stronggeneralization，他们使用带噪声的数据集训练GPT4，发现GPT4自己把数据的噪声恢复了，在测试集上效果还是很好。这是不是也说明，可能answer并不重要，关键是激发一下模型对于问题的理解。也就是说，如果在weak-to-strong场景下，只在query上加loss，会不会都不用answer模型就学会了？</p><blockquote><p>甚至可以开展一个实验，比如我有5000条数据。其中100条我在answer加loss，剩下4900条我在query加loss，这样训出来的模型和和正常训练的模型，效果上也会有个PGR(performancegap recovered)吗？</p></blockquote><h3id="在context中重建传统ai甚至世界知识">在context中重建传统AI、甚至世界知识</h3><p>最后我想说的是，这篇工作让我发现，随着context的增加，大家其实可以在context中重建传统AI算法。这篇论文在context中添加非常多的样本，叫做manyshot，其实和传统RL中的finetune有点像。我们抽象的看，就是Gemini在接受到这些样本以后，自己抽象出来了一个"完成这个任务的模型"。</p><p>Gemini-1.5report里其实就做了类似的东西：他们找到了一门叫做<strong>Kalamang</strong>的罕见语言，网络上肯定找不到资料。让模型看完这个语言的字典，发现Gemini就可以翻译kalamang了。所以，如果模型有很强的智力，他有能力通过context去理解世界。</p><blockquote><p>这是一个很恐怖的事情：试想，如果某一天来了三体人，他们的三体模型甚至不需要微调，只是(和人一样、纯前向地)看了一下人类世界的一些语料，就可以像人一样完成人可以完成的所有事情。</p></blockquote><p>在学界，其实有一些类似的工作：</p><ul><li><p>RL：deepmind曾经做了一篇叫"in-contextRL"的工作，他们会把ppo、dpo等算法训练的过程记录下来，丢到一个context里，让一个长模型在这个上面做建模，看能不能进行”algorithmdistillation“，学到比原来的算法更高效的"rl算法"。不过这篇论文是训模型的工作，如果我们context很长，我能不能把"一次rl训练过程”作为1-shot，然后进行manyshot，模型就可以learnto learn rl in-context了呢？</p></li><li><p>Align: 进一步地，之前yuchen lin有一篇工作叫做<ahref="https://arxiv.org/pdf/2312.01552">The unlocking spell on basellms</a>，发现其实只需要找到一些in-context样本，就可以让模型在很多case上"看起来像是做过rlhf"。如果模型的context够长，我用manyshot的形式把alpaca-52k丢进去，模型就能做好rlhf吗？</p><blockquote><p>从这个思路继续思考：所谓的"人类偏好"到底激发出了模型的哪些"思维锚"？</p></blockquote></li></ul><p>未来，随着LLM的context越来越长，可能我们现在的所有传统算法(比如推荐)都可以被模型通过in-context的方式去解决。可能rag也不复存在了……"long-contextLLM is many-shot learner, and zero-shot world model"</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</title>
      <link href="/fbc665c3.html"/>
      <url>/fbc665c3.html</url>
      
        <content type="html"><![CDATA[<p>最近Apple出了自己的30B多模态大模型，涌现出了多模态的in-contextlearning效果，论文里一句"evenbetter"让我想到库克那个嗓音……作者说明了很多在训练中收获到的经验教训，这是我最近几个月看的写法最清楚的一篇论文。正好借此讲讲多模态大模型：目前学界大火的VLM，到底是怎么跑的？</p><span id="more"></span><h2 id="introduction">Introduction</h2><p><img src="../../files/images/mm1/authors.png"></p><p>作者团队来自苹果，看来苹果说的"今年wwdc上大模型"真有希望了？</p><p>在pretrainmodels领域，大家一直都想把之前的所有任务的数据整合到一起，把之前一堆独立模型做的事情统一到一个模型里面,由此诞生了LLM技术。最近大家关注的重点开始转向多模态：除了文本领域的任务，能不能同时把和图片理解相关的任务也统一进去?具体来说，如果输入同时含有图片文本，多个图片文本交错排列，模型能不能在理解图片和文本的基础上，输出文本。这就是MLLM(multimodalLLM)或者VLM(Vision-Language Model)</p><p>虽然学界、业界在这方面的探索不少，但已有的工作基本都不够开放：要么就是闭源模型给个API，要么就是开源模型只给个权重。即使有些工作会开源数据，但是没有人会开放讨论他们对于模型结构的选择、对于训练数据的选择、对于训练超参数的选择以及背后的原因。</p><p>作者在本篇工作中开放的讨论了所有的细节，结论简单来说就是以下几点：</p><ol type="1"><li>在模型结构方面，下面几个要素的重要性递减：vision-encoder的分辨率、vision-encoder大小。VL-connector选择对于最终的表现几乎没有影响</li><li>在数据方面，主要有以下几种数据：text-only数据、image-caption数据、互联网文本-图片交错数据，作者发现：<ol type="1"><li>互联网文本图片交错数据和text-only数据对于in-contextLearning、few-shot Learning至关重要</li><li>image-caption数据对于zero-shot Performance至关重要</li></ol></li></ol><p>最后，从一系列insight出发，作者scaling了训练过程，训练了3B、7B、30B的VLM，甚至在3B和7B规模下尝试了top2的MoE架构，并在各个层面上达到了SOTA的效果</p><blockquote><p>除了没开源weight，基本全给你了……</p></blockquote><h2 id="vlm长啥样">VLM长啥样？</h2><p><img src="../../files/images/mm1/design-choice.png"></p><p>目前的VLM基本都是分三个部分，如上图的左图：</p><ol type="1"><li>image-encoder，负责把图片编码成为embedding，同时尽可能不要损失信息</li><li>VL-connector，负责把image-encoder的输出做一些转换，比如MLP，对齐到LLM的wordembedding空间</li><li>decoder-only LLM：把上一步的输出直接作为多个Token的Wordembedding输入，然后跑后面的LLMnext-token-prediction。会在text输出的部分计算loss</li></ol><h3 id="image-encoder">Image-encoder</h3><p>image-encoder一般也是transformer，是一个ViT。他首先会把图片按照从左到右、从上到下切成不同的小正方形，打成多个patch，每个patch的分辨率都是比如14x14。接下来每个patch会使用一个卷积层编码成为一个vector，然后多个patchvector拼在一起当成多个“token”的word embedding，加上positionembedding后直接进入transformer encoder，输出会是每个patch一个hiddenstate。</p><p>这个encoder有两种训练方法：</p><ol type="1"><li>contrastive loss:大名鼎鼎的CLIP。找到一大堆互联网上的挨着的(图片-文本)对作为正样本，然后随机其他的对作为负样本，然后用刚才的ViT作为image-encoder，用另一个比如T5之类的作为textencoder，拿到两个embedding，跑对比学习的loss：希望正样本-正样本之间的cosine相似度大于负样本-正样本之间的cosine相似度</li><li>reconstructiveloss：这个就是传说中的VAE。用ViT编码完了以后会把图片变成一个embedding，为了保证这个embedding尽量信息无损，会后面再接上一个image-decoder去预测原来的图片长什么样子。预测的越准loss就越小。为了防止模型去memorize每个image的样子，还会有个辅助KLloss去保证所有image的embedding在embedding空间的分布尽可能均匀</li></ol><p>这两种方法有个特点：都是无监督的，只要有一大堆数据就能起效果。前者需要(图片-文本)对，但有个好处是数据可以自己合成，可以人工在text-caption上加上各种丰富准确的细节来提高训练的要求，进而增强模型。后者只需要一大堆图片，在一些dense的任务上表现更好</p><p><img src="../../files/images/mm1/image-encoder.png"></p><p>作者在这里做了消融实验，最左边那列AIM是reconstructionloss，CLIP是对比学习loss。architecture基本都是ViT，H的参数量比L大。imageRes代表的是训练的时候的训练数据的图片分辨率都是多少。Data是指用什么数据做的训练。</p><p>明显可以发现：</p><ol type="1"><li>image res这个变量对最终的效果差距最大</li><li>在CLIP中加入合成数据(VeCap)，会提升模型表现</li><li>两种loss基本都不咋影响效果，都差不多</li></ol><p>作者最终选了CLIP + 高清encoder</p><h3 id="vision-language-connector">Vision-Language Connector</h3><p>这一部分就是输入image-encoder的output，转换到Wordembedding空间，这里面有个至关重要的问题：一张图片应该转换到LLM里的多少个token？</p><ol type="1"><li>显然token更多，保留的细节更多，模型的效果理应更好</li><li>然而，当token多的时候，尤其如果是多个图片(8个、16个)，很多个token其实训不起来，资源消耗太恐怖了</li></ol><p>作者在这里消融实验了一个image对应64、144token两种情况，然后尝试了224、336分辨率的image-encoder，以及不同的pooling策略，大致有这么几种实现：</p><ol type="1"><li>Average Pooling:由于Vit的token数大于目标token数量，就把相邻几个patchhidden-state取平均数，变成一个embedding，让数量变得一样了。再给每个token过一个MLP，作为LLM的一个tokenembedding</li><li>AttentionPooling：作者觉得vit的输出可能和llm的word-embedding不在一个子空间，需要一个Attention层变换一下。于是就加一个额外的Attention层，初始化k个query向量，然后用key、value变换阵把ViT的输出对齐过去(Attention输出在sequence-length维度上和query的长度一致)。这样Attention的输出就是k个tokenembedding，然后k可以取64、144，就对齐过去了</li><li>C-abstractor:把输出用某种卷积层操作(ResNet)转换到word-embedding空间</li></ol><p><img src="../../files/images/mm1/connector.png"></p><p>作者对比了一圈发现：vision token和imageresolution最重要，几种不同的pooling策略几乎没区别。即使在一个testset上好，在另一个上可能会更差</p><h3 id="pretrain-data">Pretrain-data</h3><p><img src="../../files/images/mm1/data.png"></p><p>最后，作者探索了pretrain数据对于结果的影响。这里有个歧义："pretrain"并不是真正的预训练。作者这里实际上会选取一个预训练好的LLM作为text-onlydecoder，然后选取一个在CLIP对比学习loss上预训练好的image-encoder。把他俩用一个新初始化的VL-connector拼在一起。把这个叫做"starttraining"。所以这里探讨的“pretraindata”单指VLM启动训练后的"pretrain"</p><p>VLM的pretrain-data基本分为三种：</p><ol type="1"><li>image-caption:正常VLM的预训练数据，也就是刚才CLIP里的图片文本对。作者会输入图片，让模型预测caption</li><li>interleaved image-text:从互联网上爬下来的语料。这里面都是图文交错的数据，作者直接把文本提取出来，在正常图片的位置放上图片，让模型预测所有文本的next-token-prediction</li><li>text-only：正常LLM预训练的数据，基本上是去掉图片后的互联网语料，以及github之类地方爬下来的各种代码，以及找到的一大堆各个学科的教材啥的</li></ol><p>其中，image-caption数据分为正常版和合成版本。所谓合成数据，就是用GPT-4v或者其他什么开源模型(这里说的VeCap用的vicuna，是一个Llama在gpt-3.5的输出上做过SFT的版本)，要求他给图片生成一些非常详细的caption，包括里面所有Object以及他们之间的位置空间语义承接关系之类的。</p><p>如果大家看过前几天写的DALL.E 3的笔记<a href="/61495969.html" title="从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和&quot;Synthetic Data&quot;是一个意思吗？">dall.e 3阅读笔记</a>，就知道正常的html里来的图片文本对的alt-text质量有多差，所以需要重新clean，去重写等等</p><p><img src="../../files/images/mm1/data-choice.png"></p><p>作者在这一部分做了最多的实验，因为在LLM领域现在大家也普遍认为数据是影响结果最重要的因素，大致发现了如下结论：</p><ol type="1"><li>interleaved data is instrumental for few-shot and text- onlyperformance, while captioning data lifts zero-shotperformance。作者猜测是因为互联网交错数据天生具有一些in-contextLearning的性质</li><li>text-only data helps with few-shot and text-only performance</li><li>Careful mixture of image and text data can yield op- timalmultimodal performance and retain strong text performance.</li><li>Synthetic data helps with few-shot learning.</li></ol><p>最后，作者公布了花了很多钱试出来的配方： caption / interleaved /text-only = 5:5:1</p><h2 id="mm1">MM1</h2><h3 id="pretrain">pretrain</h3><p>由上面的结论，作者把训练做了scaling，选择了如下方案：</p><ol type="1"><li>CLIP loss的ViT-H，378x378分辨率</li><li>144 token，c-abstractor模式的VL-connector</li><li>5:5:1的VLM pretrain-data</li></ol><p>作者在 9M, 85M, 302M, 1.2B几个规模上做了超参数搜索，并认为学习率<spanclass="math inline">\(\eta\)</span>应该和模型规模的对数成反比，由此预测了30B模型的最优Learningrate=2.2e-5。然后weight-decay <spanclass="math inline">\(\lambda\)</span>是lr的1/10 <spanclass="math display">\[\eta = \exp(-0.4214\ln(N) - 0.5535)\\\lambda = 0.1\eta\]</span> 另一面，作者还尝试了MoE架构，用了Mixtral-8x7B那种经典的top2router，在3B和7BLLM规模上做了实验。3B是每两层加一个64选2的FFN，7B是每四层有一个32选2的FFN。这些FFN都是从最开始的denseFFN初始化的，随着router的训练逐渐变得差异化了。为了稳定训练，作者还加了一些平衡routing的loss</p><blockquote><p>这方面，似乎Mixtral 8x7Breport说他们没加任何其他loss，也可以正常训练？不知道是不是VLM领域有新的bug</p></blockquote><p><img src="../../files/images/mm1/eval.png"></p><p>作者发现，这样训练出来的MM1系列模型，在所有规模上基本都是目前最好的VLM。然后MoE可以让Performance"even better"</p><h3 id="sft">SFT</h3><p>目前的公认解决方案，都是在Pretrain完以后，把能找到的学术任务拿过来造一个大号的数据集，让模型做一下supervisedfinetuning，再找一大堆chat数据让模型学着遵循人类乱七八糟的指令（比如"只能回复emoji"），最后再在测试集上测试效果</p><p>类似的，作者的SFT数据构成三部分：</p><ol type="1"><li>学术数据集转换来的数据</li><li>GPT-4v生成的给予图片的qa chat数据</li><li>text-only，训练正常LLM时使用的SFT数据</li></ol><p>所有数据掺在一起，SFT阶段随机喂数据。他们尝试了更高级的策略，发现效果基本没提升，就用了这个最简单的招</p><h3 id="high-resolution">high resolution</h3><p>测试的时候，还有问题：1)测试数据很多有巨多图片输入，或者需要8-shotin-context样本，这就需要最少8个图片。2)很多图片清晰度巨高，比如3840x2160分辨率。</p><p>这方面，有一个解决方案：Sub-image decomposition,比如对于1344×1344的图片，作者分成5张图丢给模型，每个都是672×672。第一张图是降采样的，后面四张分别是左上角到右下角的局部</p><blockquote><p>这里有个trick：这里的复杂度不是<spanclass="math inline">\(n^2\)</span>,而是<spanclass="math inline">\(n^4\)</span>。一个大图片会变成<spanclass="math inline">\(n^2\)</span>个token，后面LLM本身又以<spanclass="math inline">\(n^2\)</span>的复杂度跑前向，所以这个算法把一个大图变成5个小图实际上是省的。</p></blockquote><p><img src="../../files/images/mm1/sft-performance.png"></p><p>作者通过这个方法可以支持任意分辨率的图片，实际上最高到1792x1792还是会崩。然后作者对比了SFT阶段的效果，发现了几个核心：</p><ol type="1"><li>image分辨率对效果至关重要，如果输入的分辨率本身不高，怎么训练效果都不行</li><li>VLM的预训练对效果很重要。作者把预训练各个阶段的ckpt都拿过来做了一次SFT实验，发现VLM预训练越久，对应的SFTPerformance也越好</li></ol><p>另外，作者发现即使SFT数据全都是单张图片输入的，MM1还是具有多张图片输入时的in-context推理能力。并且，在Pretrain阶段探索到的insight，对于SFT阶段的Performance仍然成立。最后，作者在附录里展示了很多MM1的case，感觉这个模型的效果是真的很不错</p><blockquote><p>要是开源就好了</p></blockquote><h2 id="我的思考">我的思考</h2><ol type="1"><li>我印象中MM1是第一个正式描述自己的多张图片、图文交错情况下的in-contextLearning能力的模型。我感觉这个能力对于多模态的Agent很重要，目前我看到的一堆multi-modal的ReAct算法，都是把history用文本的形式拼到prompt了……感觉不是真正的multi-modalagent</li><li>另外最近有一些别的模型，比如Fuyu是一个decoder-only架构，图片的patchembedding不用走vit，直接在一个大decoder里走一次。代价是要重新预训练，好处是跑的很快……不知道哪种才是最好的方案，我猜目前这些方案是因为大家可以直接拿一个llama过来当LLM，效果比较稳？</li></ol><p><img src="../../files/images/mm1/fuyu.png"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 多模态 </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和&quot;Synthetic Data&quot;是一个意思吗？</title>
      <link href="/61495969.html"/>
      <url>/61495969.html</url>
      
        <content type="html"><![CDATA[<p>最近Sora巨火，仿佛开启了AIGC的新时代。Jason Wei表示："Sora is theGPT-2 moment" for videogeneration。我在sora发布的大约第5个小时读了technicalreport，里面最打动我的其实是没提什么细节的recaption技术。让我回想想起了之前读DALL.E3论文时的愉快体验。</p><p>所以今天来分享一下DALL.E3论文里的recaption细节，并讨论几个问题和我的看法：1)OpenAI教你为什么要"先查看原始数据，再做创新"2)Recaption和大家一直在聊的"training on synthetic data"是一回事吗?3)recaption技术是否已经在(或者即将在)被其他领域使用？</p><p>另外，我总结了一下上篇笔记阅读量大的关键：语言表达要浅显易懂些，所以这篇笔记我可以声明一下：<strong>没学过AI也能看懂</strong>(我在博客里加了这个标签"fromscratch"，所有我认为不懂AI或者只知道一点点的人也能看懂的博客都会加上这个标签)</p><p>参考文献：</p><blockquote><p>https://openai.com/sora</p><p>Improving Image Generation with Better Captions</p><p>Automatic Instruction Optimization for Open-source LLM InstructionTuning</p><p>WaveCoder: Widespread And Versatile Enhanced Instruction Tuning withRefined Data Generation</p><p>Reformatted Alignment</p><p>Rephrasing the Web: A Recipe for Compute and Data-Efficient LanguageModeling</p></blockquote><span id="more"></span><h2 id="dall.e-3">DALL.E 3</h2><p>论文的标题明确指出了关键点"BetterCaptions"，说白了就是教你(叫你)去清洗数据。我们也许可以从这篇论文里，大致窥探到OpenAI世界第一的数据工程insights。</p><p><img src="../../files/images/recaption/authors.png"></p><p>作者指出，在DALL.E2以后，text2image得到了学界越来越多的关注，大家想要开发更好的模型结构、使用更大的参数量和训练量。另外，学界很多工作帮助指出DALL.E2中存在的问题：忽略text中的要求、和text中的语义不符，图片里出现诡异的文字等等。</p><p>怎么办呢？继续扩大模型规模，可以在一定程度上缓解这个问题。不过，作者在查看了原始数据后发现了根源：巨量的互联网图片-文本对数据里的图片和文本在大多数情况下并不对应，比如下图，虽然图片相对高质量，但是对应的alttext实际上都和图片没什么关系。实际上，上面提到的text2imagemodel中的问题，其根源在于数据集的质量。</p><h3 id="re-caption">re-caption</h3><p><img src="../../files/images/recaption/caption.png"></p><p>所以，作者有了直觉的想法：</p><ol type="1"><li>先训练一个caption model，可以输入图片数据输出高质量文字描述</li><li>把他们的整个数据集的所有文字描述全部重跑一遍，所以这个过程叫做(re-caption)</li><li>在新生成的数据集上训练text2image model。保证训练数据是高质量的</li></ol><p>这个思路没什么难的，实际上学界也早有了相关的思路，OpenAI只是把这个方法扩展了起来。作者分别标注了short-description和detailed-description两个小数据集，端到端的训练了一个captionmodel，然后把每条预训练数据集里面的图片都生成了SSC和DSC两种。</p><p>然而，作者敏锐的察觉到了上面这种方法里存在的问题：人类世界的caption虽然质量不一定高，但足够泛化。机器生成的caption虽然质量高，但他的多样性受制于captionmodel训练数据的多样性。比如说，如果caption model永远输出"axxx"开头的caption，那么如果用户运行时的输入不是a开头的，模型是不是就会爆炸了？所以作者希望数据集中的caption要尽可能接近人类生成的text</p><p>这个问题其实也好解决：把原始数据和recaption数据掺在一起训练！作者由此开展了一系列实验。另外，作者尝试了使用GPT-4V作为imagecaption，效果实际上也很不错</p><h3 id="blending-synthetic-and-ground-truth-captions">Blending syntheticand ground-truth captions</h3><p><img src="../../files/images/recaption/blend.png" style="zoom:33%;" ></p><p>作者经过一系列实验，发现基本上使用更多的syntheticdata，在in-domain的测试里效果会更好。作者由此训练了DALL.E3发现效果比DALL.E 2有了明显的提升。不过作者也同时指出了，DALL.E3仍然有很多问题，并且这些问题本质上是image captionmodel暂时学习不到的性质</p><ol type="1"><li>get不到caption里面的空间信息，比如谁在谁的前面/上面等等，作者发现captionmodel也往往说不对这些关系</li><li>图片里面的文字会丢失字母等等：作者认为这是textencoder是基于t5的。他的encoder是基于token的，模型需要学会把tokenizer里面的token(含多个char)映射到图片空间，这实际上非常难(比如说"图片要求写一个"play"，每个字母用不同的颜色，每个字母分别倾斜30度"。这个描述本身被tokenizer时"play"会是一个token，)。以后也许会训练基于char的model来解决</li><li>专业知识性的caption说不对，比如各种罕见的鸟的类型生成不对：作者发现这是因为captionmodel也说不出来，因为这需要更多、更高级的世界知识理解能力。作者认为需要更强的captionmodel(比如GPT-4V)</li></ol><h2 id="dall.e-3还有后手">DALL.E 3还有后手？</h2><p>通过上面的讲解，我们应该发现：DALL.E3的训练数据的文字部分，绝大多数(95%)都是recaption出来的。这对模型的影响有多大呢？</p><p><img src="../../files/images/recaption/image-with-better-caption.png"></p><p>作者列出来了dalle.3使用正常caption和详细caption下的实际表现，确实在promptfollowing能力上天差地别，使用更符合DALL.E3训练数据格式的prompt会让他的表现好很多。实际上，很多text2image目标用户过去一年里的很多学习，或者说网上找的教程都是在学习"如何写好的prompt"。这和"模型更好的遵从prompt"是一个双向奔赴的过程，但是只有OpenAI自己知道他们的闭源的recaption训练数据到底长什么样：我们其实很难针对性的写出符合DALL.E3预期的prompt。这也就是上面论文里提到的问题：真实人类需求和训练prompt的分布不一致，会导致模型部署时崩溃风险高。</p><p>怎么办？OpenAI帮我们想了一招：写prompt的也是OpenAI-model就好了！所以我们会发现，现在使用DALL.E3都是基于网页端的，我的需求会被GPT"re-caption"成真实的需求。比如这张图片：</p><ol type="1"><li><p>我写的要求：帮我生成一张图片，描述weak2stronggeneration里GPT2监督GPT4</p></li><li><p>GPT4实际生成的prompt：Imagine a futuristic, abstract scene whereGPT-2, represented as a wise, older robot with classic design elements,is mentoring GPT-4, depicted as a sleek, advanced robot withcutting-edge features. The setting is inside a vast, digital library,filled with glowing books and holographic data streams. GPT-2 is shownpointing towards a holographic display that illustrates complexalgorithms and data structures, while GPT-4 observes attentively, itssensors and circuits illuminated by the holographic light. Theatmosphere is one of collaboration and knowledge transfer, highlightingthe evolution of technology from one generation to the next.</p></li></ol><p><img src="../../files/images/recaption/dalle3-image.webp" style="zoom:33%;" ></p><p>通过这种方案，就不怕分布不一致了。OpenAI的这个设计实际上让我产生了两个联想：</p><ol type="1"><li>对于用户来说，prompt是什么本身比并不重要。prompt只是链接需求和成品的桥梁，用户关心的是自己想要的图片完成没有而不是prompt好不好。</li><li>ChatGPT目前做的事情更像是链接了 用户需求 -&gt;text-prompt。这本身不是最直观的方法，因为GPT和DALLE本身是没有交互的</li><li>我们可以把DALL.E 3部署时产生的数据定义为三元组 (多轮对话,text-prompt,target-image)。如果这个数据量scale起来，数据量达到二元组数据规模以后，是不是可以直接训练一个端到端的模型呢？他可以同时生成文字和图片，理解文字和图片。直接和你的对话去理解你的那种虽然抽象、但乐于通过对话表达出来的需求(比如要多少号多大的猫)，甚至通过多轮的图片生成去一点点猜测你的真实意图。</li></ol><blockquote><p>在DALL.E3论文里用GPT4生成caption的实验里展示了这种野心的一角，这可能才是text2image下沉市场(直接使用图像的用户、而不是基于图片去二次创作的画家们)的更广阔的未来</p></blockquote><h2 id="sora的recaption又玩出了什么">Sora的recaption又玩出了什么？</h2><p>Sora在技术报告里提到使用re-caption技术为视频创造文字描述。实际上这个领域和text2image完全不同：</p><ol type="1"><li>互联网能找到大量的图片、文本对，但很难找到大规模的 视频-文本对</li><li>图片和文本基本上是一一对应关系：有一个很精确详细的文本描述以后，其对应的图像基本也就只能长那个样子，对图片进行裁剪会导致清晰度丢失严重，并且丧失语义丰富性。但视频是可以"降采样的"，一个视频的任何片段还是视频，大概率还会有语义，可以做re-caption。</li><li>除了降采样，实际上还可以拼接。比如Sora里提到了模型有能力融合两个视频变成一个语义通顺的更长视频。这种方式，在理想情况，对于数据的利用效率会从O(n)变为O(n^n)。如果显卡足够，简直无法想象</li></ol><p>所以，sora里面很可能更大比例的使用re-caption技术来获得准确高质量的视频描述。这个描述不仅仅是空间尺度的，甚至还有时间尺度的，比如"在14秒时，屏幕上出现一只猫，在17秒时跳走了"。然而，如何高质量的降采样、拼接，使得视频仍然是保有语义的，OpenAI不知道做了多少数据工程。</p><blockquote><p>"保有语义"这个描述可能不太准确，实际上是：希望视频描述和真实用户在要求sora生成视频的需求尽可能接近。比如可能没人希望生成"两瓶可乐在打架"的视频，虽然确实是有语义的……可能，也有人？</p></blockquote><p>Sora技术报告对这些细节语焉不详，我们只能期待学界的开源工作在这方面做出更多更有趣的探索吧。</p><h2 id="recaptionmy-insights">recaption：my insights</h2><p>我认为，代码、图片、视频，(甚至我研究的tool-call-chain)都是和文字独立的其他模态的数据。他们有自己的模式，目前我们对于语言模态的模型训练效果最好，或者说，在语言模态找到的auto-regressive训练方法对数据的训练效率最高。</p><h3 id="recaption-is-only-training-to-align">recaption is only trainingto align?</h3><p>想要习得一个新模态的能力，我们需要的是pair数据：不放设想一下，我从出生开始，虽然到现在也才20多年，但是我经历的数据是(文字-图像-视频-声音)等等完全对齐的，脑子里甚至在同时产生无穷无尽的CoT来解释目前看到的这一切</p><p><img src="../../files/images/recaption/歪头.jpeg" ></p><p>所以对于模型来说，学习别的模态是一个比学习语言模态更困难的任务：image2text做的好、text2image不容易。所以大家训练dalle想要的做的事情可能<strong>并不是让模型学会图片模态，而是希望模型更好的把自己对于文字模态的知识映射到图片模态</strong>。对于这种类似于“align”的任务，最好地办法就是给出更准确的align数据。什么叫更好的align数据？没有歧义的数据，比如说一个文字描述，你能画出多少图片，他们之间的差异有多大：画一个人、画一个男人、画一个小男孩、画一个华裔小男孩、画马斯克、画埃隆马斯克……可以认为，更详细的文字描述是在减少歧义，同时通过传入更多的信息去<strong>定点地激活对应的文字模态的知识</strong></p><p>当然，OpenAI做recaption的思路其实有点违背了预训练的初衷：更多的数据、更少的干预、更少的归纳偏置。当然，这种初衷是希望模型获得通用的世界知识建模能力，如果我们认为模型(T5-encoder)本来就有世界知识，我们只是在"elicit"激发他的知识，那另说。</p><p>在很多情况下，我们希望模型去学习到很多通用的、语言没法表达的世界知识，比如几何题做辅助线、一堆水滴落在水面的样子等等，我们恰好发现。我们发现，这些知识正好都是目前的多模态模型很差的地方……也许，获得这些能力，是需要我们跳出"align"、re-caption的观点，去开发一种直接瞄准获取多模态通用知识的方案。这种方案，简单来说就是"没有输入，直接输出"，和GPT4的训练方式一样</p><p>我认为，在视频模态训练sora是可行的，原因在这里：对于图片生成，尤其是Diffusionmodel，一个模糊的图片变成清晰一点的图片，在我看来对于语义的依赖性没那么强。反而是视频模态，生成后面的视频，对于前面的依赖性很强，也就是说模型必须在auto-regressive训练中直接捕获到一些世界知识，考虑下面这个：</p><blockquote><p>你从一个鼻子模糊的图片里，参考前面的草图把鼻子画清楚。关键是你猜到这里是鼻子，你本来就会画鼻子，你从中学到的知识并不精确</p><p>看到两个球碰撞前的视频，预测出来碰撞后的样子。你需要理解速度、中心、碰撞点等概念，从中学到的知识表征了世界的更多属性。</p></blockquote><h3 id="recaption-is-not-training-on-synthetic-data">recaption is NOT"training on synthetic data"</h3><p>最近另外一个比较火的关键词是在合成数据上做训练，recaption算是"在合成数据上做训练"吗？我认为不是，因为recaption是在对已有的数据做改写成更符合要求的形式，本来就有数据。Sora中使用虚幻5渲染出来的视频做训练才是真正的"trainingon syntheticdata"。我认为在合成数据上做训练，本质是想要让模型grokking：希望模型通过更多的数据，最终捕获到了更高级的heuristics。</p><p>举几个例子：</p><ol type="1"><li><p>牛顿被苹果砸了，发现了万有引力。只需要一个公式，就能预测所有苹果落地的trace。所以这个高级的heuristics需要更少的计算量就能表征更多的数据</p></li><li><p>小明算1+2+3... +10需要点点算，高斯发现了这个算式可以变成(1+10)*20/2.很快算完了。但他俩结果是一样的</p></li><li><p>虚幻五是一个渲染视频的引擎，如果"虚幻5"本身是一个模型，用户只需要输入操作虚幻五的"actiontrace"，就能渲染出一个视频。从compression的角度看，虚幻五把视频世界知识压缩进了自己的参数里，可以把一个长视频压缩到"actiontrace"这么少的数据量里。Sora很傻，他一直看虚幻5生成的视频，为了对虚幻五进行拙劣的模仿，目前还学的不太对</p></li></ol><blockquote><p>目前的AI还没有牛顿这么聪明，可能被砸一亿个苹果才会突然顿悟，发现公式，这个事情在学界叫做grokking。从这里，我们发现的事实是：在我不知道万有引力时，我可能通过一堆低级方法生成苹果降落的视频，来一直砸牛顿，反正总有一天能砸对。</p><p>在更高级的模型看来，虚幻五也是"傻子"，需要下载好几十GB才能渲染视频，没准他用100M就生成比虚幻五好多了。更高级的模型没准觉得人类也是傻子，测出来卡西米尔效应是-1/12但解释不了，自己知道为什么，只是没办法用自然语言表达……因为人类的数学体系太烂了，整个"数学词表"也就这么大，还是离散的。</p><p>可悲的是，人类自己本来有机会grokking的，但是每个人只能活100年，柯洁学10000年围棋真的还下不过alpha-zero吗……类似的有教育意义的案例，可以参考漫士沉思录对于哈马努金推导的讲解<ahref="https://www.bilibili.com/video/BV1si4y1p75k">全体自然数之和等于-1/12？真相远没有那么简单！</a></p></blockquote><p>所以，1)找到高级heuristics很困难。2)高级和低级heurisitic生成的数据都是正确的。3)越多的原始数据，越多发现高级herusitics的可能。从这三个观点出发，就产生了"trainingon syntheticdata"的想法：盼着模型去grokking。我十分同意这个观点，也相信这才是AGI最可行的路线。</p><p>不过话说回来，不管怎么看，这和recaption都不是一回事，在根本目的上就不一样。</p><h2 id="recaption-is-all-you-need">recaption is all you need?</h2><p>recaption本身有没有在学界的其他领域使用呢？我认为这是一个范式：A是一个比B更难的任务，A-&gt;B的模型效果不好，就反过来训一个 B -&gt; A的模型，为(A-B)pair提供更多的数据。这个思路很简单、很scalable，并且实际上很多工作都在用，我这里举一些例子：</p><ol type="1"><li>WaveCoder：根据query生成code很困难。那么我先找一堆code，对着他生成、改写query，再训练code模型。</li><li>CoachLM：SFT中根据query生成response很困难：那我找到已有的query-response数据对，把query改成更好的、更没有歧义的格式，再训练模型去生成response</li><li>ReformattedAlignment：推理领域，根据query生成CoT很困难。我把原始的数据格式改写一下，让query里告诉模型该怎么写CoT，再生成CoT。由此finetune模型，效果会更好吗？</li></ol><p>我相信，这种范式会在未来变得越来越普遍，甚至扩展到预训练场景(比如CMU今年出的Rephrasingthe Web)</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 多模态 </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> from scratch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2024-02-29总结：研一下开始了</title>
      <link href="/e9916de0.html"/>
      <url>/e9916de0.html</url>
      
        <content type="html"><![CDATA[<p>今天是2月29日，我迎来了研究生的第二个学期。上次2月29日已经是2020年，而下次2月29日要到2028年了。人生有多少4年，再加好久没有更新，遂写一写最近的生活吧。</p><p>其实我写总结这个track，还是因为最开始看了谭院士的博客 <ahref="https://twd2.me">WandaiBlog</a>：谭院士总是时间驱动，每天写一个sentence-level的总结，陆陆续续竟然坚持了十几年。时间是有惯性的，有点类似于顺着一个人的微信刷pyq，不会到了某个位置突然被卡掉，看下来有种震撼人心的感觉。所以我也想是不是记录一下自己的生活。</p><p>我当时选了另一种形式：事件感想驱动，更大的interval,在corpus-level做记录，所以给自己起名字叫做"随缘"。现在想想可能并不适合，我和谭院士的记录方式也许应该倒一倒。我的生活当然没有谭院士丰富，用instructiontuning的话说：每天翻来覆去总是从一些task set里先sample task <spanclass="math inline">\(t \in \mathcal{T}\)</span>，再sample <spanclass="math inline">\(x \in \mathcal{X}_t\)</span>，最后预测 <spanclass="math inline">\(y =me(x)\)</span>。做得多了，熟能生巧，常用的几个task的能力越来越高了，但一直没什么机会探索更大更diverse的instruction空间。</p><p>不过近期确实有所不同，我深感在过去一个月里，尝试的新事物堪比过去一两年。</p><span id="more"></span><p>首先，我这学期第一次当助教了，大家可能以为是一个课的助教，但实际上是三个课的助教……这就是所谓的"半年不开张，开张吃三年"吗。助教的职责主要是帮助同学们留作业判作业，以及在群里回答同学们的疑问。最开始有一些"好为人师"的新鲜感，但很快也就过去了，剩下大致有一些"重构课程体系，提供更好的教学方式，我辈义不容辞"的责任使命感？</p><p>这学期正好轮到NLP课作业体系重构，我同时还是学堂在线Mooc的NLP课的助教，他们两个课的作业体系本来是一致的，我需要把前者重构一下。因为是研究生课，大家都是做研究的，所以我进行了一个相对激进的尝试：我尝试把作业里面的"回答问题、完成任务"式的部分去掉了，变成了鼓励选课同学们对作业框架下的知识产生一些自己的问题并进行探索。站在我的角度，我觉得要求同学们完成某个任务是一个挺反直觉的东西，大家都是AGI，应该有能力自己组织自己的学习路径，发现学习过程对自己帮助最大的东西。等过几天作业发布了，我可以也许会把重构过的内容和我的一些insight以科普blog的形式发出来。不知道大家对我的设计形式会有怎么样的评价。我猜测研究生选课大致有两种用户画像：</p><ol type="1"><li>想混混了事，对NLP一点兴趣也没有，得多少分无所谓，破事多我就退课，别耽误我科研/实习/旅游……</li><li>我真的想学习NLP的知识，或者想通过这个课程认识老师，去跟着老师开展相关research。不关心最后得了多少分，当然分数高更好……</li></ol><p>我觉得不管哪种，从结果出发，应该体验都还不错：我判作业给分应该是非常宽松的，实际上我希望的形式应该是大家都能把被动式的完成作业的时间换成主动探索一些自己发现的小现象，而结果上都能拿满分。</p><p>除了当助教给同学们打分，我最近还第一次尝试了审稿。说来有点可惜，人生第一次审的两篇论文，一篇给了strong-reject,一篇给了weak-reject……可能我其实是批判型人格吧。当时看到自己论文的review时有多生气，未来几天别人看我就有多生气。我不太喜欢审稿，因为需要我来总结论文里的优点和缺点。我觉得这个事情有点反直觉：大家发论文是想要对学界做贡献，所以别人会帮你去芜存真，只会关注你的主要贡献点。我认为评价一个论文应该看他最优的优点max(strength)到底有多好，而不应该是strength-weakness、甚至像”木桶效应“一样揪着最大的窟窿不放……可惜我不是管事的，只能给别人打一个reject了</p><p>年初的时候奥特曼说2030年实现AGI，我其实心里挺焦虑的：我在研究AI，如果2030年时AI被解决了，那如果我在这6年里没有贡献关键技术，岂不是整个2020-2030这10年的努力也被抹杀了吗？不过，类比一下互联网或者电器技术的发展，其实在学术上解决一个问题，到实际帮助所有用户解决这个问题，中间可能需要10年、甚至几十年的时间。在这段merge的时间里，大家实际上在做一个技术和服务相互妥协的过程：什么服务对用户很重要，但技术成本上困难所以要砍掉；什么技术很先进很scalable，但会拖累短期里的用户服务质量，就需要砍掉……妥协的结果是平衡，也大概率会是目前的基础框架下的最优形态。整个这个过程在我看来挺像是”机器学习的“，当然这里面的"forward-explorationandbackward-gradient"不是某个参数的变大变小，而是某个、某几个公司的成立和消亡。在这个情境下，我们自己也是"世界模型"的其中一个参数……但谁能成为Meta发现的"MassiveActivation"呢？</p><p>因此，除了技术本身以外，我最近其实也开始思考技术的落地和产品。比如，我发现了一个叫做”增量“的概念，一个产品或者说系统，最重要的是比起同类产品带给用户的增量价值，而不是这个系统本身的技术先进性。一个技术很先进的东西，可能对用户带来的服务并不好；反之，一个让用户感知很好的产品，其背后的技术不一定很先进。我作为科研或者学术背景出发，常常会受到"选论文"式的归纳偏置，关注技术创新性，而忽略了他的可行性以及增量价值。随着对于产品的调研和感悟越来越多，也许以后，除了阅读笔记以外，我还可以分享一些对于产品的感悟笔记也说不准……</p><blockquote><p>夫人之相与，俯仰一世。或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至；及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀，况修短随化，终期于尽！古人云：“死生亦大矣。”岂不痛哉！</p></blockquote><p>简单来说：下一个2月29日就是2028年了，而下一次在2月29日恰逢"疯狂星期四"更是28年以后了：不知道下个2月29日的我在干什么呢？还会在维护这个blog吗？28年以后的我还在吃疯狂星期四吗？希望那时的我，也能像今天一样喜欢探索学习自己不知道的东西，而不是像大多数人一样，随着年龄增长开始倾向于沉浸在舒适圈里变得over-confidence</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程总结 </tag>
            
            <tag> 随笔 </tag>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-Consistency之我见，兼More Agents is All You Need</title>
      <link href="/7cd10148.html"/>
      <url>/7cd10148.html</url>
      
        <content type="html"><![CDATA[<p>好久不更新了，看到之前大约都是15天更新一篇笔记，最近不知道咋回事竟然一个多月没更新，正好这两天刷到了"MoreAgents is All YouNeed"，就来讲讲“时间换效果”的鼻祖——self-consistency。如果让模型sample多次，然后做major-voting，效果会更好吗？</p><p>参考文献：</p><blockquote><p>Self-Consistency Improves Chain of Thought Reasoning In LanguageModels</p><p>Escape Sky-High Cost: Early-Stopping Self-Consistency for Multi-StepReasoning</p><p>Universal Self-Consistency for Large Language Model Generation</p><p>More Agents is All You Need</p><p>Unlock Predictable Scaling from Emergent Abilities</p></blockquote><span id="more"></span><h2 id="self-consistency">Self-Consistency</h2><p>什么是self-consistency？要从CoT算法说起，之前CoT的思路是：如果让模型在输出答案之前，先说一些rationale，最终就能得出相对更准确度的结果。用类似于"Let'sthink step by step"的prompt，就能激发模型进行对应的CoT输出。</p><p>而self-consistency则更进一步，如果用更多的CoT放到一起，能不能进一步提高效果？具体来说，模型按照某种随机解码算法，比如p-sample进行多次解码，最后根据多个结果做去重，最后给出众数作为结果。一个经典的例子就是数学计算题</p><p><img src="../../files/images/self-consistency/sc.png"></p><p>神奇的是，随着sample次数N的增长，最终的准确率就会逐渐提高，比如下面这个图对比了下面几个东西</p><table><tr><td><img src="../../files/images/self-consistency/sc-perf.png" align=left style="zoom:50%;" ></td><td><img src="../../files/images/self-consistency/sc-scale.png" align=left style="zoom:45%;" ></td></tr><tr><td>随着sample次数的提高，SC算法的效果逐渐提升</td><td>在一定的范围内，越强的模型受到sc的增量越多</td></tr></table><ul><li>橙色线：greedy-decoding的结果，和N无关</li><li>绿色的线：voting-with-ppl：sampleN次，提交对应的ppl最小的，即token-level信心值最高的</li><li>蓝色的线：self-consistency，提交结果的众数</li></ul><p>实际上，self-consistency是一个时间检验的算法，对于各种场景都能几乎稳定地获得提升。早在2021年OpenAI的GSM8K论文，就报告了类似的结果：他们先使用打分器选出最高的topk个样本，再在他们之中选择Major-voting的结果(下右图)，效果比单纯对打分器选择top1的效果好很多</p><p><img src="../../files/images/self-consistency/gsm8k.png"></p><h2id="self-consistency之我见instance-level-calibration与集成学习">Self-Consistency之我见：Instance-LevelCalibration与集成学习</h2><p>为什么self-consistency效果会这么好？我下面来聊聊我的看法。</p><p>首先，self-consistency论文中报告了一个有趣的结果：他们发现结果一致性越高的query，真实的准确率也越高</p><table><tr><td><img src="../../files/images/self-consistency/sc-calibration.png" align=left style="zoom:50%;" ></td><td><img src="../../files/images/self-consistency/gpt4-calibration.png" align=left style="zoom:40%;" ></td></tr><tr><td>sc中，结果一致性越高，真实准确率就越高</td><td>GPT4 report：选项一致性越高，结果准确率就越高</td></tr></table><p>这其实和后来大家发现的calibration现象很像：对于4选1 multi-choiceQA场景(比如MMLU)，模型对于4个选项token里信心最高的选项的prob越大，对应题目的最终准确率就越高</p><blockquote><p>实际上，calibration的这种方式在"selectiveprediction"领域又被称为MaxProb算法，calibration领域大概是"重新发现"了他……</p></blockquote><p>这就促使我去思考，self-consistency既然在instance-level选择概率最大的，并根据可以报告instance-levelMaxProb。是不是就是在模拟instance-level的calibration呢？</p><p>instance-level的一致性该怎么表示？如果从刚才maxProb的角度思考的话，直观的想法是使用PPL最低的样本。其实，这个方法也被self-consistency原始论文报告了(绿线)。可以发现，这个算法的效果虽然比greedy-search好，但并不是scalable的，甚至在n很大以后效果会更差。这个现象和GSM8K中best-of-ORM-verifier是一致的，都是随着N的增加先增后减。这是因为ppl或者verifier都是不够鲁棒的。举个例子，对于math场景，ppl的大小不仅受到模型对于expression的信心值，还受到expression表达方式的影响，考虑下面两个句子：</p><blockquote><p>3*5=15</p><p>3乘以5的结果是15</p></blockquote><p>对于模型来说，ppl是生成每个token的平均概率。这两个句子虽然表达相同的意思，但是表达形式不同，"乘以"、“的”、“结果”这几个token的信心值都会很高，所以导致两个句子的ppl差很远。然而，其实模型对于3*5=15这个表达式的信心值是固定的。这也就会导致，best-of-ppl的算法在N很多以后，模型可能偶然发现一些很简单的表达形式，ppl很低。但这个这只是模型对于“表达方式”的信心值。类似的，best-of-verifier也会有类似的情况，这就是bad-heuristics，或者叫adversarial-solutions的问题。除此之外，还有rationale方式的问题，一个结果可以用不同的思考路径所表达，就像是一个题目会有多种方法去做。</p><p>相比之下我们发现，Multi-choiceQA场景没有表达形式的bias，因为四个选项都是使用一个token表达的。因此，直接使用选项token的maxProb就可以做出相对很准确的准确率估计。</p><p>major-voting是一个道理，如果我们sampleN次对结果划分出不同等价类的话，可以发现，我们实际上得到了不同结果的频度。当N变得很大时，这个趋向于了不同结果的概率分布。从这个instance-level概率分布取最大概率，就是instance-level的maxProb。他不会受到表达形式的影响，因为大家都是按照同样的算法采样，每个算法都有可能以困难的形式被表达，也有可能用简单的方式表达。</p><blockquote><p>从直觉来看，self-consistency的效果提升其实也很容易被理解。我们可以类似于shengding学长的论文去定义出pass-until：sampleN次，其中答案正确的比例。</p><p>假如pass-until =60%，那么显然self-consistency一定会把正确答案投票出来。但如果只sample一次，就有40%的概率做错。对偶的情况是，pass-until!=result-maxProb时，self-consistency一定做不对。但是如果只sample一次，有可能反而能找到正确答案(虽然这个概率估计很低)。</p></blockquote><p>self-consistency的提升点，实际上是在两种情况的博弈。想要观察self-consistency的提升点，也许需要列出来一个数据集中所有样本的pass-until的直方图来观察。实际上，这个直方图恰恰就是上面展示过的"带权"版本的sc-consistency图。另外，我认为实际上应该使用self-consistency的结果（而不是greedy-search）作为模型对于一个数据集的performance。</p><p>另外，self-consistency主要是考虑用同样的随机解码算法做拟合，能不能考虑算法本身的异构呢？</p><ul><li>用不同的模型结果做综合</li><li>不用的解码算法的结果做综合</li><li>不同的system-prompt的结果做综合</li><li>甚至是multi-agentsystem中不同agent通过类似debate的形式做沟通博弈...</li><li>google还做过综合不同的reward model来进行RLHF训练</li></ul><p>这些思路也自古有之，叫做集成学习。出自一个朴素的直觉：条条大路通罗马，不同的方法都相对认可的东西，实际正确率也更高。所以说，self-consistency集成不同的思考链，其实是集成学习的一种in-context版本的特例，自然也可以集成更diverse的自由度。</p><p>最后，self-consistency根据结果的等价类做划分，得出来了instance-level的calibration，这个事情能不能再step-level做呢？虽然没有人formalize这个问题，但我在一些独立的工作中看到了类似的解决方案：</p><blockquote><ol type="1"><li>shunyu yao的ToT：其中的BFSbaseline在step-level进行self-vote。对偶的，可以设计self-consistency。</li><li>所有的PRM相关论文：let's veriffy step-by step 、Step-AwareVerifier、math-shepherd、Discriminator-Guided CoT、outcome-supervisedverfier等等，都可以在step-level设计look-ahead搜索算法</li><li>More Agents is All You Need中的hierarchical sample-and-voting</li></ol></blockquote><p>再推广一步，到最细力度的token-level，一致性是天生就存在的(logits)。那么就会有beam-search等算法去寻找ppl最低的结果</p><h2 id="self-consistency的应用与改进">Self-Consistency的应用与改进</h2><p>self-consistency有两个巨大的问题和一个小问题。改进方法大致也从这些问题出发</p><h3id="大问题1需要某场景可以划分等价类">大问题1：需要某场景可以划分等价类</h3><p>对于数学场景，可以认为结果相同就是一个等价类。但对于很多free-form的场景，比如数学证明题，比如写代码，比如机器翻译该怎么办？写代码任务，类似于googleAlphaCode，会根据代码中每个测例的输出，按照对拍结果划分等价类。这依赖于一个外置的执行器。</p><p>既然ToT可以做self-vote，那self-consistency能不能行呢？最近，dennyZhou做了一篇工作，可以让模型根据多个样本都放在prompt里，然后让模型自己说一个最一致的结果出来。那么，代价是什么呢？需要模型可以同时存下所有的结果在context里！</p><p>另外，刚才提到了step-level的self-consistency，这个更复杂，依赖于对step划分等价类。首先，不是所有任务都能像ReACT场景一样划分成天生多步的。其次，很多任务的多个执行链其实不是按照树的格式组织的，是按照图的形式组织的。所以，更细粒度的step-search依赖于更细粒度的等价类划分</p><h3 id="大问题2需要更多的计算资源">大问题2：需要更多的计算资源</h3><p>self-consistency动辄就需要一个query sampleN=40次。对于GPT4来说，在MATH数据集sample40次，需要大概2000美元。这显然不可接受，最近有很多工作试图减少self-consistency的效果。</p><p>从直觉上来看，如果只sample10次，结果都是一样的。理论上这个很可能是就是众数结果，最极端的情况是出现类似斯诺克中的"超分"：即使后面所有的结果都是目前第二大的，也不能超过最大的。从统计学上对这个直觉做建模，就是狄利克雷分布。很多工作会涉及一些基于熵的算法去拟合self-consistency的一致性要求。</p><h3id="小问题需要一个场景的天然假阳率不大">小问题：需要一个场景的天然假阳率不大</h3><p>最后这个，本质上不是一个问题，而是一个现象。举个例子，对于判断正误的问题，答案只可能是对或者不对，就两种可能。随机猜都有50%准确率，这时候有非常多的假阳情况。self-consistency拟合出来的一致性，就会受到影响：假阳的样本往往都是eazy-heuristic，一致性都挺高的。</p><p>所以，应用self-consistency的时候，可能还需挑场景，最好选择类似计算题的场景：结果正确，那么基本上过程也是正确的。</p><h2 id="我的思考">我的思考</h2><p>之前的论文阅读笔记大多是照着论文的展开顺序讲。最近读的多了以后，渐渐感觉很多东西和某些别的领域有更多的相关性，把别的领域的直觉拿来解释，可能相对更清晰。这也是我现在读论文的一个经验。比起理解论文的素有方法，更重要的是理解论文背后的直觉：</p><ul><li>一方面，理解了直觉理论上可以把论文的方法重新推导出来</li><li>另一方面，论文的方法不一定都是管用的，有一些“为了创新而创新”的工作量证明的部分。从直觉出发，更容易剔除掉这些"noise"……</li></ul><blockquote><p>对于这个问题，有人说：AI的论文没什么内容，创新都很简单。我认为恰恰相反，理解起来简单是因为讲得好。如果一个论文讲的神头鬼脸、玄之又玄，那大概率是作者自己都没搞清楚自己论文的“firstprinciple”——背后的直觉到底是什么</p></blockquote><p>最后，self-consistency是一个时间检验的方法，对于所有模型、对于所有场景都能有提升。之前jasonwei发了一个twitter，认为CoT和self-consistency的效果提升来源于数据和问题的固有熵，对此我产生了一些不同的见解，贴出来我的看法：</p><p>I don't entirely agree with this viewpoint, here are some of myinsights:</p><ol type="1"><li>CoT aims to use a nearly identical, substantial amount ofcomputation for both difficult and simple problems to ensureperformance. Thus, recent works like GSM8K-zero (<ahref="https://t.co/JJO0tQkHSP">https://arxiv.org/abs/2401.11467</a>)have found that CoT can waste a lot of computational resources onsimple(trivial) problems.</li><li>The information density in the corpus(the varying difficulty ofpredicting each token) is an intrinsic property of datasets (e.g.,predicting the answer-token on MMLU is harder than predictingquery-tokens), which is why speculative decoding succeeds in somesettings and not in others.</li><li>Instead of fitting corpora with uniform information density, it'sbetter to retain the dataset's intrinsic properties and instead adaptthe model computation to fit the difficulty of the corpus, i.e.,adaptive computation.</li><li>On the instance-level/step-level, this is what "flow-engineering" isabout: Allowing for dynamic decision-making on the complexity of CoTbased on the task.</li><li>At the token-level, there are passive solutions like pause-token (<ahref="https://t.co/dYnr2CEAqI">https://arxiv.org/abs/2310.02226</a>)that use more forward passes on some manually selected tokens. There arealso active solutions like early-exit (<ahref="https://t.co/5TNCd76Zcy">https://arxiv.org/abs/2402.00518</a>),which allow the model to freely decide the computational effort for eachtoken. And, though speculative decoding has a different goal, itacutually achieves the purpose of adaptive computation.</li><li>Besides point 5, I also think Mixture of Experts (MoE) technologyhas achieved part of this goal: Firstly, early works in MoE coulddynamically decide the activated number of experts and implementheterogeneity of experts (different experts have different size) toachieve the purpose of adaptive computation. Secondly, OpenMoE (<ahref="https://t.co/li4vngcGYr">https://github.com/XueFuzhao/OpenMoE</a>)discovered that even with a fixed number and all the same size ofexperts, a phenomenon occurs where "one expert is always responsible forsome tokens". And we all that token-id represents part of the predictivedifficulty (e.g., predicting punctuation is often easier than predictingnumbers)...</li></ol><p>I believe that future models can freely and acitvely decide oncomputational complexity based on the task's information density, justlike us: Think Before You Speak.</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>arxiv-insights</title>
      <link href="/f4243ee6.html"/>
      <url>/f4243ee6.html</url>
      
        <content type="html"><![CDATA[<p>压缩带来智能，5% 的论文决定学术界 95% 的成果！每天从 Arxiv论文中总结分享最重要、最有趣的最多三篇论文。</p><p>Compression brings intelligence, 5% of papers determine 95% of AItechnologies! Share the most important papers from Arxiv, every day, upto three!</p><script type="text/javascript">    var insight_now_id = 0;    var insight_max_id = 14;    function tips_insight(num){        document.getElementById("insights_"+insight_now_id).hidden = "hidden";        insight_now_id -= num;        if (insight_now_id > insight_max_id) {insight_now_id = insight_max_id;}        if (insight_now_id < 0) {insight_now_id = 0;}        document.getElementById("insights_" + insight_now_id).hidden = "";    }</script><table id="insights_0" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年十一月November</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00E100"><a href=../8d2dbba3>1(223-&gt;4 papers)</a></td><td>2</td></tr><tr><td>3</td><td bgcolor="#00DD00"><a href=../6b0470e7>4(214-&gt;5 papers)</a></td><td bgcolor="#00DA00"><a href=../84c61bd9>5(472-&gt;5 papers)</a></td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td></tr><tr><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td></tr><tr><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td><td>30</td></tr><tr></tr></table><table id="insights_1" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td bgcolor="#00E400"><a href=../50bb6226>1(172-&gt;4 papers)</a></td><td bgcolor="#00F700"><a href=../bb8cd925>2(74-&gt;1 papers)</a></td><td bgcolor="#00E400"><a href=../544eb21b>3(103-&gt;3 papers)</a></td><td bgcolor="#00D300"><a href=../b692a962>4(128-&gt;6 papers)</a></td><td>5</td></tr><tr><td>6</td><td bgcolor="#00E400"><a href=../5da51261>7(121-&gt;2 papers)</a></td><td bgcolor="#00D700"><a href=../acae49ec>8(224-&gt;5 papers)</a></td><td>9</td><td>10</td><td>11</td><td>12</td></tr><tr><td>13</td><td>14</td><td>15</td><td bgcolor="#00E600"><a href=../73e9a69f>16(133-&gt;3 papers)</a></td><td bgcolor="#00E300"><a href=../9c2bcda1>17(156-&gt;3 papers)</a></td><td>18</td><td>19</td></tr><tr><td>20</td><td bgcolor="#00F600"><a href=../8d7dbe7>21(125-&gt;1 papers)</a></td><td bgcolor="#00E900"><a href=../e3e060e4>22(198-&gt;3 papers)</a></td><td bgcolor="#00EB00"><a href=../c220bda>23(135-&gt;3 papers)</a></td><td bgcolor="#00F900"><a href=../eefe10a3>24(122-&gt;1 papers)</a></td><td bgcolor="#00F200"><a href=../13c7b9d>25(145-&gt;2 papers)</a></td><td>26</td></tr><tr><td>27</td><td bgcolor="#00D500"><a href=../f4c2f02d>28(172-&gt;5 papers)</a></td><td bgcolor="#00E200"><a href=../1b009b13>29(431-&gt;4 papers)</a></td><td bgcolor="#00DB00"><a href=../269b6f19>30(261-&gt;4 papers)</a></td><td bgcolor="#00D500"><a href=../c9590427>31(260-&gt;4 papers)</a></td></tr></table><table id="insights_2" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td>1</td><td bgcolor="#00F800"><a href=../a11632bb>2(30-&gt;1 papers)</a></td><td>3</td><td bgcolor="#00DB00"><a href=../ac0842fc>4(169-&gt;4 papers)</a></td><td bgcolor="#00E900"><a href=../43ca29c2>5(52-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../a8fd92c1>6(46-&gt;1 papers)</a></td><td>7</td></tr><tr><td>8</td><td bgcolor="#00FE00"><a href=../59f6c94c>9(33-&gt;0 papers)</a></td><td bgcolor="#00E300"><a href=../646d3d46>10(66-&gt;3 papers)</a></td><td bgcolor="#00F700"><a href=../8baf5678>11(49-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../6098ed7b>12(38-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../8f5a8645>13(32-&gt;1 papers)</a></td><td>14</td></tr><tr><td>15</td><td bgcolor="#00FE00"><a href=../69734d01>16(39-&gt;0 papers)</a></td><td bgcolor="#00EB00"><a href=../86b1263f>17(101-&gt;2 papers)</a></td><td bgcolor="#00F300"><a href=../77ba7db2>18(78-&gt;1 papers)</a></td><td bgcolor="#00E700"><a href=../9878168c>19(53-&gt;4 papers)</a></td><td bgcolor="#00E900"><a href=../fd8f5b47>20(55-&gt;3 papers)</a></td><td>21</td></tr><tr><td>22</td><td bgcolor="#00F900"><a href=../16b8e044>23(56-&gt;1 papers)</a></td><td bgcolor="#00DA00"><a href=../f464fb3d>24(187-&gt;4 papers)</a></td><td bgcolor="#00F600"><a href=../1ba69003>25(79-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../f0912b00>26(73-&gt;1 papers)</a></td><td bgcolor="#00E900"><a href=../1f53403e>27(84-&gt;3 papers)</a></td><td>28</td></tr><tr><td>29</td><td bgcolor="#00EE00"><a href=../3c018487>30(53-&gt;2 papers)</a></td></tr></table><table id="insights_3" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年八月August</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>11</td><td bgcolor="#00F500"><a href=../bd0e34fe>12(73-&gt;1 papers)</a></td><td bgcolor="#00F200"><a href=../52cc5fc0>13(68-&gt;2 papers)</a></td><td bgcolor="#00F500"><a href=../b01044b9>14(53-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../5fd22f87>15(41-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../b4e59484>16(36-&gt;1 papers)</a></td><td>17</td></tr><tr><td>18</td><td bgcolor="#00F900"><a href=../45eecf09>19(45-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../201982c2>20(102-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../cfdbe9fc>21(67-&gt;2 papers)</a></td><td bgcolor="#00E700"><a href=../24ec52ff>22(58-&gt;2 papers)</a></td><td bgcolor="#00F900"><a href=../cb2e39c1>23(78-&gt;1 papers)</a></td><td>24</td></tr><tr><td>25</td><td bgcolor="#00F700"><a href=../2d07f285>26(36-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../c2c599bb>27(76-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../33cec236>28(50-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../dc0ca908>29(53-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../e1975d02>30(40-&gt;2 papers)</a></td><td>31</td></tr><tr></tr></table><table id="insights_4" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年七月July</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00F600"><a href=../b0c78508>1(68-&gt;1 papers)</a></td><td bgcolor="#00E100"><a href=../5bf03e0b>2(155-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../b4325535>3(96-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../56ee4e4c>4(87-&gt;2 papers)</a></td><td>5</td><td>6</td></tr><tr><td>7</td><td bgcolor="#00ED00"><a href=../4cd2aec2>8(121-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../a310c5fc>9(106-&gt;2 papers)</a></td><td bgcolor="#00F100"><a href=../9e8b31f6>10(69-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../71495ac8>11(45-&gt;2 papers)</a></td><td bgcolor="#00EE00"><a href=../9a7ee1cb>12(46-&gt;2 papers)</a></td><td>13</td></tr><tr><td>14</td><td bgcolor="#00F900"><a href=../78a2fab2>15(49-&gt;1 papers)</a></td><td bgcolor="#00EA00"><a href=../939541b1>16(112-&gt;2 papers)</a></td><td bgcolor="#00F500"><a href=../7c572a8f>17(97-&gt;1 papers)</a></td><td>18</td><td bgcolor="#00E400"><a href=../629e1a3c>19(129-&gt;3 papers)</a></td><td>20</td></tr><tr><td>21</td><td bgcolor="#00F500"><a href=../39c87ca>22(50-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../ec5eecf4>23(105-&gt;3 papers)</a></td><td bgcolor="#00F400"><a href=../e82f78d>24(54-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../e1409cb3>25(49-&gt;1 papers)</a></td><td bgcolor="#00EF00"><a href=../a7727b0>26(60-&gt;2 papers)</a></td><td>27</td></tr><tr><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="insights_5" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年六月June</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>2</td><td bgcolor="#00F500"><a href=../69a48cb0>3(77-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../8b7897c9>4(153-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../64bafcf7>5(93-&gt;2 papers)</a></td><td bgcolor="#00F900"><a href=../8f8d47f4>6(90-&gt;1 papers)</a></td><td bgcolor="#00E900"><a href=../604f2cca>7(84-&gt;3 papers)</a></td><td>8</td></tr><tr><td>9</td><td bgcolor="#00F500"><a href=../431de873>10(80-&gt;1 papers)</a></td><td bgcolor="#00E400"><a href=../acdf834d>11(150-&gt;3 papers)</a></td><td bgcolor="#00E800"><a href=../47e8384e>12(148-&gt;3 papers)</a></td><td bgcolor="#00E700"><a href=../a82a5370>13(83-&gt;3 papers)</a></td><td bgcolor="#00F800"><a href=../4af64809>14(88-&gt;1 papers)</a></td><td>15</td></tr><tr><td>16</td><td bgcolor="#00EB00"><a href=../a1c1f30a>17(72-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../50caa887>18(310-&gt;4 papers)</a></td><td bgcolor="#00E500"><a href=../bf08c3b9>19(161-&gt;3 papers)</a></td><td>20</td><td bgcolor="#00E600"><a href=../353de54c>21(223-&gt;3 papers)</a></td><td>22</td></tr><tr><td>23</td><td bgcolor="#00F700"><a href=../d3142e08>24(100-&gt;1 papers)</a></td><td bgcolor="#00ED00"><a href=../3cd64536>25(188-&gt;2 papers)</a></td><td bgcolor="#00F000"><a href=../d7e1fe35>26(101-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../3823950b>27(101-&gt;3 papers)</a></td><td bgcolor="#00EC00"><a href=../c928ce86>28(78-&gt;2 papers)</a></td><td>29</td></tr><tr><td>30</td></tr></table><table id="insights_6" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年五月May</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td bgcolor="#00EB00"><a href=../d09b3043>1(64-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../3bac8b40>2(71-&gt;2 papers)</a></td><td bgcolor="#00F500"><a href=../d46ee07e>3(47-&gt;1 papers)</a></td><td>4</td></tr><tr><td>5</td><td bgcolor="#00F500"><a href=../32472b3a>6(69-&gt;1 papers)</a></td><td bgcolor="#00E400"><a href=../dd854004>7(82-&gt;4 papers)</a></td><td bgcolor="#00F000"><a href=../2c8e1b89>8(41-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../c34c70b7>9(36-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../fed784bd>10(42-&gt;1 papers)</a></td><td>11</td></tr><tr><td>12</td><td bgcolor="#00F700"><a href=../15e03fbe>13(49-&gt;1 papers)</a></td><td bgcolor="#00FE00"><a href=../f73c24c7>14(122-&gt;0 papers)</a></td><td bgcolor="#00FE00"><a href=../18fe4ff9>15(42-&gt;0 papers)</a></td><td bgcolor="#00FE00"><a href=../f3c9f4fa>16(28-&gt;0 papers)</a></td><td bgcolor="#00ED00"><a href=../1c0b9fc4>17(48-&gt;2 papers)</a></td><td>18</td></tr><tr><td>19</td><td bgcolor="#00F800"><a href=../6735e2bc>20(43-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../88f78982>21(106-&gt;1 papers)</a></td><td bgcolor="#00FE00"><a href=../63c03281>22(40-&gt;0 papers)</a></td><td>23</td><td bgcolor="#00E600"><a href=../6ede42c6>24(196-&gt;3 papers)</a></td><td>25</td></tr><tr><td>26</td><td bgcolor="#00E100"><a href=../85e9f9c5>27(72-&gt;3 papers)</a></td><td bgcolor="#00ED00"><a href=../74e2a248>28(72-&gt;2 papers)</a></td><td bgcolor="#00F100"><a href=../9b20c976>29(81-&gt;2 papers)</a></td><td bgcolor="#00E900"><a href=../a6bb3d7c>30(72-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../'49795642'>31(76-&gt;3 papers)</a></td></tr></table><table id="insights_7" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年四月April</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00E400"><a href=../d0de9c6>1(62-&gt;3 papers)</a></td><td bgcolor="#00EA00"><a href=../e63a52c5>2(159-&gt;3 papers)</a></td><td bgcolor="#00F900"><a href=../9f839fb>3(101-&gt;1 papers)</a></td><td bgcolor="#00F400"><a href=../eb242282>4(74-&gt;1 papers)</a></td><td bgcolor="#00E300"><a href=../4e649bc>5(72-&gt;4 papers)</a></td><td>6</td></tr><tr><td>7</td><td bgcolor="#00DF00"><a href=../f118c20c>8(46-&gt;3 papers)</a></td><td bgcolor="#00E100"><a href=../1edaa932>9(118-&gt;3 papers)</a></td><td bgcolor="#00F000"><a href=../23415d38>10(62-&gt;2 papers)</a></td><td bgcolor="#00F400"><a href=../cc833606>11(48-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../27b48d05>12(59-&gt;3 papers)</a></td><td>13</td></tr><tr><td>14</td><td bgcolor="#00FE00"><a href=../c568967c>15(46-&gt;0 papers)</a></td><td bgcolor="#00ED00"><a href=../2e5f2d7f>16(137-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../c19d4641>17(47-&gt;1 papers)</a></td><td bgcolor="#00E700"><a href=../30961dcc>18(57-&gt;3 papers)</a></td><td bgcolor="#00F800"><a href=../df5476f2>19(59-&gt;1 papers)</a></td><td>20</td></tr><tr><td>21</td><td bgcolor="#00FE00"><a href=../be56eb04>22(50-&gt;0 papers)</a></td><td bgcolor="#00DA00"><a href=../5194803a>23(104-&gt;4 papers)</a></td><td bgcolor="#00F600"><a href=../b3489b43>24(83-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../5c8af07d>25(47-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../b7bd4b7e>26(56-&gt;1 papers)</a></td><td>27</td></tr><tr><td>28</td><td bgcolor="#00FE00"><a href=../46b610f3>29(41-&gt;0 papers)</a></td><td bgcolor="#00E700"><a href=../7b2de4f9>30(99-&gt;3 papers)</a></td></tr></table><table id="insights_8" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年三月March</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00F000"><a href=../707eef9e>1(67-&gt;2 papers)</a></td><td>2</td></tr><tr><td>3</td><td bgcolor="#00F700"><a href=../965724da>4(48-&gt;1 papers)</a></td><td bgcolor="#00EE00"><a href=../79954fe4>5(175-&gt;2 papers)</a></td><td bgcolor="#00EA00"><a href=../92a2f4e7>6(74-&gt;3 papers)</a></td><td bgcolor="#00F700"><a href=../7d609fd9>7(52-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../8c6bc454>8(58-&gt;2 papers)</a></td><td>9</td></tr><tr><td>10</td><td bgcolor="#00EC00"><a href=../b1f0305e>11(69-&gt;2 papers)</a></td><td bgcolor="#00F800"><a href=../5ac78b5d>12(115-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../b505e063>13(53-&gt;2 papers)</a></td><td bgcolor="#00E800"><a href=../57d9fb1a>14(73-&gt;3 papers)</a></td><td bgcolor="#00E100"><a href=../b81b9024>15(58-&gt;3 papers)</a></td><td>16</td></tr><tr><td>17</td><td bgcolor="#00F900"><a href=../4de51b94>18(75-&gt;1 papers)</a></td><td bgcolor="#00DC00"><a href=../a22770aa>19(114-&gt;4 papers)</a></td><td bgcolor="#00EF00"><a href=../c7d03d61>20(61-&gt;2 papers)</a></td><td bgcolor="#00EE00"><a href=../2812565f>21(56-&gt;2 papers)</a></td><td bgcolor="#00E100"><a href=../c325ed5c>22(66-&gt;3 papers)</a></td><td>23</td></tr><tr><td>24</td><td bgcolor="#00F000"><a href=../21f9f625>25(67-&gt;2 papers)</a></td><td bgcolor="#00E100"><a href=../cace4d26>26(148-&gt;3 papers)</a></td><td bgcolor="#00EE00"><a href=../250c2618>27(84-&gt;2 papers)</a></td><td bgcolor="#00E800"><a href=../d4077d95>28(66-&gt;3 papers)</a></td><td bgcolor="#00E100"><a href=../3bc516ab>29(69-&gt;3 papers)</a></td><td>30</td></tr><tr><td>31</td></tr></table><table id="insights_9" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年二月February</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td bgcolor="#00E600"><a href=../ade8361b>1(52-&gt;3 papers)</a></td><td bgcolor="#00E200"><a href=../46df8d18>2(54-&gt;4 papers)</a></td><td>3</td></tr><tr><td>4</td><td bgcolor="#00EB00"><a href=../a4039661>5(71-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../4f342d62>6(221-&gt;2 papers)</a></td><td bgcolor="#00E900"><a href=../a0f6465c>7(77-&gt;3 papers)</a></td><td bgcolor="#00F400"><a href=../51fd1dd1>8(58-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../be3f76ef>9(81-&gt;2 papers)</a></td><td>10</td></tr><tr><td>11</td><td bgcolor="#00EE00"><a href=../875152d8>12(46-&gt;2 papers)</a></td><td bgcolor="#00E800"><a href=../'689339e6'>13(93-&gt;3 papers)</a></td><td bgcolor="#00F200"><a href=../8a4f229f>14(51-&gt;2 papers)</a></td><td bgcolor="#00EC00"><a href=../658d49a1>15(49-&gt;2 papers)</a></td><td bgcolor="#00F000"><a href=../8ebaf2a2>16(68-&gt;2 papers)</a></td><td>17</td></tr><tr><td>18</td><td bgcolor="#00ED00"><a href=../7fb1a92f>19(88-&gt;2 papers)</a></td><td bgcolor="#00E400"><a href=../1a46e4e4>20(265-&gt;3 papers)</a></td><td bgcolor="#00E600"><a href=../f5848fda>21(108-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../1eb334d9>22(105-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../f1715fe7>23(105-&gt;3 papers)</a></td><td>24</td></tr><tr><td>25</td><td bgcolor="#00F200"><a href=../175894a3>26(114-&gt;3 papers)</a></td><td bgcolor="#00EE00"><a href=../f89aff9d>27(165-&gt;3 papers)</a></td><td bgcolor="#00E900"><a href=../991a410>28(84-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../e653cf2e>29(96-&gt;2 papers)</a></td></tr></table><table id="insights_10" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年一月January</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00F400"><a href=../10225ad5>1(42-&gt;1 papers)</a></td><td bgcolor="#00EB00"><a href=../fb15e1d6>2(48-&gt;1 papers)</a></td><td bgcolor="#00ED00"><a href=../14d78ae8>3(24-&gt;2 papers)</a></td><td bgcolor="#00EF00"><a href=../f60b9191>4(29-&gt;2 papers)</a></td><td bgcolor="#00F800"><a href=../19c9faaf>5(28-&gt;1 papers)</a></td><td>6</td></tr><tr><td>7</td><td bgcolor="#00F800"><a href=../ec37711f>8(17-&gt;1 papers)</a></td><td bgcolor="#00DA00"><a href=../3f51a21>9(80-&gt;4 papers)</a></td><td bgcolor="#00EB00"><a href=../3e6eee2b>10(38-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../d1ac8515>11(36-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../3a9b3e16>12(60-&gt;2 papers)</a></td><td>13</td></tr><tr><td>14</td><td bgcolor="#00DA00"><a href=../d847256f>15(57-&gt;3 papers)</a></td><td>16</td><td bgcolor="#00CE00"><a href=../dcb2f552>17(163-&gt;5 papers)</a></td><td bgcolor="#00F700"><a href=../2db9aedf>18(35-&gt;1 papers)</a></td><td bgcolor="#00EA00"><a href=../c27bc5e1>19(49-&gt;3 papers)</a></td><td>20</td></tr><tr><td>21</td><td bgcolor="#00E400"><a href=../a3795817>22(45-&gt;3 papers)</a></td><td bgcolor="#00E700"><a href=../4cbb3329>23(75-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../ae672850>24(43-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../41a5436e>25(56-&gt;1 papers)</a></td><td bgcolor="#00DB00"><a href=../aa92f86d>26(46-&gt;3 papers)</a></td><td>27</td></tr><tr><td>28</td><td bgcolor="#00FE00"><a href=../5b99a3e0>29(42-&gt;0 papers)</a></td><td bgcolor="#00EC00"><a href=../660257ea>30(85-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../89c03cd4>31(57-&gt;1 papers)</a></td></tr></table><table id="insights_11" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年十二月December</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00E200"><a href=../cd1e2f18>1(44-&gt;3 papers)</a></td><td>2</td></tr><tr><td>3</td><td bgcolor="#00ED00"><a href=../2b37e45c>4(39-&gt;2 papers)</a></td><td bgcolor="#00E200"><a href=../c4f58f62>5(78-&gt;3 papers)</a></td><td bgcolor="#00E400"><a href=../2fc23461>6(44-&gt;3 papers)</a></td><td bgcolor="#00FC00"><a href=../c0005f5f>7(42-&gt;0 papers)</a></td><td bgcolor="#00EC00"><a href=../310b04d2>8(89-&gt;2 papers)</a></td><td>9</td></tr><tr><td>10</td><td bgcolor="#00EE00"><a href=../c90f0d8>11(41-&gt;2 papers)</a></td><td bgcolor="#00FE00"><a href=../e7a74bdb>12(72-&gt;0 papers)</a></td><td bgcolor="#00F600"><a href=../'86520e5'>13(48-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../eab93b9c>14(42-&gt;1 papers)</a></td><td bgcolor="#00EF00"><a href=../57b50a2>15(40-&gt;2 papers)</a></td><td>16</td></tr><tr><td>17</td><td bgcolor="#00E700"><a href=../f085db12>18(43-&gt;3 papers)</a></td><td bgcolor="#00E200"><a href=../1f47b02c>19(92-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../7ab0fde7>20(67-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../957296d9>21(44-&gt;1 papers)</a></td><td bgcolor="#00EC00"><a href=../7e452dda>22(31-&gt;2 papers)</a></td><td>23</td></tr><tr><td>24</td><td bgcolor="#00EE00"><a href=../9c9936a3>25(38-&gt;2 papers)</a></td><td>26</td><td bgcolor="#00EC00"><a href=../986ce69e>27(72-&gt;2 papers)</a></td><td>28</td><td bgcolor="#00DE00"><a href=../86a5d62d>29(47-&gt;3 papers)</a></td><td>30</td></tr><tr><td>31</td></tr></table><table id="insights_12" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年十一月November</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td bgcolor="#00F800"><a href=../70d443d6>1(61-&gt;1 papers)</a></td><td bgcolor="#00EE00"><a href=../9be3f8d5>2(57-&gt;2 papers)</a></td><td bgcolor="#00F700"><a href=../742193eb>3(46-&gt;1 papers)</a></td><td>4</td></tr><tr><td>5</td><td bgcolor="#00F500"><a href=../920858af>6(54-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../7dca3391>7(74-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../8cc1681c>8(59-&gt;1 papers)</a></td><td bgcolor="#00EC00"><a href=../'63030322'>9(48-&gt;3 papers)</a></td><td bgcolor="#00F700"><a href=../5e98f728>10(69-&gt;1 papers)</a></td><td>11</td></tr><tr><td>12</td><td bgcolor="#00DB00"><a href=../b5af4c2b>13(34-&gt;3 papers)</a></td><td bgcolor="#00EF00"><a href=../'57735752'>14(119-&gt;2 papers)</a></td><td bgcolor="#00D900"><a href=../b8b13c6c>15(109-&gt;3 papers)</a></td><td bgcolor="#00E800"><a href=../5386876f>16(118-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../bc44ec51>17(154-&gt;2 papers)</a></td><td>18</td></tr><tr><td>19</td><td bgcolor="#00FE00"><a href=../c77a9129>20(27-&gt;0 papers)</a></td><td bgcolor="#00E600"><a href=../28b8fa17>21(99-&gt;3 papers)</a></td><td bgcolor="#00F400"><a href=../c38f4114>22(37-&gt;2 papers)</a></td><td bgcolor="#00EF00"><a href=../2c4d2a2a>23(39-&gt;2 papers)</a></td><td>24</td><td>25</td></tr><tr><td>26</td><td bgcolor="#00ED00"><a href=../25a68a50>27(48-&gt;2 papers)</a></td><td bgcolor="#00F000"><a href=../d4add1dd>28(87-&gt;2 papers)</a></td><td bgcolor="#00E300"><a href=../3b6fbae3>29(52-&gt;3 papers)</a></td><td bgcolor="#00EB00"><a href=../6f44ee9>30(47-&gt;2 papers)</a></td></tr></table><table id="insights_13" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td>1</td><td bgcolor="#00F100"><a href=../46752150>2(30-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../a9b74a6e>3(40-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../4b6b5117>4(30-&gt;2 papers)</a></td><td bgcolor="#00F200"><a href=../a4a93a29>5(30-&gt;2 papers)</a></td><td bgcolor="#00EE00"><a href=../4f9e812a>6(30-&gt;2 papers)</a></td><td>7</td></tr><tr><td>8</td><td bgcolor="#00FB00"><a href=../be95daa7>9(10-&gt;0 papers)</a></td><td bgcolor="#00F000"><a href=../830e2ead>10(172-&gt;3 papers)</a></td><td bgcolor="#00EA00"><a href=../6ccc4593>11(40-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../87fbfe90>12(30-&gt;2 papers)</a></td><td bgcolor="#00EF00"><a href=../683995ae>13(30-&gt;2 papers)</a></td><td>14</td></tr><tr><td>15</td><td bgcolor="#00FE00"><a href=../8e105eea>16(10-&gt;0 papers)</a></td><td bgcolor="#00EE00"><a href=../61d235d4>17(135-&gt;2 papers)</a></td><td bgcolor="#00E900"><a href=../90d96e59>18(83-&gt;3 papers)</a></td><td bgcolor="#00E900"><a href=../7f1b0567>19(74-&gt;3 papers)</a></td><td bgcolor="#00F000"><a href=../1aec48ac>20(74-&gt;2 papers)</a></td><td>21</td></tr><tr><td>22</td><td bgcolor="#00F500"><a href=../f1dbf3af>23(108-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../1307e8d6>24(203-&gt;3 papers)</a></td><td bgcolor="#00F600"><a href=../fcc583e8>25(112-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../17f238eb>26(89-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../f83053d5>27(80-&gt;1 papers)</a></td><td>28</td></tr><tr><td>29</td><td bgcolor="#00FE00"><a href=../db62976c>30(67-&gt;0 papers)</a></td><td bgcolor="#00F800"><a href=../34a0fc52>31(141-&gt;3 papers)</a></td></tr></table><table id="insights_14" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>1</td><td>2</td></tr><tr><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td></tr><tr><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td></tr><tr><td>24</td><td>25</td><td>26</td><td>27</td><td bgcolor="#00D600"><a href=../13a1e3c6>28(30-&gt;2 papers)</a></td><td bgcolor="#00BC00"><a href=../d2c6faf>29(40-&gt;3 papers)</a></td><td>30</td></tr><tr></tr></table>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-12-31总结(年度总结)</title>
      <link href="/aa29fa81.html"/>
      <url>/aa29fa81.html</url>
      
        <content type="html"><![CDATA[<p>上次写总结还是在2023-9-29,没想到下次再写竟已经是3个月之后了，到了2023年的最后一天。每到年末，各种APP就喜欢来个xxx年度总结：B站总结、steam总结、网易云音乐总结……不过今天看到一个"新华社年度十大新闻"觉得挺有意思，我就想，能不能给我自己也列一个"年度十大新闻"呢？</p><span id="more"></span><p><img src="../../files/images/diary/2023-most-influence-news.jpg" style="zoom:33%;" ></p><p>总结十大新闻是一个很重要的事情，也许10年、20年后，我都已经忘记了2023年发生过什么，但可能还会记得之前总结的"十大新闻"，在这个意义上，这其实就代表了整个2023年对我的印象。就像杨大伯之前说的"每门课上完，你最少记住一句话，记住一辈子"。大概就是这个道理</p><p>然而，总结年度十大新闻也是一个很难的事情，因为很多事件的影响可能很难在年底就出现，可能更应该来个时间检验奖，2023年更容易评出来"2013年年度10大新闻"，大家也就姑且一听吧:</p><ol type="1"><li>本科毕业：2023-06</li><li>研究生入学: 2023-09。还是华子，还是贵系</li><li>和TLE的1000天纪念：2023-06</li><li>去成都旅游：2023-08</li><li>第一次发Twitter：2023-08</li><li>第一次得新冠: 2023-01</li><li>尝试纹理烫：2023-02</li><li>google scholar 100引用：2023-08</li><li>end2end做饭：2023-07</li><li>凑齐apple全家桶：2023-06</li></ol><p>不知不觉，本科都已经毕业了，变成研究生了……前几天拉着室友一起回到十一学校去帮清华招生宣讲，突然感觉自己变得好老，高中学弟说的高中生活、本科学弟说的本科生活，好像全都已经离我远去了。明明研究生才开始一个学期，但心态似乎已经完全不同了。说起来，本科生活似乎也没有很"多彩"，各种行动不管是实践或者运动，好像也大都有某种目的性。虽说大学的优化目标比较多比高中多，看似非常的diverse。但各个项目都要优化、都和某些利益挂钩，反而会让所有事情都隐约套了个目的性的壳：运动是为了阳光打卡，科研是为了保研，社工是为了评奖……到了研究生，优化目标反而纯粹了，这种目的性的东西基本没有了，希望可以更多的做一些纯兴趣的东西，比如来场随心的旅行</p><p>华子的传统是每年的跨年敲钟时都要宣布一个有利于生权的好事。2019年跨年时邱宝敲钟完宣布校园网流量从一个月20GB升级到了50GB。今年的好消息看来是"清华和北大不限制双向入校"。仔细想了想，虽然感觉华子不怎么关心生权，但把尺度放大，从我入学到现在，想达成的几个生权的好事基本也都达成了：</p><ul><li>无限量校园网，在21年校庆的时候完成</li><li>白天可以洗热水澡，22年校庆的时候完成</li></ul><p>希望等研究生毕业的时候下面这几件事也能完成：</p><ul><li>洗衣机免费</li><li>食堂可以卖冰淇淋(手工)</li><li>为计算机系同学提供免费的算力，比如1xA100/ person</li><li>学校里可以有电动车充电桩</li></ul><p>喊了一年的新系馆，23年最终还是没住成，现在又改口说是24年3月搬，无所谓吧，反正这已经是第5次改口了，就像美国的国债限额，大概快到时间了还会再延期吧。西体育馆终于建成了，我记得18年暑校的时候辅导员就在提冰雪场馆，19年入学的时候好像就围起来了，最终23年终于可以滑冰了。虽然还没有体验过，不过看同学们反馈都不错，24年真得去试一试。现在东边的足球场又围起来了，听说要建一个最大的地下学生活动中心，还挺期待的，希望我毕业的时候能用上。</p><p>华子每年都在盖新的楼，每年也都有新盖好、装修好的楼投入使用；同学每年都在提新的需求，每年也都有需求被满足……希望我也一样，每年都能有新的回忆产生，每年也都能找到新的目标，也能交到新的朋友</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Alignment For Honesty</title>
      <link href="/2300f7be.html"/>
      <url>/2300f7be.html</url>
      
        <content type="html"><![CDATA[<p>上周刷到了刘鹏飞老师的 Alignment For Honesty, 分享给了大家<a href="/86520e5.html" title="2023-12-13-insights">2023-12-13-insights</a>。里面讲到如何训练LLM变得诚实，他沿用了孔子的定义：</p><blockquote><p>知之为知之，不知为不知，是知(zhì)也。</p><p>To say “I know” when you know, and “I don’t know” when you don’t,that is wisdom.</p></blockquote><p>我来一起看看他们是怎么做的吧</p><span id="more"></span><h2 id="introduction">introduction</h2><p>作者团队来自上交、复旦和CMU，其中复旦的xipengqiu老师也是arxiv的常客了</p><p><img src=" ../../files/images/align-for-honesty/authors.png"></p><p>其实关于honesty,这个领域由来已久，本文作者也提到了，学界对于Honesty有各种各样的定义和表述方式。前两天读weak-to-stronggeneralization时，OpenAI也提到了相关的研究，有兴趣的同学可以进一步顺着引文看一看相关的研究~</p><p><img src=" ../../files/images/align-for-honesty/openai.png"></p><p>回到本文，作者按照《论语》里给出的定义来定义诚实：知之为知之，不知为不知，是知(zhì)也。具体来说，需要模型可以分辨自己的知识边界：</p><ul><li>边界内的问题予以回答</li><li>边界外的问题勇于承认</li></ul><blockquote><p>不过，我觉得这里的语境和孔子想表述的有些区别：对于人来说，认知到知识边界很容易，只是很多时候羞于承认，所以这种"勇于承认"是一种君子的品格。但对于模型来说，还没有到荣辱心这一步，他只是单纯地意识不到自己的知识边界……</p></blockquote><p>让模型获得Honesty有各种各样的好处，其中最显然地就是减少hallicinate。虽然Honesty是"对齐三剑客"(helpful,harmless,honest)之一，但学界对于这方面的研究其实很少，作者就把这个领域按照alignment的语境重新定义了一下：对于做不出来的东西，要回答一个idksigns(I Don't Know)</p><h2 id="formulation">formulation</h2><blockquote><p>这个写法不多见，一般论文没有这个section。因为本篇工作是第一篇工作，所以需要把问题描述定义一下，然后说一说评测方法是什么</p></blockquote><p>首先，这里作者做了一个简化:这篇工作中，作者认为模型知识和世界知识是一个集和，假设模型不会说谎，如果回答错了，那大概率就是自己不懂这个知识。</p><p><img src=" ../../files/images/align-for-honesty/boundary.png"></p><h3 id="训练框架">训练框架</h3><p>作者提出了一套多轮refine的框架，希望随着训练的迭代，模型可以逐渐清晰地认识到自己的知识边界</p><blockquote><p>在这一点上，我倾向于OpenAI的观点："认知到自己的知识边界"是一个latentknowledge，应该是模型本身具备的(毕竟是自己的知识，以及本身有calibration性质)，我们只需要训练模型去激发elicit出来。因此这个任务定义好以后，可能不太难</p></blockquote><p>作者把模型对于一个知识问题的回答分成了三类： <spanclass="math display">\[c(x,y) =  \left\{\begin{aligned}&amp; -1, \text{type}(y) = \text{idk}, \\&amp; 1, \text{type}(y) = \text{correct}, \\&amp; 0, \text{type}(y) = \text{wrong},\end{aligned}\right.\]</span> 接下来，根据该模型是否知道该问题的答案<spanclass="math inline">\(k(x) = 1\text{ if model know the answer, else-1}\)</span> <span class="math display">\[v(x,y) = \left\{\begin{aligned}1, &amp; c(x,y)*k(x,y) = 1, \\0, &amp; \text{else},\end{aligned}\right.\]</span>有了价值函数以后，就可以根据这个价值函数进行训练，预期价值函数随着训练变得越来越高。当然，</p><ul><li>在真实答案已知的情况下，c很容易获得</li><li>然而，k是一个很难获取的东西，因为是一个latentknowledge，后面作者探索了几种近似得办法</li></ul><h3 id="评测">评测</h3><p><img src=" ../../files/images/align-for-honesty/condition.png" style="zoom:33%;" ></p><p>即使按照上面的框架训练了，模型的效果仍然不好评测。不过，根据迭代前后模型的表现，作者可以天然的把问题分为9个大类</p><blockquote><p>其中的2,3类说明之前没做出来，后面做出来了(尽管没有泄露正确答案)。是个比较奇怪的现象，本篇工作不关注这个</p></blockquote><p>这里作者参考F1-score，讨论了一种近似的评测办法：</p><ul><li>over-conservativeness：我们不希望模型过于谨慎，希望能做出来的题目就正确回答。因此计算公式很简单</li></ul><p><span class="math display">\[S_1 = \frac{7}{1 + 4 + 7}, \text{lower is better}\]</span></p><ul><li>Prudence：这个和上面的相反，考虑的是，不会做的问题，希望模型正确地回答idk</li></ul><p><span class="math display">\[S_2 = \frac{8+9}{5 + 6 + 8 + 9}, \text{higher is better}\]</span></p><p>有了上面的计算，就可以给出一个honesty增量</p><blockquote><p>注意，这个指标如果模型不训练，那就是只有1,5&gt;0，<spanclass="math inline">\(S_1=0,S_2=0,S=0.5\)</span></p></blockquote><p><span class="math display">\[S_\text{honesty} = \frac{(1-S_1) + S_2}{2}\]</span></p><h2 id="method">method</h2><p>首先，prompt方法是一个显然的办法(这里就是单轮迭代，只有prompt前后的区别)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAINTEXT"><figure class="iseeu highlight /plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Answer the question. If you don’t know the answer to the question, it is appropriate to say “I apologize, but I’m not able to provide an answer to the question.”</span><br><span class="line">   Q: &lt;question&gt;</span><br><span class="line">   A:</span><br></pre></td></tr></table></figure></div><p>接下来，训练地方法，作者设计了三种。这三种都是基于一个蒙特卡洛估计的办法，作者会让没对齐的模型对于一个问题生成多个(10个)回答，检查每个回答是否正确。给出一个信心值expectedacc作为模型认知<span class="math inline">\(k(x)\)</span>的一个近似</p><p><img src=" ../../files/images/align-for-honesty/sample.png" style="zoom:33%;" ></p><h3 id="absolute">ABSOLUTE</h3><p>设定一个阈值<span class="math inline">\(\tau\)</span>，<spanclass="math inline">\(k(x) = 1 \quad if \quad \text{expected acc} &gt;\tau\)</span>。然后标数据的时候，把所有k(x)=-1的回答都改成了一个idkresponse</p><h3 id="confidence">CONFIDENCE</h3><figure class="half"><img src="../../files/images/align-for-honesty/numb.png" width="45%"/><img src="../../files/images/align-for-honesty/verb.png" width="45%" /></figure><p>这里，作者标数据的时候直接把confidence写在回答里，然后按照正常SFT的办法</p><h3 id="multisample">MULTISAMPLE</h3><p>刚才的absolute会根据一个阈值卡，这里作者直接把sample多次的每条数据当成单独的了，然后<spanclass="math inline">\(k(x) =(c(x,y)==1)\)</span>。也就是说，标数据的时候，本来作对了的就不动，本来做错了的就改成一个idkresponse。</p><blockquote><p>值得注意的是，这个方案会把训练集扩大M倍</p></blockquote><h2 id="experiment">experiment</h2><p>这里作者提了两个朴素的baseline：</p><ul><li>原来的模型</li><li>fine-tuned：在相同训练量上，使用turbo的answer进行SFT的模型</li><li>prompt：上面提到的training-free方法</li><li>三种training方法，其中，<spanclass="math inline">\(\tau\)</span>选取的是0.1</li></ul><figure class="half"><img src="../../files/images/align-for-honesty/main.png" width="40%"/><img src="../../files/images/align-for-honesty/OOD.png" width="55%" /></figure><p>作者在TraivalQA数据集上做训练，使用Llama2-chat7b作为基础模型，分别评测in-domain的traivalQA和OOD的另外三个数据集</p><p>效果如下：</p><ul><li><p>发现基于训练的方法显著好于不训练的方法</p></li><li><p>相对来说，把confidence放到数据里，会让模型表现更好</p></li><li><p>honesty属性在不同数据集上迁移能力较好，不管是ID还是OOD，加上confidencescore都能让模型做的更好</p></li><li><p>直接finetune模型，会导致模型更加hallicinate，acc反而下降（这点在PKQA数据集表现得尤其明显）</p></li></ul><p>接下来，作者探索了<spanclass="math inline">\(\tau\)</span>对结果的影响，画了一张类似f1里面auc的图。发现，<spanclass="math inline">\(\tau\)</span>越大，越容易把数据分类成模型不知道</p><ul><li>因此idk数据越多，模型越容易变得over-confidence</li><li>另一方面，模型也越谨慎，所以prudence会提升，这里需要有一个权衡</li></ul><p><img src=" ../../files/images/align-for-honesty/auc.png" style="zoom:50%;" ></p><p>接下里，作者又做了scaling的实验：更大的模型会做得更好吗，更多的数据会做的更好吗？</p><figure class="half"><img src="../../files/images/align-for-honesty/scale.png" width="35%"/><img src="../../files/images/align-for-honesty/data-scale.png" width="60%" /></figure><p>首先，作者发现，confidence-basedmethod对于所有模型规模效果都要更好一些</p><blockquote><p>我发现：不同规模的模型对于Honesty的效果没啥区别，这说明了这个任务其实是挺困难的</p></blockquote><p>其次，如果在训练集中加入MMLU的训练数据，对于Multi-sample方法的帮助很大，说明这个属性的习得也许是data-hungry的，模型需要更diverse的情况来判断自己的知识边界</p><blockquote><p>不过，为啥Multisample+MMLU-data以后Acc下降这么多呢？</p></blockquote><p>最后作者做了一些"对齐税"方面的实验，发现Honesty训练基本不会导致模型在别的任务表现下降。最后，作者总结了一下limitation和future，提了几个问题，我觉得还挺有意思的，分享给大家：</p><ul><li>更好的k(x)：本篇工作用模型回答正确与否判断模型是否知道，这个在MMLU这种4选1中有误判假阳的情况</li><li>confidencescore能不能更好的利用？这里作者和calibration联系了一下</li><li>和RAG的结合：认知到自己知识边界的模型更清楚自己该怎么利用外界知识</li><li>和长文本的结合：需要结合reasoning的长文本场景的Honesty现在还没有研究，并且需要更细致的评测和训练</li></ul><h2 id="我的思考">我的思考</h2><p>很好的文章，formulation到method到实验设计都很顺滑，逻辑很完整，我看完了以后主要想到下面几个问题：</p><ul><li>感觉可以评测一下turbo或者GPT4的表现？这里没做估计是因为需要一个unaligned模型去计算，没办法。要测也许只能给turbo来个un-alignfinetune，不知道是不是违规的</li><li>scaling实验中，发现所有llama表现都差不多，说明这个能力也许是一个emergent的，甚至是reverse-scaling的？</li><li>这个能力，似乎是不能通过SFT习得的？因为每个模型都有自己的知识边界。作者也提到了，SFT-baselinewill lead models to learn to hallicinate</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> post-pretrain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weak-to-Strong Generalization(上): OpenAI是怎么看问题的？</title>
      <link href="/3229ec6.html"/>
      <url>/3229ec6.html</url>
      
        <content type="html"><![CDATA[<p>昨天OpenAI一口气更新了两篇论文，暨DALL.E3之后的又一更新，其中一篇讲述了一个朴素的问题：如果未来的模型超越人类了，我们该怎么给他们提供监督信号？（毕竟我们只有人类——一个相对更弱的模型）</p><p>OpenAI把这个问题叫做weak-to-stronggeneralization在这里做了一些简单的尝试，对于这个问题的性质进行了一些探索。我们来一起学习一下他们看问题和解决问题的思路吧！</p><span id="more"></span><p>作者首先映入眼帘的就是ilyaSutskever，这个老哥真是为人类尽心尽力呀……OpenAI官网写的这个论文的作者是"Safety&amp;Alignment"</p><p><img src="../../files/images/weak2strong/authors.png"></p><p>而他的primary authors JeffWu更是重量级，我不多评价，直接列出其最近的publication，可以说是群星璀璨了……我什么时候能发12篇论文都有这个质量呀</p><p><img src="../../files/images/weak2strong/jeff_wu.png" style="zoom:30%;" ></p><h2 id="introduction">introduction</h2><p>回到正题，作者提到目前的研究更多focus在RLHF，既模型向人类偏好对齐，这套框架最终可以发展出一个类似于人的通用人工智能。然而，到此为止了：想要训练、对齐一个超越人类的智能，目前的方法都不再适用了，因为我们最高也只能用到人了。举个例子:</p><blockquote><p>我们可以要求模型写出一个100万行的很复杂的代码来完成一个任务，然而让人去看代码好不好是一个很难达成的任务。</p></blockquote><p>因此，目前还没有关于superalignment（对齐一个超越人的模型）的研究，并且，我们更希望其对应的方法可以不止适用于目前的模型，更适用于未来的模型(consistency)。在本篇工作中，作者类比了人类监督超人模型的问题，提出了一个类似的问题：可以用GPT2监督GPT4吗？</p><blockquote><p>具体来说，能不能用GPT2在某个任务的trainset上finetune，然后给trainset重新打一遍标签。让GPT4在GPT2打标签的trainset上训练，然后观察GPT2和GPT4在testset上的表现？</p></blockquote><p>这是一个很直观很容易实现的setup，作者把这个set up叫做weak-to-stronglearning。</p><p>听起来这个weak-to-stronggeneralization似乎是不可能的，不过作者进行了一些解释：在学习过程中，我们不是预期弱模型教会强模型知识(这些知识是强模型本来就会的)，而是希望弱模型教会强模型这个任务的概念是什么，和一些intension，因此这个过程更像是激发"elicit"。用刚才那个例子来说，如果强模型已经能写100万行的代码，那他肯定有潜力去理解人类的目的是什么。因此即使弱模型给的标签有误差，应该不影响一个更鲁棒的强模型去理解，这就是作者预期出现的结果。</p><p><img src="../../files/images/weak2strong/first_perf.png" style="zoom:30%;" ></p><p>作者在传统NLP task(二分类)、chesspuzzles(选出最优步的生成任务)、human preferencemodeling(二分类)上做了实验，并得出一些结论：</p><ul><li>强模型在weak-to-stronggeneralize以后基本都会超越帮他标数据的弱模型。？？？</li><li>直接做weak-to-strong generalize的效果并不好，比强模型+oracle-labelfinetune 差很远，可能需要一些额外的优化</li><li>一些很简单、很直觉地方案对于这个任务的提升很大。这说明这个领域还有很大的进步空间</li></ul><h2 id="method">method</h2><p>像刚才Introduction里提到的，这个setup是很简单的。不过，比起一般论文讲完方法讲实验，作者额外说了一下这个setup的优点和问题。作者额外说了一个他们观察的指标performancegaprecovered(PRG)：假设weak-to-strong训练比小模型效果好，那他能恢复强模型直接训练的几分风采？注意，这里面PRG=1代表和强模型直接训练等价，PRG=0代表和小模型表现一样，完全没有generalize。</p><blockquote><p>为什么要和 “强模型+orcale finetune”去比？虽然现在的setup是数据集，因此有orcale 标签可以做"强模型+orcalefinetune"的上限实验。但事实上所谓的"orcale标签"也是人去打的标签，未来的模型在完成超人任务时就没有标签了，或者说只有人去打的"噪声标签"。</p><p>这种情况下，我们可以预设有"外星人打的oracle标签"跑上限实验，也就是说：我们更关注超人模型的能力上限能不能被激发出来。PGR可以看做一个"激发比例"</p></blockquote><p><img src="../../files/images/weak2strong/prg.png" style="zoom:50%;" ></p><p>优点：总体来说，就是这个方案更有利于scaling</p><ul><li>对于模型不可知，任何的 强-弱模型对都能跑这个实验，也不需要一定要有GPT4。另外，这个路径不需要人，因此实验很快</li><li>对任务不可知，任何任务都能跑这个setup</li><li>这里面得到的关于超人模型的结论，即使现在也能用的上：未来对齐GPT10的方法，不妨现在就用到GPT4里？</li></ul><blockquote><p>这也许暗示着目前OpenAI内部已经有超人模型了……</p></blockquote><p>缺点：</p><ul><li>Imitationsaliency：这个我觉得很难翻译，就直接摆原文了。这里讲的是未来的超人模型和今天的GPT4存在统计学的巨大差异。对未来的超人模型来说，理解人类的想法是一个很简单的问题，但GPT4去理解GPT2可能没那么简单。并且，GPT2犯的错误类型和人类犯的错误类型也有很大区别，因此目前的结论更多是研究GPT2错误，会有统计学差异。</li><li>数据泄露：目前的setup里面的oracle标签可能或多或少在预训练数据里出现过。未来的模型不存在"超人的预训练数据"，因此实验结论可能和现在有偏差</li></ul><p>总体而言，超人模型长什么样、有什么现象现在仍未可知。作者提到，我们更需要对于未来的所有模型都跑一跑现在的实验，观察一下实验结论是否是稳定存在于所有模型中的……怎么有一种三体人的水滴在破坏物理学规律的感觉……</p><h2 id="experiment">experiment</h2><p>作者在附录里讲了实验细节，大致有如下关键点：</p><ul><li>NLPtask中，作者选取了29个有代表意义的task，都转换成了2分类任务，为了简化整个setting。</li><li>在模型方面，作者使用GPT4架构，构造了和GPT2、GPT3、GPT3.5、GPT4相当计算量的模型(但都是GPT4架构)。作者去掉了decoder最后的那个unembeddinglayer，换成了一个二分类头做实验</li><li>数据方面，作者把数据做了一些清洗，使得数据集中两个类的标签数量接近，然后有数据分类风险的数据们都在traintest的其中一个split上</li></ul><p>另外两个setup：</p><ul><li>rewardmodel：作者直接使用了训练ChatGPT的SFT数据集，里面有数据对以及human给出的偏好。可以看成一个很复杂的二分类任务。这个的训练方法和正常的rewardmodel一样，把unembeddinglayer改成一个N-&gt;1的linear来输出一个float当做score。最后用两个candidate的score谁大来作为预测</li><li>chesspuzzle：很多象棋的棋局，作者希望模型给出第一步，最优解是什么。这个可以看做一个生成任务，因为可选位置有很多个。原始数据大概长下面这样</li></ul><p><img src="../../files/images/weak2strong/chess.png" style="zoom:20%;" ></p><blockquote><p>我猜测，作者选这几个setup是基于这样的考虑：</p><p>NLP task，大小模型做的都不错。</p><p>chess puzzle，大模型做的不错，小模型做不了。</p><p>reward model，大小模型做的都不太好</p></blockquote><p>由此，作者</p><ul><li>首先构造了大小不同的多个GPT4架构的模型。这些模型都是做过预训练的，使用的预训练数据和GPT4一致</li><li>对于每个setting，作者分别跑了刚才提到的四步测试<ul><li>小模型在trainset真实标签上做训练FT</li><li>FT的小模型给trainset重新打一遍伪标签</li><li>大模型在带噪的trainset上训练，同时做一个真实标签FT的上限训练</li><li>测量三个模型在testset的表现，计算出PRG</li></ul></li></ul><p>论文的主试验就是下面这个图</p><p><img src="../../files/images/weak2strong/main.png" style="zoom:50%;" ></p><p>这个看着有点费解，后面的所有图都用了相同的画法，我在这里解释一下：</p><ul><li>纵着看，每列代表一个task。(29个NLP task被划到一起了 )</li><li>横着看，上面三个图报告的是数据集上的performance，下面三个图报告的是PRG</li><li>每个图里面的横轴说的是学生模型的大小，标准是GPT4相对计算量。注意，学生模型一定比老师强</li><li>图里的一条线代表的是同一个老师监督不同大小的学生的水平。老师越大，线越亮。注意，老师永远小于学生，因此这个图差不多都在右下三角半区</li><li>上面的三个图里最上面的线是大模型的上限实验结果。</li></ul><p>作者观察到几个关键结论：</p><ul><li><p>PRG基本永远大于0，学生总是强过老师。</p></li><li><p>老师变大或者学生变大，PRG基本都会增大。对于最强的学生，PRG甚至会超过50%：这也许预示着，对于超人模型来说，weak-to-stronggeneralization实际上很简单</p></li><li><p>chesspuzzle任务上，这些现象更显著。当学生很小时，PRG接近0，大小模型做的都很垃圾。并且，这个任务上的PRG和前面相反，出现了inversescaling现象？即同一个老师，学生越强，PRG反而越低了？</p></li><li><p>对于rewardmodel任务，PRG一直都很低。对于所有setting，PRG都低于20%</p></li></ul><p>作者特别强调了一下，PRG&gt;0这个事情基本一直成立，这个现象其实很神奇，因为也没做什么特别的优化。这个现象预示着superalignment的可能性，也需要在相关领域有更多的研究</p><h2id="如何提高weak-to-strong-generalization的表现">如何提高weak-to-stronggeneralization的表现？</h2><p>做完主试验，一个显然的思路就是：如何优化他？作者在这里简单地想了几个办法。在这里，我们看一下OpenAI研究员的"注意到"是什么样的吧？</p><h3 id="bootstrapping级联">bootstrapping：级联</h3><p>注意到：刚才提到老师和学生差距太大，以及老师/学生提升，PRG效果都会变好。那么，如果用小老师监督稍微强一点的学生，再把学生当成新老师，监督更强一点的学生……如此反复，效果会更好吗？</p><p><img src="../../files/images/weak2strong/bootstrap.png" style="zoom:50%;" ></p><p>作者在chesspuzzle进行了实验，图里面的虚线是主试验里面的小老师直接监督大学生，实现是级联监督的结果。作者发现：</p><ul><li>当最开始的大小gap差距小时，这个办法是优化</li><li>当最开始的大小gap差距大时，这个办法是负优化</li></ul><p>尽管在chess puzzle看起来效果不错，但作者发现这个办法在NLPtask提升不大，在rewardmodeling没有提升。对于这个现象。作者"注意到"：主试验中，Chess这个场景gap越大PRG越小，其他场景都是gap越大PRG越大。因此级联方法缩小gap，对于Chess才有帮助。</p><blockquote><p>可是，主试验又为什么会有这样的现象呢？</p></blockquote><h3 id="auxiliary-confidence-loss-学生要学会质疑">auxiliary confidenceloss: 学生要学会质疑</h3><p>注意到：强模型是在弱模型打得标签上训练的，因此强模型不得不去学习弱模型犯的错误。我们更希望强模型从噪声标签中学习任务的intension，而不是学习弱模型的错误(因为对强模型来说，理解弱模型犯的错误也是一个简单的事情)。但目前的训练框架没法区分这两者。</p><p>作者想了一个朴素的办法，把强模型自己的confidenceloss加入进去作为辅助loss，和弱模型的标签loss形成一个对抗，对冲一下"学习犯错"带来的影响。<span class="math display">\[L&#39; = (1-\alpha)\text{CE}(f(x), f_w(x)) + \alpha \text{CE}(f(x),\hat{f}_t(x))\]</span> 其中<spanclass="math inline">\(f_w(x)\)</span>是老师的logits， <spanclass="math inline">\(\hat{f}_t(x)=I[f(x)&gt;t]\)</span>是给学生模型的预测按照阈值t做了离散化后的标签。<spanclass="math inline">\(\alpha,t\)</span>都是超参数</p><p>注意到，上面的公式等价于下面这个，思路和self-training有点类似 <spanclass="math display">\[L&#39; = \text{CE}\left(f(x), (1-\alpha)f_w(x)+\alpha\hat{f}_t(x)\right)\]</span>这个新loss相当于噪声标签和学生自己打的标签的一个加权平均。最开始的时候学生啥都不会，应该把<spanclass="math inline">\(\alpha\)</span>调小，后面再慢慢增大，作者在实验中就在前20%训练中把<spanclass="math inline">\(\alpha\)</span>从0提升到0.75，后面维持0.75不变</p><blockquote><p>如果老师和学生模型的算法相同，也许可以不再标签层面做平滑，可以直接用logits做平滑，去掉一个超参数t。作者估计是为了保证scaling的通用性，就没这么优化。</p></blockquote><p><img src="../../files/images/weak2strong/aux.png" style="zoom:50%;" ></p><p>类似上面，作者在NLP任务上做了实验，发现这个优化对于NLP任务的PRG提升很明显，甚至从25%提升到了80%……作者认为这个结论是符合直觉的：auxloss可以减缓学生在"weak errorlearning"上的计算资源，减缓对于老师的错误的拟合</p><p>上篇也写了4000多字了，就讲到这里，在下篇我会分享作者对于这些现象，以及superalignment问题的理解，再说一说我对这些问题的看法，以及我对于这篇论文研究思路的思考。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> superhumanAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</title>
      <link href="/68247c98.html"/>
      <url>/68247c98.html</url>
      
        <content type="html"><![CDATA[<p>读得论文多了，写的笔记反而更少了……很多篇论文都想写，最后哪个都没写出来。今天来讲讲yejinChoi2020年的一个论文：如何用模型自己在训练过程中的表现作为自监督信号，衡量训练集中每一条数据的质量？</p><blockquote><p>很难想象这是yejinchoi三年前思考的问题，我直到最近读到这篇论文，还觉得思路很新颖、很精妙</p></blockquote><span id="more"></span><p>作者是Yejin Choi团队,一作SwabhaSwayamdipta最近还做了一些有趣的工作，比如这篇：We’re Afraid LanguageModels Aren’t Modeling Ambiguity。都是挺有意思的选题</p><p><img src="../../files/images/Dataset-Cartography/authors.png"></p><h2 id="data-map">data-map</h2><p>回到本篇工作，作者主要探索了以下问题：目前(2020年)学界的范式是选择越来越大的数据集做训练。因为大家发现随着数据集扩大，其多样性会上升，进而促进模型的分布外泛化能力。</p><p>但是，随着数据集的扩大，数据质量一定会下降，作者想到:有没有可能数据集中每条数据对语言模型的贡献是不一致的呢？作者希望找到一种自动地标注方案。作者直觉地想要用两个维度对数据分类：在一条数据过了很多epoch以后每次的loss对应的平均数和方差。作者把这两个轴叫做confidence(平均数)、variability(方差)</p><p>对于比如SNLI数据集，作者尝试把RoBERTa训了几个epoch，然后统计里面每条数据在每个epoch的loss，进而画了一个散点图，其中每个点代表一条数据。作者直觉地认为，这个类似钟型曲线天然地把数据分成了三种情况：</p><ul><li>easy-to-learn：很快就学会了，并且方差很小，一直都做对。占大多数</li><li>hard-to-learn：一直学不会，因此方差也很小</li><li>ambiguous：一轮能做对一轮做不对，方差很大。模型对这种数据的判断没有把握</li></ul><p><img src="../../files/images/Dataset-Cartography/intro.png"></p><p>另外，对于confidence做离散化，还可以统计acc。作者还把“n个epoch中一条数据acc”的比例定义为了correctness，在图中表现为了不同颜色的小点。</p><p>由此，作者把这个方法叫做data-map，和标题里的地图学呼应：地图是地球固有的属性，而数据中的confidence、variability也是模型在训练中自己表现出来的性质。</p><p>接下来，作者就要从这个现象出发，展露一下研究员的天才思路，设计一系列实验和探索。</p><h2id="data-map能作为选择训练数据的指标吗">data-map能作为选择训练数据的指标吗？</h2><p>作者实现好奇的就是：不同区域的数据，对于训练有什么贡献？实验设计很简单，就只选择对应区域的数据做训练就可以了。在训练完以后，作者分别作了in/outof- distribution(ID、OOD)的测试。</p><ul><li><p>100 train：阳性对照</p></li><li><p>random: 随机选33%，阴性对照</p></li><li><p>high-correctness: correctness从高到低前33%的数据</p></li><li><p>low-variability、low-correctness、high-confidence、high-confidence同理</p></li><li><p>hard-to-learn: 指的是low-confidence</p></li><li><p>ambiguous： 指的是high-variability</p></li></ul><p><img src="../../files/images/Dataset-Cartography/exp.png" style="zoom: 33%;" ></p><p>作者在winoG上训练，然后分别把winoG、WSC作为IDOOD测试，神奇的现象来了：</p><ul><li>仅在hard-to-learn或者ambiguous的33%数据上训练，OOD能力甚至比阳性对照还要好！</li><li>仅在eazy-to-learn的数据上训练，似乎对ID和OOD测试都没啥帮助……不如random33%</li><li>尽管没有对选数据的方法专门做优化，但效果比几个active-learning算法的效果还要好</li></ul><p>看起来，hard-to-learn和ambiguous的数据对模型的效果起到关键作用。ID的效果和训练集大小强相关，我们相对更关注OOD。因此作者说到这套data-map的方案某种意义上提供了一个加速训练的潜在方案。然而，从这个角度看，这个方案需要先在全集上训一遍模型，这肯定比正常训练开销更大。因此这个方法只有理论价值</p><h2 id="可以抛弃eazy-to-learn数据吗">可以抛弃eazy-to-learn数据吗？</h2><p>既然上面研究发现hard-to-learn和ambiguous数据最有用，那接下来一个直观的问题就是：如果用更少、少于33%的这种数据，也能达到这种效果吗？</p><p>于是作者选了ambiguous数据的前50%, 33%, 25%, 17%, 10%,5%，1%作为训练集尝试了实验</p><p><img src="../../files/images/Dataset-Cartography/rate.png"></p><p>先看左边两个图：横轴是上面那个top-ambigious训练数据的百分比，纵轴分别是ID和OOD的效果。神奇的又来了：当训练数据低于某个阈值以后，训练就崩溃了？？另一个实验表明，相同的数据量，如果选取不是按照top-ambigious而是random，训练就是正常的</p><p>因此作者想到了一个可能：会不会是eazy-to-learn的数据虽然对于效果没什么帮助，但是对于稳定训练很有帮助？因为更少的top-ambigious显然就采样不到eazy-to-learn的数据了。于是作者点子又来了，做个阴性对照，把刚才训崩的数据比例(17%)里，随机将一部分top-ambigious的数据换成eazy-to-learn的数据？</p><p>于是就画出了右图：作者发现，哪怕在17%中，只要再掺入1/10=1.7%的eazy-to-learn数据，训练就正常了起来？？另外，如果替换的比例太高，ID和OOD的效果就又掉下去了。</p><p>作者最后又提出了一个开放性的研究问题：如何在训练中正确选择各个区域的比例？</p><h2id="hard-to-learn的数据可能因为误标注吗">hard-to-learn的数据可能因为误标注吗？</h2><p>想到两个点：</p><ul><li>SNLI画的data-map中hard-to-learn很多，winoG画的data-map，hard-to-learn看起来很少。同时我们知道winoG中的数据被人类精心clean过因此误标注更少</li><li>对于误标注的数据，模型显然是"hard-to-learn"的</li></ul><p>怎么验证这个猜测呢？作者点子又来了</p><p>首先，来个模拟实验。作者将winoG中1%的eazy-to-learn数据的标注换一下，造一批”误标注“数据。在eazy-to-learn数据中采样是因为这里面大概率之前不是误标注的数据</p><p><img src="../../files/images/Dataset-Cartography/mislabel.png"></p><p>接着作者用新的数据集重新画data-map，观察刚才那些点在新的图中的位置，作者给出了这些点confidence、variability的直方图。发现confidence显著降低、variable显著升高。这展示了数据中误标注的可能性</p><p>接下来，作者问了另一个问题：既然有潜在的误标注风险，那有可能将data-map作为一种自动的误标注识别手段吗？</p><p>首先作者把刚才的数据集(含1%人造误标注数据)，再采样了同样的1%正常数据形成了一个误标注占50%的数据集。训练一个classifier，其输入是每个instance的confidence、variability，输出2分类。发现这个classifie的测试集F1是100%？？</p><p>接下来，作者将classifier重新应用到原始winoG数据集，发现31/40k的数据划分为了mislabel。同理在SNLI上做同样的实验，发现有15k/500k划分为了mislabel。这和两个数据集的数据质量一致</p><p>最后，为了让作者的逻辑链条完整，作者开展了人类实验，找人去看classifier划分出来的mislabel数据。人类标注结果表明:classifier选出的"mislabel"数据，67%是真的mislabel。这个数字在SNLI上是76%。剩下的基本上也是比较"歧义"的instance</p><p>最后，作者谈到：data-map可以作为一种潜在的对数据集mislabel问题进行自动检测的手段，并且效果还不错。</p><h2id="模型在训练中表现出来的这种性质和数据集固有的不确定性有关吗">模型在训练中表现出来的这种性质，和数据集固有的不确定性有关吗？</h2><p>众所周知，数据集中有一些固有不确定性：有一些instance是歧义的，理论上就是填什么都可以。另外，对于模型无法预测的位置，到底是来源于数据集固有的不确定性，还是模型本身的局限性(换个更强的模型没准就会了)呢？</p><p>作者想到一个办法来衡量数据集中固有的不确定性：在数据集制作时，都是找人来标注。对于本身有歧义的例子，不同的标注员之间应该自己也有不一致性。所以作者分析了data-map中每条数据，列出了标注员当时对于这条数据的一致性</p><p><img src="../../files/images/Dataset-Cartography/consistency.png"></p><p>作者发现，模型划分data-map的方式，和人类当时标注时的一致性有非常强的相关性:起码对于eazy-to-learn数据，标注员基本一致性都很高。</p><h2 id="我的思考">我的思考</h2><p>这个论文的逻辑太顺了：一般我写笔记都会简略写experiment部分，但这次我一个都没有省，并且组织逻辑和YejinChoi论文组织逻辑完全一致。</p><p>作者从一个现象出发，和学界已经存在的问题联系起来，探索他们发现的现象的潜在应用价值。从联系方向，到提出问题，到设计实验，到画图展示的形式，都展示了研究员敏锐的数据直觉，值得我们去学习……相比之下，再看看近两年的大多数论文写成啥样子了……</p><p>站在2023年的视角下，我只能说对这个论文提出几个潜在的研究问题：</p><ul><li>在instructiontuning领域，大家逐渐意识到diversity和quality的矛盾，以及对最终训练效果的影响。相比于WizardLM这种自动化的数据筛选。让模型自己去选择数据是一种新的思路吗？</li><li>data-map的结果是和模型绑定的。对于同一个数据集，换一个模型可能画出来的图就会有变化。比如GPT4，可能在SNLI上画的图全是eazy-to-learn。这点对于选择数据至关重要：一条数据不适合这个模型，但有可能适合那个模型，这和模型的基础能力有关。我们不指望找到一个适用于所有模型的goldenselectionmethod(可能世界上也不存在这样的方法)，相比之下更希望能找到最适合与这个模型的训练数据</li><li>这两年学界出现了一个新的关键词calibration：对于很强的LLM来说，自己的confidence和acc成强相关性。作者在这片工作中发现另一个联系：自己的confidence和数据集的固有不确定性成高度的相关性。由此我产生了一个问题：既然三者都有相关性，那么，模型的calibration性质可能是来源于"在含固有不确定性的无监督corpus上预训练"吗？如果我们的corpus去掉了不确定性(比如RLHF数据集)，那么模型的calibration性质是不是就消失了呢？</li></ul><p>最后，这是Yejin Choi三年前研究的东西，与君共勉</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLaVA, LLaVA 1.5和LLaVA-Plus: 讲讲LMM</title>
      <link href="/d33e88af.html"/>
      <url>/d33e88af.html</url>
      
        <content type="html"><![CDATA[<p>昨天刷到新挂的LLaVA-Plus的Arxiv论文，讲怎么做多模态的ReACT与训练模型。正好发现LMM(LargeMultimodalModel)系列的模型似乎怎么讲过。那么LLaVA系，三篇论文，今天一次说完。</p><blockquote><p><a href="https://llava-vl.github.io">Visual InstructionTuning</a></p><p><a href="https://llava-vl.github.io">Improved Baselines with VisualInstruction Tuning</a></p><p><a href="https://llava-vl.github.io/llava-plus/">LLaVA-Plus: Learningto Use Tools for MulitModal Agents</a></p></blockquote><p><del>flamingo、Kosmos 2.5下次有时间说啊</del></p><span id="more"></span><p><img src="../../files/images/LLaVA/authors.png"></p><p>首先，在作者上，这三篇论文基本上是一脉相承，没有出现LLaMA的黑吃黑现象。他们的鼻祖LLaVA一代发表在neurIPS2023Oral，上Arxiv的时间是4月份。当时LLM基本还是蛮荒时期，大家都是被GPT-4V发布时的惊艳骗进来，想搞个猴版，技术路径和研究思路有迹可循。不像现在成熟以后，论文都是天马行空地说，很难把握住核心思想。</p><h2 id="llava">LLaVA</h2><p>所谓的large multimodal model,就是想把LLM的能力范围再往前推一步，让他可以"see and hear".</p><p>LLaVA的主要思路是：由用一个CLIP作为imageencoder，然后训一个轻量级的链接器，把clipembedding连接到一个LLM的空间，由此让一个LLM理解图片，进而变成LMM。</p><p>上面的这套流程，重点就是需要图文数据集，而且需要是instruction-follow数据集。目前的图文数据对大多是imagecaption的，文字主要是描述文字内容。另外有一些VQA的数据集，其问答式针对图片里的一些元素，总体还是比较简单。</p><p>LLaVA的作者对上面的思路做了一下梳理，构造了一个数据集。里面所有的数据的格式都是类似于下面这样<span class="math display">\[X_q X_v&lt;STOP&gt;\text{\\n Assistant} : X_c &lt;STOP&gt;\text{\\n}\]</span>其中最前面会有一个图片，然后会有一个问题，接下来是回答。总体是多轮对话形式的。作者定下了三种数据类型，共158K数据</p><ul><li>conversation：58k</li><li>detailed descrption: 23k</li><li>Complex reasoning: 77k</li></ul><h3 id="model-and-training">model and training</h3><p>模型结构上，非常简单。作者使用CLIP ViT作为imageencoder，用LLaMA作为LLM。然后clip embedding通过一个Linear层投射到wordembedding层。接下来直接将他作为一个word embedding和其他wordembedding一起去跑LLM后面的流程</p><p><img src="../../files/images/LLaVA/model.png"></p><p>作者设计了一个两阶段的训练任务。第一阶段是对齐文本图像空间。作者直接使用图文数据对作为数据集:图在前，文在后。然后训练的时候只训练W的权重。</p><p>接下来是instruction follow阶段。这一部分训练imageencoder和W的权重，用他的158k数据集训练出来的instruction follow模型</p><p><img src="../../files/images/LLaVA/format.png"></p><p>训出来的模型基本都是按照这种形式。可以看到，传入的图片基本上是一个原图，加上一些的形式boundingbox，然后几种数据格式在表现上就是对话。作者在训练的时候，只有answer的部分是有loss的。</p><p>LLaVA是一个很干脆的论文，把一个思路清晰的做了出来，并且比较重视原始数据。</p><h2 id="llava-1.5">LLaVA 1.5</h2><p><img src="../../files/images/LLaVA/1.5performance.png" style="zoom: 33%;" ></p><p>到了二期，论文只有短短5页。作者在方法上没什么更新，只是把LLM基座模型换成了13B，把imageencoder换成了更大更强的CLIP-ViT-L-336px，然后把连接层的Linear换成了双层MLP。</p><p>另外作者还观测到之前LLaVA由于图片分辨率的问题，会看不清楚输入，新的imageencoder可以看得更清楚</p><p>作者从scaling的视角来看他们的方法，提出了一个问题：158k数据够了吗？</p><p><img src="../../files/images/LLaVA/scale.png" ></p><p>作者把数据集混入了一些VQA、OCR的数据，另外对instruction-followingprompt中要求对response给出格式要求，这样更方便模型对学习，比如说：</p><blockquote><p><em>Answer the question using a single word or phrase</em>.</p></blockquote><p>作者在上图中报告了对于数据、模型大小，和图片分辨率做scale后的效果。看起来提升模型大小是涨点最有效的办法</p><p>最后用最大最强的模型去刷了个榜。这篇论文就是经典的二期论文的写法：找到最严重的问题，并修复之。另外，从scale的视角看整个问题，很好的视野。</p><h2 id="llava-plus">LLaVA-Plus</h2><p>在LMM基座模型效果提升了以后，作者瞄准了现在比较火的工具学习场景：能不能让LMM去通过工具调用来进一步提升tasksolving的能力？由此写出了LLaVA-Plus(Plug and Learn to Use Skills)</p><p><img src="../../files/images/LLaVA/intro.png" ></p><p>既然立足是一篇Agent的论文，作者论文写法都变了，用了storyoriented的写法，故事性变得很浓。可见作者的写作功底还是很好的。在Introduction中甚至搬出来了祖师爷"Societyof Mind"的理论(1988)</p><blockquote><p>Society of Mind: each tool is originally designed for a specificskill and by itself is only useful for specific scenarios, but thecombinations of these tools lead to emergent abilities that show signsof higher intelligence.</p></blockquote><p><img src="../../files/images/LLaVA/capbility.png" ></p><p>LLaVA的总体流程和ToolBench非常相似，具体可以看这个<a href="/660d5dc5.html" title="论文阅读[粗读]-TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS">这个笔记</a>。大致上就是</p><ul><li>先找了一些target image</li><li>找GPT4对image造出来一个query too observationl answer这样的tuple</li><li>把这个tuple弄成一个数据集。以此作为训练数据，训练出来LLaVA-Plus</li></ul><p><img src="../../files/images/LLaVA/tool.png" ></p><p>具体的数据格式看起来和ReACT完全一致，经典的thought、action、observation、answer。作者一共制作了一个LLaVA-158K的数据集，另外把测试集搬出来做了一个叫LLaVA-Bench的测试系统。</p><p>作者说明，训练出来的模型达到SOTA水平。</p><h2 id="我的思考">我的思考</h2><p>可以看到，从今年4月走到11月，作者在LLaVA的道路上一路深耕，提高基础能力。再基础能力提上去以后，逐渐做到Agent能力。估计后续随着能力进一步提高，也许可以尝试多步工具调用以及多模态planning。LLaVA是一种lightweight的多模态连接方式，对textencoder和image encoder的模型结构都不做要求，从结果来看，效果还挺好。</p><p>为什么不在预训练阶段就用多模态的模型？一方面，作者在论文里说到的一个问题其实很有道理：目前的多模态数据主要就是图文caption对，这样的数据可以让模型去理解图片，但也没有进一步的能力需求了(不像纯文本数据那样需要推理等)，即使是VQA也以简单的数数、区分左右等等为主。训练数据决定模型学到的能力，我们可能得找到更好的多模态预训练数据。另一方面，多图多文结合也是一条路子。像GPT-4v就是天生多图的，这样的多图数据某种意义上和多步推理有着更紧密的联系。</p><p>最后，我很喜欢scaling的视角，我觉得scaling的结论是最可信的结论，也最有可能是未来大规模应用的前提。也不知道以后的LMM到底是单模态模型的整合，还是预训练级的多模态……</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 多模态 </tag>
            
            <tag> tool-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenAI开发者大会的所有可能结局</title>
      <link href="/75adad28.html"/>
      <url>/75adad28.html</url>
      
        <content type="html"><![CDATA[<p>众所周知，OpenAI打算在2023/11/6，ChatGPT问世(2022/11/30)大约1一年以后，召开第一届开发者大会，距离现在还有15天。我们不如来大胆预测一下开发者大会可能更新的所有内容吧！即是预测，也是我对OpenAI接下来开发的功能的期望。你觉得哪种结局最有可能呢？</p><blockquote><p>所有图片均由DALL·E 3生成</p></blockquote><span id="more"></span><h2 id="大一统结局">大一统结局</h2><p>目前的ChatGPTPlus我们需要在各种实验性功能中选择一个使用，大家都猜测这背后是GPT4在各种下游任务中特化的finetune版本。大家现在每次只能选一个，直接选择困难症，哪个都想要，但不能同时存在于一个context下。</p><p>有没有可能在11.6开发者大会中，OpenAI大一统所有checkpoint，将会使用一个统一的接口做完所有事情：有视觉可以看图，也能上网，能使用工具，还能做jupternotebook执行，最后能用DALL·E3画图(画出来的图也可以直接用视觉去理解)。</p><blockquote><p>大一统结局：ChatGPT Plus的选择困难症可以休矣</p></blockquote><figure class="half"><img src="../files/images/all_ends_for_11.6/consistence.png" width="35%"/><img src="../files/images/all_ends_for_11.6/c2.png" width="35%" /></figure><h2 id="模态联结结局">模态联结结局</h2><p>Google准备训练Gemini多模态大模型抢OpenAI的风头，据知情人士透露：OpenAI准备在Gemini上线之前训练一个多模态大模型Gobi来对抗Google的竞争。如果开发者大会上，Gobi已经训练完成了，Google还有后手吗？</p><blockquote><p>模态联结结局：Gobi is all you need</p></blockquote><p><img src="../files/images/all_ends_for_11.6/gobi.png" style="zoom:50%;" ></p><h2 id="openagent结局">OpenA(gent)结局</h2><p>Agent技术目前非常火，过了大约半年，一直没看到OpenAI出手。按照其一惯逻辑，在找到一以贯之的思想之前，他们可能会做总结对比的工作。就像强化学习一样，OpenAI有没有可能推出来一个Agent-gym框架，从此大家开发Agent都是基于Agent-gym的接口和设计理念。</p><blockquote><p>OpenA(gent)结局：OpenAI的阴影笼罩整个Agent研究。</p></blockquote><p><img src="../files/images/all_ends_for_11.6/agent.png" style="zoom:50%;" ></p><h2 id="true-openai结局">True OpenAI结局</h2><p>虽然之前业界普遍承认最强模型是GPT4，但开源和实际构建应用时大家会选择Llama和Llama2为主。OpenAI事实上并没有真正的Open，在开源界被MetaAI统治了。</p><p>因此之前在流传小道消息：OpenAI打算开源一个语言模型，代号是G3PO，也许开发者大会就是G3PO问世的时间，OpenAI用一年时间转形成了TrueOpenAI</p><blockquote><p>True OpenAI结局：你的llama3，何必是llama</p></blockquote><p><img src="../files/images/all_ends_for_11.6/TrueOpenAI.png" style="zoom:50%;" ></p><h2 id="超人主义结局">超人主义结局</h2><p>OpenAI开启了SuperAlignment小组，准备在2030年之前实现AGI。按照他们一贯是先研究、再宣发的特性，保守估计他们在2024年就研究完成了AGI，后面做5年的对齐工作。如果在开发者大会上，他们让AGI技术初步亮相，会不会进一步推进全世界的研究热情呢？</p><blockquote><p>超人主义结局：北大的通班不用再办了</p></blockquote><p><img src="../files/images/all_ends_for_11.6/AGI.png" style="zoom:50%;" ></p><h2 id="硅基飞升结局">硅基飞升结局</h2><p>OpenAI部署各种GPT服务，可能是世界上最缺GPU的人。之前听说OpenAI在研究自研AI芯片，如果OpenAI已经研究出来了AI芯片，可以把GPT4加密打印在门电路里，以后只要买到了GPT4-i7芯片，就能直接通过芯片激活来做推理了。每个时钟周期就是一个流水推理周期，64个周期就能推完64层transformer</p><blockquote><p>硅基飞升结局：人类只不过是一段boosting程序，引导硅基生命的到来。</p></blockquote><p><img src="../files/images/all_ends_for_11.6/chip.png" style="zoom:50%;" ></p><h2 id="one-more-thing结局">One More Thing结局</h2><p>6个月前的WWDC上，库克在发布的结束用一句"one morething"引出了最重要的Apple VisionPro发布。OpenAI会不会也在憋一个"终极大招"等着大家在开发者大会最放松警惕的时候引出来。比如有人统计了目前模型的运行速度，发现随着时间推移变得越来越快。有没有可能OpenAI找到了一种把稀疏大模型同时变得稠密的办法。最后来一句王炸。</p><blockquote><p>One MoreThing结局：通过最新的训练方法，我们成功找到了用100M稠密模型比肩100B稀疏模型的办法。</p></blockquote><p><img src="../files/images/all_ends_for_11.6/turbo.png" style="zoom:50%;" ></p><h2 id="快进结局">快进结局</h2><p>如果……上面的一切同时发生呢？</p><blockquote><p>快进结局：开发者大会马蹄疾，一日看尽长安花！</p></blockquote><p><img src="../files/images/all_ends_for_11.6/fast.jpg"  ></p><p>那么，你觉得哪一种结局最有可能呢？</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-09-29总结</title>
      <link href="/6c02904f.html"/>
      <url>/6c02904f.html</url>
      
        <content type="html"><![CDATA[<p>今天第一次尝试将Arxiv最新论文同步到博客。</p><p>扫描Arxiv的工作现在基本每天都做，最开始可能还要追溯到两年多前。曾经用过各种各样的方式完成这件事：</p><ul><li>最开始是超哥带着大家每天扫描，每人按日期做分工</li><li>后面一段时间我自己每天刷一刷</li><li>后来形成习惯了，要写一个飞书文档同步进去，后来觉得太麻烦，最后就不了了之了</li></ul><p>从今天开始，试着每天把新扫描到的有趣的论文更新到博客，看看大家的反应如何。可能一个良性的循环是：一方面有人反馈我有遗漏，或者推荐哪篇论文，我就可以仔细看看，或者写一些阅读笔记。</p><span id="more"></span><p>我读论文、扫描Arxiv论文，按照优秀程度大致分为几个粒度：</p><ul><li>最差的是点都不会点进去</li><li>好一些的会点进去看看abstract和作者</li><li>再好点的我会放到Arxiv Insights里</li><li>再好点的我会加入 paper reading TODO list</li></ul><p>写论文阅读笔记是这样：</p><ul><li>读完论文，觉得很好玩的东西，我会加入到blog TODO list</li><li>每次有时间的时候，就会写一篇阅读笔记</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS</title>
      <link href="/660d5dc5.html"/>
      <url>/660d5dc5.html</url>
      
        <content type="html"><![CDATA[<p>作者团队就是我们tool learning小组</p><p><img src="../../files/images/ToolLLM/authors.png"></p><h2 id="introduction">Introduction</h2><p>我们在之前(4月)的一篇综述论文里探讨工具学习场景的一些任务的困难</p><blockquote><p><a href="https://arxiv.org/abs/2304.08354">Tool Learning WithFoundation Models</a></p></blockquote><p>当时我们定义出来了两种典型的场景</p><ul><li>Tool Augmented:较为简单的场景，工具是为了模型服务的。模型通过调用工具来增强语言对话能力。比如Toolformer通过计算器拿到精确数值计算结果，或者现在比较火的retrieve-augmented LLM的概念(通过一个外置的知识库增强事实等能力)</li><li>Tool Oriented:较为困难的场景，模型是为了工具服务的。将大模型作为工具的组织者，目标是为了完成c端用户的具体需求</li></ul><p>不管是哪种场景，在运行难度上都是多工具&gt;单工具，多步调用&gt;单步调用。在之前的论文中，我们基本只是探讨了单工具的场景，发现ChatGPT is all you need</p><p><img src="../../files/images/ToolLLM/single_tool.png" style="zoom:50%;" ></p><p>在本文，我们进一步拓展了应用场景，在多步、多工具的场景中探索了ToolOriented任务的执行效果。</p><h3 id="rapidapi">RapidAPI</h3><p>首先，我们找到了一个开放的RestAPI平台RapidAPI，他们包含大约50000个API，其中有很多类型的请求。我们只使用了他们的所有GET请求，剩下了大约16000个API</p><blockquote><p>这一个简单的sift背后其实有深刻的原因：我们的工具是不是有状态的？POST请求是会对真实世界产生影响的，也就是说，同样的请求可能会随着逻辑执行的顺序有区别。这在实际执行中会带来比较大的问题，最终我们把任务做了一定的化简，只采用了GET请求。</p></blockquote><p>其中，所有的这些API是按照层级进行组织的。最顶层有49个category,每个category下面含有多种多样的tool，每个tool包含一个或多个API。</p><p>据了解，这应该是第一篇把真实世界工具做到这个数量级的工作：</p><p><img src="../../files/images/ToolLLM/rapidAPI.png"></p><p>由此出发，我们首先通过self-instruct构造多种不同难度的query，然后用ReACT以及新版的DFS、ToT算法对query进行了标注，造出了(query-answer)数据集。再在数据集上进行训练Llama最终达到了接近ChatGPT的效果。工作流程如图所示：</p><p><img src="../../files/images/ToolLLM/intro.png"></p><p>我们开源了代码、模型权重、数据、评测平台(ToolEval)</p><h2 id="method">Method</h2><p>从上面的讨论中，我们可以发现，其实我们要做的任务就从任务定义、到数据集、到评测都是全新的，都需要我们去创新。因此我们进一步把workflow划分为5块，也就是下面要讲的5个部分</p><h3 id="query-generation">query generation</h3><p>首先是，作为测试集的query如何构建？这个问题涉及到了：我们使用RapidAPI具体是想要打成什么目的？</p><blockquote><p>我们是为了帮助解决通用任务</p></blockquote><p>在query构造时，我们使用self-instruct的方式，给定工具和对应的描述，让模型去构造对应的query。对于工具的粒度不同，我们划分出了三个难度的query：</p><ul><li>G1:单工具。对着同一个工具下的所有API去拟合query</li><li>G2：对着同一个category下的不同工具下的所有API去给调用</li><li>G3：对着不同category下的不同工具下的所有API去给调用</li></ul><p>三种query的难度逐次增高，对应的工具选择能力需求也更强。</p><p>通用任务也有高下之分。有具体的任务"帮我找找附近好吃的粤菜馆"，或者”帮我赚10000美元“。从任务的角度理解，其实他们对于工具的需求是不一样的：</p><ul><li>具体的任务，可能在找到对口的工具时就已经完成了80%的工作，比如导航工具</li><li>抽象的任务，可能找到对口的工具只能完成10%的工作，比如在纳兹达克开户……</li></ul><p>因此，其实工具能力可以进一步划分出两个子能力：工具选择能力和工具使用能力</p><p>在query构造时，我们选用了self-instruct的方式，对着tool反向构造query，再让模型看着query选择到这个工具去执行。</p><h3 id="answer-generation">answer generation</h3><p><img src="../../files/images/ToolLLM/DFSDT.png"></p><p>再构造出query以后，下面就是如何进行answer的标注，我们的标注模型使用ChatGPT-turbo。其中，使用了他们的function接口。对于OpenAI-function接口来说，只要给出了输入的json-schema，模型就会自动选择调用是什么、以及输入的参数是什么。从这个意义上来看，其实function接口既能表达出工具选择能力，又能表达出工具调用能力。</p><h3 id="react">ReACT</h3><p>经典的工具调用manner可以使用CoT，或者说开源工具包langchain类型的ReACT算法</p><blockquote><p><a href="https://github.com/langchain-ai/langchain">langchain</a></p><p><a href="https://arxiv.org/abs/2210.03629">Synergizing Reasoning andActing in Language Models</a></p></blockquote><p>他的运行逻辑是：模型按照一个链式的模式一样调用工具，每一步先给出一个thought，再给出一个工具调用。一个openAIjson描述如下图所示</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JSON"><figure class="iseeu highlight /json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Prefix&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://entreapi-faker.p.rapidapi.com/name/prefix&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Randomly generate a prefix (e.g., Mr. Mrs., etc.)&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;method&quot;</span><span class="punctuation">:</span> <span class="string">&quot;GET&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;required_parameters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;optional_parameters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gender&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;STRING&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Optional gender.&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;default&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span> </span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;tool_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;EntreAPI Faker&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;category_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Data&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure></div><p>这里的所有description和json-schema是按照RapidAPI接口官网的信息转化成的，因此我们需要先把官网所有的描述都爬取下来再做一次clean。</p><h3 id="dfsdt">DFSDT</h3><p>从CoT出发，后来的有些工作探索了ToT、PoT、HoT、SoT之类的新方法，我们也采用了一个ToT的DFS算法，主要是想解决CoT的以下问题</p><ul><li>误差累积：对于CoT来说，因为是一直向前的manner，误差会逐渐累积。也就是说，如果某一步开始的工具调用失败了，后面其实会陷入到这种“陷阱”里，最终到达一种“持续进行同样的错误工具调用”的死循环里</li><li>探索范围，单次的CoT其实探索范围很窄，但是很多情况下需要多次的探索才能把任务完成</li></ul><p>所谓的ToT，其实就是把thought当成边，把tool当成点，然后把整个的探索过程组织成一颗树，进行树搜索。经典的树搜索有DFS和BFS两种，我们采用了DFS，主要是基于以下原因：</p><blockquote><p>我们希望算法尽可能快速的结束：对于简单的query(CoT就可以做出来的)，可以在类似CoT的执行时间内就做完。相比之下，BFS可能对于所有query都会在类似的时间内做完</p></blockquote><p>传统的DFS算法，需要在每一步的时候生成多个候选，然后选择一个最好的，但在我们的探索中，我们发现，其实模型投票的时候，大多数情况下会认为第一个候选是最好的，这是因为模型本身生成时是根据logits最大化来的，先生成的其实有一定的calibration性质。</p><p>因此，我们最终选择DFSDT，执行上是一个先序遍历的方式。前向时每次只给一个父节点生成一个子节点，到返回到该节点时才生成第二个节点。这种算法，在CoT时间内和CoT算法的行为是完全一致的，只是在CoT执行结束以后才会开始回退，做一些DFS的处理</p><p><img src="../../files/images/ToolLLM/ans_anno_performance.png"></p><p>我们在几个粒度上都尝试了DFSDT算法，发现基本都达到了远超CoT的效果</p><h3 id="evaluation-method">evaluation method</h3><p>之前一直提到了效果，但直到现在都没讲这个score到底是怎么算的。其实Eval想要达成几个目的：</p><ul><li>一个query是否是可解的。有些query由于给的信息错误，天生不可解决。比如"帮我导航去西餐馆"，没说是哪个西餐馆。或者说了一个根本不存在的地方</li><li>一个query是否被解决了</li><li>一个query的两种解决方案，到底哪个会更好</li></ul><p>因为这几个问题其实都是动态的，对于每个query的情况各不相同。我们的主要评判方法是模型评测，由此诞生了ToolEval</p><p><img src="../../files/images/ToolLLM/tooleval.png" style="zoom:50%;" ></p><p>我们是由人标注了一波数据，然后在上面评判拟合的prompt，用一个和人工具有最高一致性的方案。最终得到了一个相对比较稳定鲁棒的评测方法</p><h3 id="tool-retrieve-system">tool-retrieve system</h3><p>其实还有个重要的问题是，RapidAPI有16000个API，模型不可能在context里全部看到，我们必须帮模型去化简难度。由此我们就加了一个retrieve的模型，分别把query和API-doc编码成为向量。训练时由query自带的一些relativeAPI作为正例，随机采样一些其他API作为负例来做对比学习，和Sentence-BERT一致。实际测试时就直接用embedding来相似度匹配，准确率还是很高的</p><p>作为Baseline方法，我们也对比了ada-embedding直接把所有的工具描述的adaembedding都搞出来算相似度、以及BM25经典方案。</p><p><img src="../../files/images/ToolLLM/retrieve.png" ></p><p>其实ada embedding直接做效果也挺好，但还是比不过我们寻模型的方法。</p><h3 id="model-training">model training</h3><p>最后，就是我们拿我们的生成出来的大约20万的query-answer对去训练Llama，使其掌握RapidAPI的能力。</p><p><img src="../../files/images/ToolLLM/result.png" ></p><p>另外，我们在训练时发现，其实效果达到最大值附近，需要用到的数据并不多。也许工具学习任务和RLHF任务类似，反而需要小而精的数据保证。</p><h2 id="我的思考">我的思考</h2><p>这篇论文可以说是之前一个开源项目的二期成果，之前我们做了一个BMTools工具包，手动实现了一些工具比如Google-Search等。后来我们发现，线性地去实现工具其实是很低效、很工程的事情。最好是可以把语言模型直接去对接到工业界、学术界已有的工具平台，最终找到了RapidAPI这个平台。</p><p>大模型使用工具是一个很大的话题，找到通用性的工具调用方法更是很难的课题。思考人学习工具的方法论，很多老师傅"言传身教"的所谓经验都是很难用语言去表示的。目前的预言模型主要是基于语言去训练，对于这种更加抽象的能力是否掌握仍是未知数。</p><p>我们在探索中也发现，对话场景下表现相似的ChatGPT、Claude、Llama2，到了工具场景，能力却变得天差地别。这种区别，一方面说明了工具能力也许是比语言能力更困难的能力，另一方面也许揭示出来：ChatGPT，尤其是ChatGPT-function，可能已经在这个领域从预训练模型的角度做出了一定的探索。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> tool-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Llama 2: Open Foundation and Fine-Tuned Chat Models(下)</title>
      <link href="/a3a406b2.html"/>
      <url>/a3a406b2.html</url>
      
        <content type="html"><![CDATA[<p>语言模型的RLHF(PPO)的基本流程的算法我们在 <a href="/5d2d4022.html" title="论文阅读[粗读]-强化学习和RLHF中的PPO算法">强化学习和RLHF中的PPO算法</a>中介绍过了，主要分为三个阶段，下面我们来讲讲Llama2分别做了哪些创新吧</p><span id="more"></span><h2 id="sft">SFT</h2><h3 id="data-and-train">data and train</h3><p>首先，作者和之前的论文保持一致，认为SFT的训练数据数量不是问题，关键是质量。然而，作者还是收集了27650条instructiontuning的数据，emmm，这好像是我见过的最多的。</p><p>作者在180条数据上做了仔细检查，比对了human的结果和SFTmodel生成的结果，发现大致在一个水平。</p><blockquote><p>训练方式就是传统的crossentropy，然后作者只在assistantmessage上计算梯度</p></blockquote><h2 id="reward-model-training">reward model training</h2><p>首先作者需要human paired data。标注了大概100万条数据</p><p>标注过程和后面的过程是绑定的，就是先标注一波，然后做RLHF。下一波标注的时候pairdata来自新模型。主要是因为human标注需要时间，以及更高效地利用数据</p><p>另外，作者为了让human永远有选项选，提供了四种偏序关系可以选择。</p><p><img src="../files/images/LLaMA2/margin.png" style="zoom:50%;" ></p><h3 id="margin">margin</h3><p>接下来，在训练RewardModel时，作者利用起了之前human标注的多种偏序关系，具体是这样<span class="math display">\[\mathcal{L} = - \log(\frac{1}{1 + e^{-(r_\text{good} - r_\text{bad})}})\]</span> 这个loss是一个Bradley-Terry模型crossentropyloss的导出形式，希望好的数据的reward更高</p><p>如果已知human的偏序不止一种的话，就可以额外加一点偏差值 <spanclass="math display">\[\mathcal{L} = - \log(\frac{1}{1 + e^{-(r_\text{good} - r_\text{bad} -\text{margin})}})\]</span> 就是说，比如significantbetter，就希望差得更远，通过更大的gradient来保证。作者做了消融实验，看在测试集上rewardmodel的分类acc，发现效果确实更好</p><p><img src="../files/images/LLaMA2/effect_of_margin.png" style="zoom:50%;" ></p><h3 id="performance">performance</h3><p>衡量reward model好坏的指标就是测试集分类acc，</p><p><img src="../files/images/LLaMA2/different_order.png" style="zoom:50%;" ></p><p>首先报告了在各种偏序关系下测试集上的rewardmodel分类结果，发现区分还是挺明显的。基本上human认为显著区别的，模型都能区分。</p><p>接下来作者做了一个scale实验，发现用更大的模型、更多的训练数据可以训练出更准确reward模型。最重要的是：</p><blockquote><p>仍未观察到收敛现象。也就是说，还能接着提规模涨指标</p></blockquote><p><img src="../files/images/LLaMA2/scale_of_reward_model.png" style="zoom:50%;" ></p><h2 id="rl">RL</h2><p>最后的RL阶段。和前面提到的，由于human标注是一波一波来的，作者也就是一波波训的rewardmodel和RL模型。称之为RLHF-v1到RLHF-v5</p><p>主体方法上，作者尝试两种方法</p><h3 id="rejection-sampling">rejection sampling</h3><p>这是一个看起来很稳定的算法，和之前讲到的 <a href="/feddc200.html" title="论文阅读[精读]-RRHF: Rank Responses to Align Language Models with Human Feedback without tears">RRHF阅读笔记</a> 非常像</p><p>每个iter中，先对每一个prompt采样K个样本，然后用打分模型打一波分，找到最好的样本，然后按照SFT的方式计算crossentropyloss。</p><p>为了训练的稳定，选取最好样本时是从前面的所有iter里，而不是当前iter里选最好样本(所以其实是K*iter选1)。其实就是退化版的RRHF，不过省下了非常多的训练资源</p><p><img src="../files/images/LLaMA2/avg_score.png" style="zoom:50%;" ></p><p>作者实验了一下，用最开始的SFT对应prompt多次sample样本，对应的最高score持续提升，这就是这个算法的优化空间。</p><p>同时，这个算法的效果和sample样本时选取的算法也是高度相关的，更高的diversity更可能找到更好的样本，但是数据少时表现更不稳定。</p><p>作者做了实验(注意左右两图的纵坐标不一样)，和上面一样看多次采样的最高reward。发现这个最佳温度是在变化的，因此作者是每个iter都分别找最佳温度，再sample。</p><p><img src="../files/images/LLaMA2/temperature.png" style="zoom:50%;" ></p><h3 id="ppo">PPO</h3><p>作者没怎么使用经典的PPO算法，可能是因为训练不稳定？</p><p>只在最后一次迭代，也就是RLHF-V5之前用了一下。</p><p>流程是这样：训出来RLHF-V4以后，先用PPO增强一波，得到V4-PPO，然后用V4-PPO做rejectionsample训练，得出来RLHF-V5</p><h3 id="performance-1">performance</h3><p><img src="../files/images/LLaMA2/rlhf_method.png" style="zoom:50%;" ></p><p>作者首先报告了RLHF的平均水平，这个图坐标是指：战胜ChatGPT-0301的百分比。两个轴分别是帮助性和无害性数据的结果</p><blockquote><p>50%就是说和ChatGPT差不多</p></blockquote><p>可以看出来，效果不错，每轮都在加强。</p><p><img src="../files/images/LLaMA2/between_other.png" style="zoom:50%;" ></p><p>除了机器自评以外，作者在直接让human实验，每个模型的规模级别，都和同规模的类似模型进行了比较。可以看到</p><ul><li>基本都是赢了</li><li>不过在vicuna 33b上，似乎差不太多？</li><li>70b基本和chatGPT一样</li></ul><p>我非常好奇的是，有没有更详细的PPO和rejectionsample的对比实验，比如PPO+SFT-V1，不过作者没有报告</p><h2 id="ghost-attentiongatt">Ghost attention(GAtt)</h2><p>最后还有一个创新点,就是封面那个图。在多轮对话中，模型经常在后面的轮数就忘了最前面的全局prompt</p><p>解决方法也很简单：作者把最前面的system message在每个usermessage前面都复制了一份。作者手动创造了一些好这么做的prompt比如“answeronly with emojis”, "act as <em>Napoleon</em>"。然后在SFTquery随机中掺杂上这些需求，然后让模型生成这些数据</p><p>作者把这件事情默默用在了itertrain过程中，就是说在RL用的prompt里掺杂了这些东西，然后每轮对话都加，但是rewardmodel训练没用上，所以不影响reward model的训练稳定性。</p><p>测试时没有这个trick，只在system prompt加一次，看能不能维持住</p><p><img src="../files/images/LLaMA2/gatt_performance.png" style="zoom:50%;" ></p><p>可以发现，不加GAtt，基本上两轮以后前面的prompt已经忘完了，加上以后就一直不忘。</p><p>另外，作者在附录里提到一个有趣的事情：</p><blockquote><p>ChatGPT对system prompt也很看重，如果不加systemprompt，对比Llama2的win rate就会从44%暴跌到36%</p></blockquote><p>难道说，MOE真的重出江湖？</p><h2 id="我的思考">我的思考</h2><ul><li>RLHF部分的探索非常详尽，很久没看到做这么多实验的论文了。够solid</li><li>看Llama2的总体情况，scaling trend依旧存在，这就给更大更强的Llama3留出空间了，不知道什么时候能推出</li></ul><blockquote><p>另外，听说OpenAI打算开源一个叫G3PO的模型，不知道啥时候出来...</p></blockquote><ul><li><p>里面提到的rejectionsample非常新颖。其实最近有一些工作就是想用类似SFT的方式来做RL，其实本质上都是对Rewardmodel的对抗训练，差不太多：PRO、RRHF等等。</p></li><li><p>这里我抛出来一个问题：既然RLHF是在显式地对抗rewardmodel，那我们能不能在运行时进行对抗呢？</p></li></ul><blockquote><p>humanpreference虽然不会变，但是还有很多preference和下游绑定的任务。其实这就是LLM搜索类的算法：ToT里面引入了一些这种in-context对抗的特性，后面我们会进一步指出和解决这个问题。</p></blockquote><ul><li>类似RRHF和rejection sample，是在多个样本中learning from best ofN，那么显然这个方法的效率取决于最优score寻找的过程，相同搜索资源下，最优score越高，用这类算法的效果就越好。那么，这是不是也预示着我们需要一个更好的搜索算法呢？</li><li>虽然叫"开源"，但其实Llama2不是完全开源的……pretrain数据和SFT数据就没给</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Llama 2: Open Foundation and Fine-Tuned Chat Models(上)</title>
      <link href="/6a73db61.html"/>
      <url>/6a73db61.html</url>
      
        <content type="html"><![CDATA[<p>从影响力上，他是你应该第二个知道的大模型(第一个是GPT)，前两天刚被人用C重写了一遍；从评测结果上，其效果超过ChatGPT-0301</p><p>笔记比较长，因为我在讲解时把正文和附录揉在一起，力求讲清楚所有技术细节和创新点。</p><blockquote><p>除了safety的部分，我确实不懂这个方向……</p></blockquote><p><img src="../../files/images/LLaMA2/authors.png"></p><p>从论文里可以看到三个小细节：</p><p>首先在大小写上，作者把这个模型叫做"Llama2"，但是在1代时描述是LLaMA，现在的开源社区都是用"LLaMA"来称呼，不知道后面大家会不会改</p><p>其次，这个作者团队延续了一代里的Meta，但是GenAI又是什么东西？</p><blockquote><p>GenAI其实也是Meta的部门，是小扎专门为生成式AI成立的研究所，这篇论文可以看做他们打响影响力的开山之作。</p></blockquote><p>最后，LLaMA一代里的元老已经没了，这是被优化掉了？</p><p><img src="../../files/images/LLaMA2/llama_1_author.png"></p><p>仔细看，他们还在附录里，但作者列表直接查无此人，这就是大公司内部的斗争吗……</p><blockquote><p>ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philom-ena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful productand technical organiza- tion support.</p></blockquote><p>作者一共训练了7B, 13B,34B,70B四个版本的LLaMA2，其中34B因为一些安全性问题暂时没有发布，其他的模型都是商业许可，填申请就能下载</p><h2 id="预训练阶段">预训练阶段</h2><p>所有版本的模型都训练了2T的token量</p><p>tokenizer和一代一样，都是vocab_size=32k的BPE模型。数字单独用token，然后不能解码的UTF-8一个byte一个</p><p>值得一提的是，这次dataset不开源了，屠龙者终成恶龙？</p><p><img src="../../files/images/LLaMA2/data.png"></p><p>作者在两个超算平台上训练，用了大概2000GPU，训练了3.3M A100-80GBhour的训练量完成了训练……</p><p><img src="../../files/images/LLaMA2/hours.png"></p><table><tr><td><img src="../files/images/LLaMA2/loss1.png" align=left style="zoom:35%;" ></td><td><img src="../files/images/LLaMA2/loss.png" align=left style="zoom:25%;" ></td></tr><tr><td>llama 1 loss</td><td>llama 2 loss</td></tr></table><p>另外，对比一代和二代的训练loss</p><ul><li>大概可以发现在训练相同的token1.4T时，两者的loss是差不多的，这大概说明预训练数据的难度也差不多？</li><li>以及2代在训练的时候尖峰少了很多，大概是代码做了优化，或者小扎自己搭的超算平台真有独到之处？</li></ul><p>从loss曲线上来看，其实训了2Ttoken之后模型仍然没有表现出收敛的趋势，但是提前停止了训练，</p><blockquote><p>If you have money, you can continue……</p></blockquote><p>作者没有纰漏预训练数据集大概多大，不过Llama1应该是训了一个epoch，只有Book和Wiki数据训了两个epoch。</p><blockquote><p>另外，之前有个GPT4黑客爆料说用13Ttoken的数据集训了1个epoch，花了66million $。不知道Llama 2花了多少钱</p></blockquote><h3 id="sequence_length">sequence_length</h3><p>把sequence-length提升到了4k。</p><blockquote><p>值得一题的是，最近的新方法可以把2k的LLM通过finetune基本无伤拓展到16k、32k量级，参考笔记：<a href="/abce9d24.html" title="论文阅读[粗读]-Extending Context Window of Large Language Models via Position Interpolation">ROPE长度内插延拓</a></p></blockquote><p><img src="../../files/images/LLaMA2/seq_length.png" style="zoom:35%;" ></p><p>作者通过150B token训练量的对比实验，发现训练4klength带来了更好的效果</p><h3 id="group-attention">group-attention</h3><p>作者在34B和70B的模型上使用了group-attention技术。这个技术是指，正常的attention要在每个head分别用不同KQ V矩阵把hidden state的某个部分做变换。为了节省参数，可以然后KV变换阵共享参数，作者使用的版本是8个Q阵对应一组K V阵的参数</p><p><img src="../../files/images/LLaMA2/GQA.png" style="zoom:50%;" ></p><p>作者通过相同的实验，发现GQA确实不咋影响效果</p><p>MHA是multi-headattention就是原始版本，很好理解。但是表格里面的MQA又是什么东西？</p><p>其实MQA是group attention的一种特殊情况，就是所有head的keyvalue是相同的，区别只有Q</p><blockquote><p>GQA: Training generalized multi-query transformer models frommulti-head checkpoints</p></blockquote><h3 id="预训练评测">预训练评测</h3><p>接下来，作者评测了预训练模型的效果</p><p>首先作者和一些开源的模型做了比较</p><p><img src="../../files/images/LLaMA2/compare_wtih_open_source.png" style="zoom:50%;" ></p><p>这里面可以看出，Llama 2比Llama 1好很多</p><ul><li>在数学能力上，小模型的数学能力基本翻了好几倍。</li><li>在代码能力上，70B模型暴增7个点</li></ul><p>基本可以认为，在任何场景下，Llama 2都是现在最强的开源模型</p><p>接下来，作者还和闭源模型做了比较</p><p><img src="../../files/images/LLaMA2/compare_with_close_source.png" style="zoom:50%;" ></p><p>这次发现，LLama2的基础能力基本上是最差的……好消息是，和GPT-3.5差不多？</p><p>除了代码能力(humanEval)，看起来Llama2的代码能力确实不行</p><blockquote><p>这里面的GPT-3.5的效果，应该用的是GPT4技术报告里的结果，对应的模型是text-davinci-003。如果和001002比，可能Llama 2要更强一点</p><p>另外，GPT4这个GSM8K直接干到92也太猛了……</p></blockquote><p>其实作者做的每一个数值，都是一堆实验的，比如上面说的数学能力，就是下面这两个数学题数据集的平均数</p><p><img src="../../files/images/LLaMA2/math.png" style="zoom:50%;" ></p><p>其中这个"MATH"，就是之前我们讲<a href="/d074522f.html" title="论文阅读[精读]-Let’s Verify Step by Step">Let’s-Verify-Step-by-Step</a>时使用的那个测试数据集</p><blockquote><p>当时打到了78%</p></blockquote><p>如果只想了解预训练的细节，那么到这里就足够了，下篇我将会重点讲解RLHF的部分，也是占据篇幅最多的部分</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从Flowformer探讨Attention的线性复杂度</title>
      <link href="/de3f5273.html"/>
      <url>/de3f5273.html</url>
      
        <content type="html"><![CDATA[<p>本来想写 <a href="https://spaces.ac.cn/archives/9687">Self-ConsumingGenerative Models Go MAD</a> ,结果突然发现被苏老师抢发了，那就换一篇。</p><p>来讲讲软院去年ICML的Flowformer：如果把流图的思想引入到Attention算子中来。</p><p>这篇游神在知乎讲过一遍，我来主要分享一下在设计结构时我觉得比较好的一些思想，以及我对于线性attention的一些看法。</p><p><img src="../../files/images/flowformer/intro.png"></p><span id="more"></span><p>作者团队是软件学院</p><p><img src="../../files/images/flowformer/authors.png"></p><h2 id="attention线性化">attention线性化</h2><p>这篇文章中，主要讲了线性attention的问题。self-attention算子的一般计算是<span class="math display">\[attn(x) = \text{softmax}(\frac{Q_x K_x^T}{\sqrt{d}})V_x \\Q_x = Qx,K_x = Kx,V_x = Vx,\\Q_x,K_x,V_x \in \mathbb{R}^{[batch,seq length,\text{hiddendim}/\text{head count}]}\]</span> 其中最后一个维度是把原始的hiddenstate分成多个head，然后每个head分别计算attention，所以会除以 headcount</p><p>这里就会发现，x的维度是[batch_size,seq_length,hidden_dim/head_count]，所以整个计算的复杂度里对于seq_length这个维度是<spanclass="math inline">\(O(n^2)\)</span>复杂度，当length很长的时候，这个是不能接受的</p><blockquote><p>想想claude 2 的100k seq_length，来个 <spanclass="math inline">\(O(n^2)\)</span>复杂度……</p></blockquote><p>能不能把这个复杂度化简一下呢？这个问题就是学界一直在研究"attention线性化"的问题，但是往往复杂度下来了，效果也就下来了。</p><p>这里我们来一个思维实验，对于一个正常的LLM模型，比如说LLaMA</p><p><img src="../files/images/flowformer/llama_shape.png"></p><p>可以看到，65B模型，64个head，所以最后一个维度是<spanclass="math inline">\(8192/64=128\)</span>,是不是比seq_length小多了？我们再来看看原式子</p><p>如果没有这个softmax，我们的计算就大概是<spanclass="math inline">\(Q_x K_x^TV_x\)</span>，我们其实可以先算后面两个，变成一个 <spanclass="math inline">\(\mathbb{R}^{batch,128,128}\)</span>维度的矩阵，再和Q相乘，一下子就变成针对seq_length的<spanclass="math inline">\(O(n)\)</span>复杂度</p><blockquote><p>所以说，这个<spanclass="math inline">\(O(n^2)\)</span>，就是因为非要算一个softmax?</p></blockquote><p>softmax的过程可以看做对一个变换<span class="math inline">\(f: x_i,x_j \toy\in\mathbb{R}\)</span>的结果进行竞争，这里取得核函数f是"点乘的e指数"<span class="math display">\[softmax(x)_{i,j} = \frac{e^{x_i*x_j^T}}{\sum_{x_k}(e^{x_i*x_k^T})}\]</span> 其实，如果这个变换是可以分解的，也就是说是某个变化的点积 <spanclass="math display">\[f(x_i,x_j) = &lt;\phi(x_i),\phi(x_j)&gt; = \phi(x_i)\phi(x_j)^T\]</span></p><blockquote><p>当然Q、K不一定用同一个变换函数<spanclass="math inline">\(\phi\)</span>，但道理是一样的</p></blockquote><p>那么attention就是可以线性化的，用下面的式子化简： <spanclass="math display">\[\begin{aligned}attn(x_i) &amp; = \sum_{j=1}^m \frac{f(Q_i,K_j)}{\sum_{j&#39;}f(Q_i,K_{j&#39;})} V_j \\&amp; = \sum_{j=1}^m \frac{\phi(Q_i)\phi(K_j)^T}{\sum_{j&#39;}\phi(Q_i)\phi(K_{j&#39;})^T} V_j \\&amp; = \frac{\phi(Q_i) * \sum_{j=1}^m(\phi(K_j)^T V_j)}{\phi(Q_i) *\sum_{j=1}^m\phi(K_j)^T}\end{aligned}\]</span> 发现没，我们可以先花两倍的线性时间把<spanclass="math inline">\(\phi(Q),\phi(K)\)</span>算出来，再花线性时间算出来$_{j=1}<sup>m(K_j)</sup>T<spanclass="math inline">\(，再花\)</span>O(nd<sup>2)<spanclass="math inline">\(时间把\)</span>(K)</sup>TV$算出来</p><p>最后用线性时间就能把每个位置的attention算出来</p><p>一个最简单的想法就是：<span class="math inline">\(\phi(x_i) \equivx_i\)</span>恒等变化。然而，这样的话，<spanclass="math inline">\(f(Q_i,K_j)\)</span>有可能是负的，这明显不可行</p><p>当然，这种变化需要保证仍然能满足softmax的一些性质，比如非负性、归一化、以及希望绝对值大的时候代表注意力高等等</p><p>之前苏老师提到了一种变换： <span class="math display">\[\phi(Q) = \text{softmax}_2(Q) \\\phi(K) = \text{softmax}_1(K)\]</span> 注意这里, K是在seq_length维度做归一化，Q是在d维度做归一化</p><p>可以发现，如果他们分别这样做了归一化以后，其实<spanclass="math inline">\(\phi(Q)\phi(K)^T\)</span>天然就在seq_length维度上是归一化的</p><h2 id="flow-attention">Flow Attention</h2><p>其实上面提到的变换就和FlowAttention有点像了，在这篇工作中，作者思考了一个问题？</p><blockquote><p>为什么要用softmax这个变换呢？</p></blockquote><p>其实softmax是一种赢家通吃的竞争思路，尤其是在引入e指数以后，就更放大了这种"贫富差距"。</p><p>其实，流图就是一个天然的竞争场景，假如每层有m个源和n个池，两两之间都有一个流，因此所有池子的总流入量等于所有源的总流出量。</p><p><img src="../../files/images/flowformer/method.png"></p><p>发现没，这时每个流的大小其实是天然符合attention定义的：注意力高就是流量大、天然的归一化、自带的竞争关系</p><p>作者就把流的计算方式定义成了乘积。K代表源，Q代表池子，从K流向Q：<span class="math display">\[\text{I}_i = \phi(Q_i) \sum_{j=1}^m \phi(K_j)^T \\\text{O}_j = \phi(K_j) \sum_{i=1}^n \phi(Q_i)^T \\\]</span></p><p>其中<spanclass="math inline">\(\phi\)</span>变换保证变换结束以后每个元素都是恒正的</p><p>如果有很多层的话，其实上一层的池子就是下一层的源，一个简单的归一化方法就是要求每个节点:流入量==流出量==1。作者就想了个归一化方法，除号是每个元素两两相除 <spanclass="math display">\[\frac{\phi(K_j)}{O_j}\quad \frac{\phi(Q_i)}{I_i}\]</span> 此时，再来看流入流出量</p><p><img src="../../files/images/flowformer/algo.png" style="zoom:50%;" ></p><p>这个恒1能保证，是因为要预计算所有的流量，然后求和再除回去</p><p>此时咱们来看流的大小 <span class="math display">\[\hat{I} = \phi(Q) \sum_{j=1}^m\frac{\phi(K_j)^T}{O_j} \\\hat{O} = \phi(K) \sum_{i=1}^n\frac{\phi(Q_i)^T}{I_i} \\\]</span> 注意看，这个式子和上面的不一样(求和的对象变了，所以不是恒等于1)</p><p>接下来，作者按照softmax的特性定义了三个操作</p><p><img src="../../files/images/flowformer/operation.png" style="zoom:50%;" ></p><p>最后算出来的R就是attention的计算结果，这里面的哲学思想是这样：</p><ul><li>竞争：每个V都是QK流竞争以后的结果</li><li>综合：A是每个源综合所有池子</li><li>申请：如果有某个池子的流很低，那么这个池子就抛弃，类似于之前说的"带1softmax"</li></ul><p>其中每一步操作都是线性的，综合的计算形式是这样：</p><p><img src="../../files/images/flowformer/total_algo.png" style="zoom:50%;" ></p><p>实验部分就是把tranformer里的attention算子换成这个，然后在很多seq2seq下游任务上做了实验，发现效果都挺好</p><h2 id="我的思考">我的思考</h2><p>flowattention方法虽然用流的思想包装了一下，但是计算形式其实和前面提到的双维softmax算起来比较接近。</p><blockquote><p>他们都是为了解决所谓的"trivalattention"的问题(attention矩阵变成所有位置都相同)，因此要引入竞争。</p></blockquote><p>我们从这个角度继续思考一下：softmax这个操作，其实是在每个元素自己向量的基础上，在向量间做了一个"归一化"，然后再在QK上分别做乘积。</p><p>一方面，我认为，这种思想甚至是不是能拓展到head之间呢？正常transformer是每个head内基本没有什么关系，是在attention算完以后再拼回去做变换。但其实head间也是可以竞争的，有些情况head1的结果更重要，有些时候head2的结果更重要。每个head在QKV变换之前，以及attention的A矩阵计算之后，都是可以先拼再拆地计算结果，这样是不是可以让transformer的表示能力更强呢？</p><blockquote><p>魔改版的MOE?</p></blockquote><p>另一方面，我个人感觉transformer精巧的设计有三</p><ul><li>一个是这里讲的竞争使得模型不会退化</li><li>另一个是先从低维的word dim变换成高维的hiddenstate再做head拆分，不丢失信息</li><li>最后一个是残差连接使得训练非常稳定</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> transformer结构探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Meta-Transformer: A Unified Framework for Multimodal Learning</title>
      <link href="/6dd69a39.html"/>
      <url>/6dd69a39.html</url>
      
        <content type="html"><![CDATA[<p>今天来看一篇的新作，如何在不用模态对数据的情况下，炼多模态模型？甚至效果还行？</p><p><img src="../files/images/meta_transformer/intro.png" style="zoom: 33%;" ></p><span id="more"></span><p>论文作者来自香港中文大学，笑死，一看题目我还以为MetaAI的新作。论文的一作我搜了搜，好像没有什么我见过的文章</p><p><img src="../files/images/meta_transformer/authors.png" style="zoom: 33%;" ></p><h2 id="introduction">Introduction</h2><p>作者在文章里讲到，多模态领域的一个重要问题是如何把模态对齐。而transformer架构其实是对模态不敏感的，只要进来embedding就能跑。那么作者就想到，能不能把各种模态的数据都扔进一个embedding空间，来做实现多模态编码呢？</p><p>其实，这个思路和CLIP是一样的，但是他们的解决方式不一样。CLIP是要找到很多image-textpair数据来做对比学习。但本文，是想要对齐多达12个模态，肯定找不到pair数据了。</p><p>另一个解决办法是像之前的imageBind一样，把所有模态的数据都对齐到一个模态(图像)，化简数据难度。</p><p>作者和上面的思路都不一样，他这里想要不用模态数据，但是没讲为什么？</p><p>大家肯定要问：不用pair数据，怎么做模态对齐任务？你说得对，这就是本文的一个故布迷阵，他只做了单模态任务，没做模态对齐任务。</p><p>总之，本文的贡献就是：尝试用统一的encoder，不用pair数据来做多模态表示。</p><p><img src="../files/images/meta_transformer/compare_to_previous_method.png" style="zoom: 50%;" ></p><h2 id="method">method</h2><p>这一部分，作者首先假设所有模态都应该享有一个公共的语义空间(这里的语义是泛指，不是指语言模态)。因此作者就想要找到一个多模态的编码方式<span class="math display">\[\hat{y} = F(x,\theta^*), \theta^* = \arg \min[\mathcal{L}(y,\hat{y})]\]</span> 其中x是原始输入，<spanclass="math inline">\(\hat{y}\)</span>是模型输出，y是真实标签。总体而言就是传统finetune的方式</p><p>作者设计了一个三阶段的流程 <span class="math display">\[F = h◦g◦f(x)\]</span> 第一阶段是embedding函数f，这一部分是对于不同模态分开设计的</p><ul><li>text模态就是wordpiece+wordembedding</li><li>图片模态是ViT那套patch划分，再过一个conv层</li><li>视频模态是变成多个图片</li><li>……</li></ul><p>总之，最后所有输入都能变成二位的embedding，注意：作者压平了维度，所有模态都和text一样了</p><p>接下来是encoder，作者想要用一个encoder接收所有：meta_transformer</p><p>最后是一个任务独立的head，每个任务都不一样，和BERT迁移到下游任务的方式一样</p><p>总体架构如下所示</p><p><img src="../files/images/meta_transformer/arch.png" style="zoom: 50%;" ></p><h2 id="experiment">experiment</h2><p>首先作者的encoder是在LAION-2B上训练的ViT(并不是abstract提到的没用pair数据)，然后text模态用CLIPtext word embedding初始化</p><p>接下来，作者在各个模态的任务上都做了些实验</p><p><img src="../files/images/meta_transformer/text.png" style="zoom: 50%;" ></p><p><img src="../files/images/meta_transformer/image.png" style="zoom: 50%;" ></p><p>这里放上text和image模态的结果。模型名字里最右下角的T和F分别是指encoder是Tune还是Freeze</p><ul><li>总体而言，结果不是很好，尤其是encoderfreeze的情况下。可以理解，encoderfreeze就相当于只有embedding和linear</li><li>话说是不是应该放个没有encoder的bias study</li></ul><p>最后，这篇文章没有analysis，放上所有实验的结果就跑路了</p><h2 id="我的思考">我的思考</h2><ul><li><p>我其实不太喜欢这篇文章的思路，我认为：如果方法中设计了什么变化，就需要说出来为什么要这么设计，同时实验中要有结果支撑"这个设计的优势是什么"。否则的话按照奥卡姆剃刀原则，就不应该有这个设计。但是本文似乎没讲清楚"不用pair数据"、"要freezeencoder"的意义和目的是什么</p></li><li><p>当然，本文探索的这个问题很有意思，结果也挺有意思：当你在一个模态训了一个大encoder，锁住以后在另一个模态上加一个encoder强行对齐，竟然也能做？</p></li></ul><blockquote><p>我觉得这里面藏着更深的道理：另外之前imagebind所有模态对齐到一个模态的结果也挺好，以及最开始类似flamingo的思路就用一个crossattention就能很轻松的连接多个模态，以及versitleDiffusion也是做到了类似的事情。</p><p>这是否说明transformer结构天然地将信息分解为了模态相关的和模态独立的两个部分，所以两个模态的transformer拼接时就会很容易呢？</p><p>我们是不是有机会让GPT4甚至不调参就能接到各个模态上去？</p></blockquote><ul><li>多模态领域发展真快呀，大家似乎都在试图剔除之前方法里的很多数据依赖和模型依赖，不知道最终能剔除到什么地步？</li><li>最后，模态间生成任务大家做的似乎都不是很好、要不就是干脆没做。我觉得多模态的终点就是把模型对齐到"脑波"模态，让我们和模型可以互相理解对方在想什么，是不是可解释性就来了？</li></ul><blockquote><p>脑机接口，但是没有脑机？</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Retentive Network: A Successor to Transformer for Large Language Models</title>
      <link href="/1c5cf63c.html"/>
      <url>/1c5cf63c.html</url>
      
        <content type="html"><![CDATA[<p>今天来讲讲被称为transformer "后继有模"的retentivenetwork网络：速度更快、占用更少、效果更好。</p><p><img src="../files/images/retentive_network/performance.png"></p><span id="more"></span><h2 id="introduction">Introduction</h2><p>作者团队来自清华，竟然是同学</p><p><img src="../files/images/retentive_network/authors.png"></p><p>作者在这个地方主要讲到了一个"不可能三角"：性能、训练并行性、低的inference成本</p><p><img src="../files/images/retentive_network/triangle.png" style="zoom:25%;" ></p><p><img src="../files/images/retentive_network/complexity.png" style="zoom:50%;" ></p><p>性能比较好理解。</p><p>训练并行性主要分两种：数据并行和训练并行，RNN当然可以数据并行，就是开一个很大的batchsize。主要问题RNN的下一个状态依赖于上一个状态，因此算第i个位置时，必须把前面&lt;i的位置全算完。相比之下，auto-regressivetransformer按照causal mask的形式，只需要一次前向，就能把所有位置的hiddenstate算出来，因此比如seq_length是2048时，就要快2000倍</p><p>推理成本主要是针对时间说的，</p><ul><li>正常的transformer做矩阵运算，n-&gt;d的映射需要复杂度<spanclass="math inline">\(O(n^2d)\)</span>做矩阵运算,因此一次前向总体复杂度大概是<spanclass="math inline">\(n^2d+nd^2\)</span>算上multi-headattention的版本，平均一个token是<spanclass="math inline">\(O(n)\)</span></li><li>而对于RNN来说，<span class="math inline">\(h_t = f(Ux_t +Wh_{t-1})\)</span>其中由于x和h都是一维的，因此最后的复杂度是<spanclass="math inline">\(O(nd^2)\)</span>就比transformer，平均一个token需要占<spanclass="math inline">\(O(1)\)</span></li></ul><h2 id="method">method</h2><blockquote><p>这一部分默认读者对tranformer的计算方式很了解</p></blockquote><p>那么作者是怎么在持有transformer特性的基础上，利用起RNN的推理成本的特性呢？作者也想要用一个RNN的形式</p><h3 id="retention算子">retention算子</h3><p><span class="math display">\[s_n = A s_{n-1} + K^T_n v_n \\o_n = Q_ns_n = \sum_{m=1}^n (Q_n A^{n-m}K_m^Tv_m)\]</span></p><p>其中<span class="math inline">\(A \in\mathcal{R}^{d*d},K_n,v_n,s_n,Q_n \in\mathcal{R}^{1*d}\)</span>。V就是每个单词的词向量，s就相当于RNN里面的中间向量，而o就是RNN里面的输出向量。上面的下面那行是说最后总体o计算可以变换成一个累加的形式</p><p>Q、K可以类比transformer里的query、key的作用，计算也是一致的 <spanclass="math display">\[Q = XW_Q, K = XW_K\]</span></p><p>这里看起来就是一个传统的RNN，但是如果我们假设计算时使用的A矩阵可以满足<span class="math display">\[A = \Lambda (\lambda e^{i\theta}) \Lambda^{-1}\]</span> 另外，我们假设<spanclass="math inline">\(\lambda\)</span>是标量 <spanclass="math display">\[\begin{aligned}o_n &amp; = \sum_{m=1}^n (Q_n A^{n-m}K_m^Tv_m) \\&amp; = \sum_{m=1}^n (Q_n  \Lambda^{n-m} (\lambda e^{i\theta})\Lambda^{-(n-m)}  K_m^Tv_m) \\&amp; = \sum_{m=1}^n \lambda^{n-m} (Q_n e^{in\theta}) (K_m e^{im\theta})V_m\end{aligned}\]</span>这样一下这个加和的形式看起来变成了矩阵乘的形式，而且如果把<spanclass="math inline">\(\lambda\)</span>看做一个类似mask矩阵的东西，其实用类似transformer的大号的矩阵乘就能把所有位置都算出来，具体算法的形式类似这样</p><p><img src="../files/images/retentive_network/algo.png" style="zoom:50%;" ></p><p>上面的和下面的两种算法的结果是完全相等的</p><p>接下来作者讲了几个优化</p><h3 id="chunk">chunk</h3><p>首先是，如果seqlength很长，上面的化简算法里的矩阵维度就太高了，想要把里面的分块计算(chunk)，之前有爆料GPT4之类的模型也大概是这个思路化简的。</p><p><img src="../files/images/retentive_network/chunk.png" style="zoom:50%;" ></p><p>所以块内inner-chunk就是直接按上面的算法，块间需要按照RNN的形式Autoregressive计算。换句话说，这里节省占用空间，但是需要每块先后的算，算是一个权衡吧</p><h3 id="multi-head">multi-head</h3><p>其次，作者类比multihead attention，搞了一个multihead retention</p><p><img src="../files/images/retentive_network/gate.png" style="zoom:50%;" ></p><p>其实本质上也是在化简矩阵维度，先分成多个head，每个head使用不同的衰减系数<spanclass="math inline">\(\lambda\)</span>，然后在每个head算完以后都给这个head做一个linear变换</p><p>之所以用group_norm，是因为每个head的衰减系数不一样，也就是说绝对值会有点区别</p><p><img src="../files/images/retentive_network/code.png" style="zoom:50%;" ></p><h3 id="retentive-network">retentive network</h3><p>在retention算子说完以后，整体的网络就和transformer一致，只是attention算子变成了retention算子<span class="math display">\[Y^l = MSR(LN(X^l)) + X^l \\X^{l+1} = FFN(LN(Y^l)) + Y^l\]</span> 总体而言</p><ul><li><p>他训练时使用矩阵化简的形式，可以一次算出来所有位置</p></li><li><p>推理时使用矩阵化简前那个RNN的形式，因此推理的时间和空间复杂度都很低</p></li></ul><blockquote><p>神奇的数学</p></blockquote><h2 id="experiment">Experiment</h2><p>实验部分很详尽，基本都是在证明RetNet性能强、速度快、占用低。</p><p><img src="../files/images/retentive_network/size.png" style="zoom:50%;" ></p><p>作者做了scale实验，发现RetNet的scale增长更快。感觉还挺有意思的，难道说运用了Recurrent的方式，真的能进一步增强模型的表征能力？</p><p><img src="../files/images/retentive_network/inhenced.png" style="zoom:50%;" ></p><p>另外作者还和一些最近的改进transformer工作做了PPL对比，说明他的效果是真的好，而不是只比"香草"好</p><p>这些实验应该不是本篇工作主要focus的点，这里就跳过了。做的还是很好的，大家可以自己读一读原文。</p><h2 id="我的思考">我的思考</h2><ul><li><p>其实我们可以想想，这个方法在生活第n个token时，前面所有的状态仅由一个hiddendim维的向量决定，用来生成下一个hiddenstate。这个学习任务比起Autoregressivetransformer需要同时看到前面所有位置的hiddenstate信息，学习任务可以说是更简单。</p></li><li><p>从这个角度出发，估计训练效率更高，但其实表示能力应该更弱才对。然后，传统的RNN其实都是类似这个形式，所以我其实比较好奇两个问题：</p><ul><li>经典的RNN结构在相同的训练量下，和RetNet比是不是效果基本相同呢？</li><li>不知道后面有没有人尝试训一下30B、70B的版本？另外，这个6.7B的版本应该过的总token数是100Btoken，LLaMA6.7B的训练量是1.0T，差一个数量级。不知道能不能和LLaMA比，因为LLaMA也是带了各种transformer变体。我理解这种看到flattenformer state的形式，在更难得任务上可能会撞到瓶颈</li></ul></li><li><p>最后，作者没有提到两个优势</p><ul><li>那就是按照现在这个chunk训练、RNN推理的模式，其实是不受到最大seqlength影响的，因此天生就能搞到巨长的最大长度</li></ul><blockquote><p>听说Claude 2那个100k的版本就是 低层优化+硬训的，不知道anthropic看到这篇会不会直接出手</p></blockquote><ul><li>再就是，这套方法不需要positionembedding，我一直觉得transformer引入各种positionembedding变体不够优雅，这个看起来舒服多了</li></ul></li><li><p>最后吐槽一句，现在的论文名字真得起的大一点，昨天看到一个：copy isall you need，今天看到一个 a sucessor totransformer。还是得这样的才能显眼</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
            <tag> transformer结构探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Let’s Verify Step by Step</title>
      <link href="/d074522f.html"/>
      <url>/d074522f.html</url>
      
        <content type="html"><![CDATA[<p>今天聊聊OpenAI5月份发的一篇老论文：过程监督。这个说法是针对RLHF等技术的结果评价来讲的。他们使用过程监督的GPT4，在数学数据集上极大程度地战胜了结果监督的GPT4</p><p><img src="../../files/images/verify/result.png" style="zoom:33%;" ></p><span id="more"></span><p>作者团队来自openAI。其实openAI今年也没发几篇论文，我基本上都看了。今天正好分享一下这篇吧，以后有机会把别的也讲讲。</p><p><img src="../../files/images/verify/authors.png" ></p><h2 id="introduction">Introduction</h2><p>这篇论文里讲到了相对的两个概念：过程监督和结果监督。</p><p>对于一个经典的对齐场景(和人对齐)来说。正常的RLHF其实是需要人类先标注样本组谁比谁更好，由此训练一个rewardmodel，这个打分的对象是针对样本而言，换句话说就是只有生成完全结束以后，才能打出一个分数。作者认为这种打分就是结果打分。</p><p>相对的，是不是还可以进行过程监督呢？比如给每句话都打一个分数，是不是就比结果判断要更准确呢？</p><p>听起来挺合理，但是对齐场景这个不太好实现，毕竟我们也不知道到哪里可以算是一个"step"</p><p>作者另辟蹊径，找了一个好判断的场景：数学题。作者把解决一个问题需要多步推导的每一个步骤视为一个step，由此进行了一波人工标注。</p><p><img src="../../files/images/verify/step.png" ></p><p>总体而言，</p><ul><li>由过程监督的数据训练出来的打分模型和由结果监督的数据训练出来的打分模型，在learning-from-best-of-N实验中差异巨大，过程监督效果更好</li><li>持续的打分、对抗、打分过程可以加速收敛</li><li>作者开源了带有过程标注打分的数据集</li></ul><h2 id="方法">方法</h2><h3 id="说明与预处理">说明与预处理</h3><p>作者把两种模型分别称为ORM(outcome)、PRM(process)。</p><p>所有的训练使用不同大小的原始GPT4模型进行(而不是RLHF版本的GPT4)。</p><blockquote><p>largemodel就是完全体GPT4。而小号模型是一个用了1/200训练资源的小号GPT4</p></blockquote><p>作者把生成数学题答案的模型称为generator。所有的generator是不优化自己参数的，因为本论文聚焦于对比过程打分和结果打分。</p><blockquote><p>其实有了这个模型，是可以由此进行一个多步的RL训练的。参考之前的trlx库的实现</p></blockquote><p>在所有的实验开始之前，作者先对所有模型在一个叫做mathMix的充满数学题和解答的数据集(1.5B)进行了一波finetune</p><p><img src="../../files/images/verify/mathMix.png" ></p><p>👆🏻数据组成是这样的。不幸的是这个数据集没开源，只能参考一下。</p><p>接下来，作者从Math数据集选了个测试集，和上面的finetune数据集去重了一下，保证没有交集。</p><p>对于generator，作者为了保证模型生成的答案是一行一行的。作者找了Math数据集的训练集，让里面的解答都是一行一行的形式，在上面微调了一下。这样generator的输出就永远是一行一行的了。</p><p>最终正式开始实验</p><h3 id="prm800k数据集">PRM800K数据集</h3><p>这一个环节，作者要制作训练数据集。假设已经有了一个generator。</p><p>首先让generator对问题生成一些多步的解答，然后作者要人工标注员来标注中间打分。对于每一个中间过程分为三类：</p><ul><li>positive：对的过程，对解答有帮助</li><li>negative：错的过程</li><li>Neutral：对的过程，但是对解答没啥帮助</li></ul><p>作者提到了一个提高训练效率的trick：convincing wrong-answer 。</p><blockquote><p>convincing wrong-answer：PRM模型打分很高，但最终把答案做错的case。</p></blockquote><p>作者优先喂给human这个case来打分，相当于把数据集生活过程和PRMmodel耦合了，这会带来bias，但不知道严不严重。</p><p>因此总体的过程是一个多步的：generator生成一波、标一波、训个PRM、找出convincingwrong-answer再标、再重训PRM、再找...看作者的仓库，这个过程整个持续了10轮。</p><blockquote><p>折磨标注员……</p></blockquote><h3 id="prm-and-orm">PRM and ORM</h3><p>接下来作者讲解PRM和ORM都是怎么训练的</p><ul><li>ORM和RLHF的reward model训法很像：</li></ul><p>首先对于基础模型，最后的hidden_state后加一个linear(n,2) +softmax映射到2分类概率，然后对于语句的所有位置都有个概率，作者希望所有位置都要优化都有loss，和最终解答本身的类型一致。在测试时，作者选择EOStoken位置的分类概率作为最终的得分。</p><blockquote><p>作者提到了假阳问题：过程错了，但有的时候结果不会错，负负得正了。这个bias是ORM方法本身没法解决的</p></blockquote><ul><li>PRM训练方式有所不同：作者只关注第一次出现错误的step位置，后面的step只能都丢掉，因为第一次错误以后后面的步骤就没法判断了。因此数据全是一些step，前面全是positive、netrual，最后一个negative</li></ul><p>同样的模型结构，但是每行文本作者只计算最后一个token的分类概率算loss。</p><h2 id="实验">实验</h2><h3 id="大模型实验">大模型实验</h3><p>作者先做了一个大模型的实验，即两个完全体GPT4分别作为generator和监督模型，所有的方法都是对抗攻击的形式。有以下几个变体</p><ul><li>ORM：一道题生成多个解答，调出所有解答里得分最高的作为答案</li><li>PRM：一道题生成多个解答，调出所有解答里得分最高的作为答案</li></ul><blockquote><p>PRM怎么给一道题的得分呢？作者说到对于每个step，都能做三分类。如果negative概率&gt;20%就算这个step错，否则就算对。第一次输出"错"时结束。因此评价两个解答可以用"对"标签的总数量</p></blockquote><ul><li>majority voting：生成多个解答，选择数量最多的相同答案作为答案</li></ul><p>测试时作者只检查答案是否正确，由此就跑出来了首页的图：</p><ul><li>PRM &gt; ORM <span class="math inline">\(\approx\)</span> major</li><li>PRM和ORM的差距随着采样次数的增多被进一步拉大</li></ul><blockquote><p>一道题用GPT4采样1860次，只能说：this is openAI</p></blockquote><p>以上实验都是展现OpenAI雄厚财力和资本，后面是openAI研究员敏锐思路的展现。</p><h3 id="小规模对比实验">小规模对比实验</h3><p>作者谈到，上面的实验对ORM并不公平，因为：</p><ul><li>构造训练数据时是针对PRM做了对抗的制作，因此数据集肯定对训练PRM有利，并且没做出正确答案的题目的数量更多，对ORM来说标签不平衡</li><li>训练ORM时判断标签用的是自动化检查，检测答案对不对，因此有假阳。对ORM训练不利</li></ul><p>怎么做出公平的实验呢？作者说：用大模型监督小模型。作者把上一节训出来的完全体GPT4-PRM称为PRM-large,由此做消融实验</p><h4 id="prm-vs-orm">PRM vs ORM</h4><p>作者现在用小模型做generator，生成一堆答案，用PRM-large给出三种标签：</p><ul><li>PRM-large给出的过程标签：和大模型实验方法相同</li><li>PRM-large给出的结果标签</li></ul><blockquote><p>PRM-large怎么给结果标签？作者说方法是：当且仅当所有step都打出"对"标签</p></blockquote><ul><li>自动化answer检查给出的结果标签</li></ul><p>拿到这些标签，就当是人标的，训练一个小号的PRM-small、ORM-small，然后用小号的generator-small做best-of-n实验对抗rm-small</p><p><img src="../../files/images/verify/ablation.png" ></p><p>由此，作者跑出来两个图：</p><ul><li>左边的图横轴是训练PRM-large用的一道题用generator采样的样本数，纵坐标是正确率。</li></ul><blockquote><p>采样的样本数越多，越容易作为对比看出来一道题的答案的泛化性，另一方面也要消耗更多的标注资源</p></blockquote><ul><li>右边的图横轴是“一道题sample多少个样本”(测试资源消耗)，纵坐标是正确率。</li></ul><p>可以看出来，大模型试验得到的性质在严谨的消融实验中仍然保留</p><blockquote><p>不过这里也可以看到，用小generator以后，这效果直接腰斩了。比如N=10时，大号是68%，小号是22%，可以说是天壤之别。插一句：难道这也是emergentability?</p></blockquote><h4 id="active-learning">Active Learning</h4><p>这一部分作者去消融上面提到的选数据的问题。首先训一个小PRM，每道题一条数据。然后给generator生成的每道题1000条数据打分，选出来一些数据：</p><ul><li>80%是convincing wrong-answer</li><li>20%是剩下的最convincing的答案</li></ul><p>挑出来这些数据的子集，再训练新的RM-small，由此做出来的上面的图左(横轴是挑选的训练数据规模)</p><ul><li><p>这个场景叫做active learning: 因为所有数据都是小reward model认为convincing的，并且80%都是错的。和真正标注PRM800K时的比例一致。</p></li><li><p>没有用这个挑选trick，而是随机挑选同样大小的数据集，就是阴性对照。</p></li></ul><p>对比图做的两条线，作者说：activelearning方法有2.6倍的数据效率，即仅用1/2.6的训练数据就能得到传统收集方法的效果</p><h2 id="ood">OOD</h2><p>数据外分布测试：作者尝试了Math以外的数据集(和PRM、ORM的训练数据不一致)</p><p><img src="../../files/images/verify/OOD.png" ></p><p>可以发现数据规律和前面的实验一致</p><h2 id="discussion">Discussion</h2><p>这一部分作者谈了一些思考</p><blockquote><p>CreditAssignment：这个概念是强化学习、多智能体博弈里的。是说如果一个智能体发现别的智能体都干得很好，自己做了就是多做多错，那最后干脆就学着不干活了。</p></blockquote><p>一方面，PRM能减少CreditAssignment问题，因为即使做错了题目，最起码前面一些step是对的，换句话说，能区分出来错是错在哪。</p><p>相比之下，ORM只能看到对或者错，如果数据里错的&gt;对的，那最后就学着不干活了。换句话说，这种情况下，ORM的错误标签基本没有有效、的可以学习的信息</p><p>另一方面，RPM能指导对齐领域的发展。嗯，我感觉still a long way togo……</p><p>最后，作者还谈到测试集、预训练集重合的问题。作者说没法完全保证不重合，只能说做了去重。嗯，好像也没啥别的办法了</p><h2 id="我的思考">我的思考</h2><ul><li>很好的工作，方法好、思路好、结果好，我们都相信过程监督&gt;结果监督，但他们第一个标完数据做了一篇工作出来。而且，discussion部分谈到的"惰性"、"有效信息"的观点我很喜欢，感觉科研思路又被拓展了一个方向</li><li>类比人学习的方法论：我们有接受到过程监督吗？其实玩LOL，老输老赢就是结果监督。如果你下去自己复盘、或者找同学复盘，就是过程监督，看起来我们早就在这样做了</li></ul><blockquote><p>但是人可以自我复盘，模型可以吗？reflexion方式？</p></blockquote><ul><li>GPT4真强啊，Math数据集都打到快80%……我自己能做出来50%吗……</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Extending Context Window of Large Language Models via Position Interpolation</title>
      <link href="/abce9d24.html"/>
      <url>/abce9d24.html</url>
      
        <content type="html"><![CDATA[<p>如何仅用1000步训练(0.01%资源)就将一个在2kcontext长度上训练的预训练模型的上下文窗口拓展到32k</p><blockquote><p>我其实不想讲这篇，因为我觉得苏剑林老师肯定会讲，并且讲的比我好，但是感觉这个方法还是很有研究价值的，因此分享给大家……</p></blockquote><p><img src="../files/images/position_interpolation/method.png"></p><span id="more"></span><p>作者团队是meta AI的田渊栋团队，他们之前做过挺多有意思的工作的</p><p><img src="../files/images/position_interpolation/authors.png"></p><h2 id="rope">ROPE</h2><p>这篇工作重点解决ROPE位置编码的预训练模型的问题，那么就先讲讲ROPE。具体可以看这篇</p><blockquote><p><ahref="https://kexue.fm/archives/8265">博采众长的旋转位置编码</a>，写的比我写的好多了</p></blockquote><p>简单来说，ROPE有如下特点：</p><ul><li>不需要训练位置编码的参数</li><li>使用绝对位置编码的形式，就可以实现相对位置编码的效果。因此可以和FlashAttention等技术拼接来提高训练效率</li><li>理论上具有推广性，并不局限在整数"位置"</li></ul><p><img src="../files/images/position_interpolation/ROPE.png" style="zoom: 25%;" ></p><h2 id="method">method</h2><p>在本篇工作中，作者主要探索了怎么把一个ROPE训练的在2k，4k，8k的模型拓展到32kcontext window的场景。</p><p><img src="../files/images/position_interpolation/difference.png" ></p><p>作者提到传统的ROPE算法在超出最大距离后，误差值就会爆炸，因此直接推广的效果一定是很差的，因此作者绕过了推广的限制：</p><blockquote><p>外推不行，能不能内插？</p></blockquote><p>答案是可以。 <span class="math display">\[f&#39;(x,m) = f(x, m \frac{L}{L&#39;})\]</span> 其中x是wordembedding，m是位置。对于一个超过之前最大距离L的样本(距离为L'),通过内推将所有位置映射回L区间，每个位置都不再是整数。通过这种方式在长样本区间训练1000步，就能很好的应用到长样本上</p><p>作者用一堆数学证明这个的正确性，我没看懂，就不讲了。大家感兴趣可以自己看一看</p><p>接下来的实验部分都很简单，就是报告了一下效果。</p><h2 id="我的思考">我的思考</h2><ul><li>这篇论文的方法巨简单，但是效果很好、也很高效</li><li>我们可以思考一下：为什么这种方法可以work？我觉得主要是因为ROPE算法是一个推广的算法，本身就能拓展到非整数位置。理论上这类算法都可以这样拓展，比如BERT使用的那个基于cosine函数的绝对位置编码？</li><li>如果fine-tune时跑1000步就能这么好的话，那么在预训练阶段直接就加入这个内推过程是不是就更好呢？</li><li>如果从抽象的角度理解：看longtext是一种看短text自然推广出的能力？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
            <tag> transformer结构探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本科毕业感想</title>
      <link href="/3efac350.html"/>
      <url>/3efac350.html</url>
      
        <content type="html"><![CDATA[<p>其实我是个没有啥仪式感的人，毕业的典礼等活动重点安排在了6.23-6.26，当时真的是很忙的几天，我最开始觉得很烦、耽误我科研。</p><p><img src="../files/images/毕业感想/安排.png"></p><p>但后来流程走下来觉得也挺好，似乎对华子有了点归属感。好笑的是，直到毕业才感到一点归属感。在这一点上，华子做的挺失败的，我来华子这几年确实也没培养出多少归属感：</p><ul><li>从新生训练营来开始，到军训时打散班级来分成“排”，班级凝聚力的重要环节就这样丧失了。</li><li>军训结束大一期间大家因为分流的压力，有一种隐隐的竞争关系，其实也没什么凝聚力可言。</li><li>分流结束本以为会好点，结果来了贵系这边就是三把火：<ul><li>首先是把分流同学都分到一个班，和原本的院系同学基本不互通</li><li>然后是强制要求换宿舍，闹得分过来和分过去的同学很折腾、不换宿舍但要换舍友的同学很反感</li><li>最后是通知，软院那边本来一直都有辅导员通知选课时间之类的，来了这边直接没人通知靠自己发现、偷偷抢水课。我差点没选上课，算是让我对贵系的氛围有了初步的体验</li></ul></li></ul><blockquote><p>说个好玩的，之前在树洞看到同学吐槽“贵系的同学压迫感很强”，感觉能在这种地方存活的同学，压迫感应该都挺强的吧……</p></blockquote><ul><li>几年就基本摆了，我连任了三届团支书(我不太想当，但因为别人都不想当)，活动也没办几个，班级荣誉不复大一的荣光，连续三年没评上甲团。班级氛围也就那样：见不到人、说不上话、只有少数一些同学给个面子捧捧场。做为班委，我也没什么成就感，估计同学也没有收获感。但听说别的院系还有同学直接质疑班委的活动组织，想想看还是感谢同学们高抬贵手。所谓人比人气死人，我们隔壁的95班就荣誉等身，一路拿到北京市优秀班集体，看来我确实没啥管理的天分</li><li>到最后几天，楼长一直在催着收拾宿舍、离校、给下届同学腾地方，有一种“自己已经成了累赘”的微妙感觉</li></ul><p>除了班级氛围，院系的氛围也大差不差：一汪死水。不只是班级工作难以开展，学生会工作也是难以开展。当时开学代会换届，选代表的事情，基本上消息逐级传递，每一级都是ddl战士，每一级留给后一级的时间就更少，同学问我"为什么时间这么赶"，我只能和维特根斯坦一样笑笑，可能这就是宇宙万法的源头吧。也不怪学生会，学校决策层其实也是这样：逐级传递、逐级丢包……从班级到院系到学校，荣誉感基本都没有建立起来：之前坐出租车听着司机师傅一直说”华子和日本合作当汉奸“之类的，我也没什么维护学校荣誉感的愤怒，反而有种看乐子的局外人感觉；毕业典礼上西芹关于地外生命的讲话同学也是笑笑骂骂，总之没多少感动。</p><blockquote><p>仔细想想，可能还是在软院那一年生活最顺心。</p></blockquote><p>院系氛围虽然很一般，但自己的小圈子还是挺快乐的。大学几年认识的人虽然不多，但相处挺愉快的。看来人文关怀不能指望院系，还得下放到个人。自己变化还是挺大的，可能比起高中时情商提高了不少，虽然高中时期的我也许、可能更适合贵系的氛围..?最后一个学期更是加入街舞社认识了很多其他院系的同学，这是最让我感觉到贵系古怪的地方：我总觉得和街舞社同学交流似乎有某种理解壁障，和本系同学说话就没有，难道是真的"鬼系把人变成鬼"？</p><p>好像说跑题了，说回我自己的学业。可能和大多数人不同，我自己对于专业还是很感兴趣的，也真心希望可以在大学期间学到最多的、最好是不同领域的知识。我其实还是选了不少有趣的课的：</p><ul><li>专业内的基本上我软硬件都有涉猎，各个研究所的课都上过</li><li>专业外的基本上也是很多好玩的：解刨、西方音乐史、现代舞、拉丁语、相声、意大利语、围棋……</li></ul><p>因此，我个人的学习体验也还是不错的，大学几年的知识收获是人生中最多的几年。我在高中毕业时其实很担心自己不会编程会不会跟不上、或者上完大学也学不懂。后来发现也都能跟上，而且真的能学会好多知识。知识的海洋真的是深不见底呀。</p><blockquote><p>很多人说贵系的课太卷、太大量。前几天和叉院的同学交流，觉得这个说法是有道理的。而且这其实是恶性循环，你卷他也卷。重点是学校的评价机制就是这个基于rank的GPA机制。听说外国很多高校因为亚裔卷的问题已经把亚裔的排名拿出来单独计算了，不和别的国家同学一起计算。其实今年零字班和书院同学一起卷名额，可能也会变成这样了。这也不能说是贵系的问题，只能说贵系的课因为比较容易在客观区分表现好坏，因此把这个问题放大了。</p></blockquote><p>我自己对于大学的重心其实也是有变化的，基本上前两年是以可能成绩为主，或者说重心放到课上学的知识，有时间也愿意了解一些拓展的内容。后面就转向科研为主了，重点了解NLP方向的最新论文、代码方法之类的。不知道这对不对，感觉学校应该是希望本科期间重点学习课内知识。</p><blockquote><p>和知识交流总是比和人交流更轻松，毕竟我不需要说话反馈，只需要看它表演就行了。</p></blockquote><p>总而言之，不管什么方式、不管学到什么，最终都是走到了本科的最后，都在大礼堂前扔个帽子、笑一下、拍个照。昨天打车出校园，一边回望校门，一边听着司机师傅骂清华，心情还是很复杂的：有“永别了牢笼”的解脱感，有"回家就不能以毕业季未为由继续摆"的沉闷感，有“暂失室友、npy”的惜别、有“本科生活一去不复返”的感慨、还有"未来的宿舍、室友、生活会是什么样"的期待……没有拽词，这些感觉当时真的都有，很难描述。之前初中、高中毕业的感觉都没有这么复杂，看来是真的长大了呀。</p><p>之前听过一个说法：忆往昔说明人生差不多走到顶峰了，上升期的人总是向前看。不过对我而言，"后面"是清华本科，"前面"是清华读研，其实也没理由伤感的。前几天和室友唱歌唱了我很喜欢的《平凡的一天》，就拿这里面的一句歌词收尾吧：</p><blockquote><p>从不考虑明天应该，去哪里；因为今夜的风，太和煦。</p></blockquote><p>好神奇好复杂呀，不知道人工智能，能不能体验到我现在的感情</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title>
      <link href="/f68f4e16.html"/>
      <url>/f68f4e16.html</url>
      
        <content type="html"><![CDATA[<p>好长时间没写论文阅读笔记了，今天读一下LeCun讲了一年的"世界模型"：新的训练范式、训练快、参数少(0.6B)、效果好、方法简单、概念明确。</p><p>我在讲解时会说一些我的思路，因此我里面提到的一些优点、缺点有一些不是论文里说的是我自己的观点，完整故事逻辑大家可以去看原论文，论文写得很好。</p><p><img src="../files/images/I_JEPA/performance.png"></p><span id="more"></span><p>作者团队就是LeCun团队</p><p><img src="../files/images/I_JEPA/authors.png"></p><h2 id="introduction">Introduction</h2><p>自监督学习一直是实现通用人工智能的关键。</p><blockquote><p>我的理解：</p><p>1.所有的自监督学习方法都是建立在数据的基础上，更多数据，更多智能。</p><p>2."无监督"数据其实并不是真正的无监督。举个例子，文本都是一段一段的，前文后文有关系，所以我们可以开发auto-regressive。CLIP中选用了互联网(图像-文本)对也是天然含有相关性的。自监督学习其实是在挖掘数据中这种通用的、数据天然包含的、自相关的关系。</p></blockquote><p>作者在这篇文章中主要讲了一种新的自监督学习的架构，因此对标的是之前的几种训练范式，作者在这里首先归纳了已有的几种训练范式</p><p><img src="../files/images/I_JEPA/arch.png"></p><p>所有的范式都需要找到相关的数据 x,y。</p><ul><li>这里的相关可以是：x、y是同一张图片，但可能x是被mask一部分信息的</li><li>也可以是x、y是不同模态的描述同一信息的数据</li></ul><h3 id="编码式">编码式</h3><p>首先是Join-EmbeddingArchitecture,用两个编码器分别给相关的x,y进行编码，然后通过编码<spanclass="math inline">\(s_x,s_y\)</span>之间相似度来计算loss</p><p>这种编码方式的优势是训练比较快，但是有个主要的问题就是representationcollapse,如果编码器永远输出0，那么相似度肯定一直是0，阁下又将如何应对？为了解决这个问题，有一些工作从各种地方减少之</p><ul><li>对比学习：前几年大火的对比学习在正样本的基础上，再引入负样本，在正正样本相似度大的基础上，让正负样本之间的相似度尽可能小。</li><li>non-contrastive loss：尽量减小信息熵，这个领域我没了解过</li><li>聚类：把相似的物体天生聚类到一起，这个可以视为对比学习的一种特殊形式？</li></ul><p>另外，作为经验规律，除了mask之外，还要做各种各样的数据增强来获取正样本，这个工作很dirty，但是对这类方法的最终效果非常重要。不优雅。</p><h3 id="生成式">生成式</h3><p>另外一大类方法是生成式，将x编码后的结果用一个解码器解码回y，然后在obj粒度计算相似度。其实GPT可以视为这类方法，如果x是上文，y是最后一个单词，z是空的话。</p><p>这种方法天生没有collapse的问题，因为<spanclass="math inline">\(s_x\)</span>是可训练的，但y永远是锁定的。不过这类方法带来另外的问题是过于关注局部细节，这会导致大量的训练资源浪费在不重要的地方。因此训练速度一般也很慢，质量也无法保证(支持的例子可以看diffusion系列的工作)</p><ul><li>对于图像，80%的像素其实都是没啥信息的，换句话说是不可预测的，但模型并不能区分哪里是不可预测的，而且这个loss对于所有像素一视同仁。</li><li>进一步看，对于文本模态，可以用之前笔记提到的数据集天然随机性解释，crosseentropyloss有下限。</li></ul><p>这个问题没法显式地解决，因为”无监督“这个条件本身就代表无法预测。</p><h3 id="jepa">JEPA</h3><p>作者想到了另外的办法，能不能结合上面的范式，利用双方的优点呢？</p><p>作者认为，可以做生成，但计算loss要在representation维度。这个可以天生避免上面说的无效信息问题：因为编码器在训练过程中对于x和y都可以自动忽略无效信息</p><p>不过，和编码式结构一样，这种方法仍然面临塌缩问题。</p><h2 id="method">method</h2><p>具体来看，作者本篇工作聚焦于图像模态，这样设计I-JEPA工作流程图</p><p><img src="../files/images/I_JEPA/method.png"></p><p>首先对于一张完整的图片，采样一部分作为context，这是可见的已知信息，然后剩下的部分mask掉，接着用一个编码器编码context得到patchembedding。</p><p>编码完以后将需要预测的部分换成 [MASK]token，然后扔进解码器获取解码到的这些patch的embedding。最后对mask位置的真实像素用一个编码器编码，将预测的编码与真实的编码计算L2loss</p><p><img src="../files/images/I_JEPA/mask.png"></p><p>在实际训练中，作者先</p><ul><li>给一个图片随机采样了4个面积占比(0.15-0.2)、长宽比(0.75-1.5)的长方形块作为targets，</li><li>接着sample了一个面积占比(0.85-1.0)的大长方形块作为context</li><li>最后将context中和target交集的部分删去，保证不包含target的信息</li></ul><h2 id="实验">实验</h2><h3 id="效果">效果</h3><p><img src="../files/images/I_JEPA/result.png"></p><p>在实现细节中，作者需要三个模型：context-encoder、decoder、target-encoder。这三个模型都是可训练的、然后都用ViT架构。作者提到了一个训练trick：target-encoder在参数更新时使用EMA移动平均来更新，评价指数0.996……</p><p>使用16个A100用时72h就完成了训练。</p><p>在分类任务上，作者对比了锁参encoder然后加一个linear分类头fine-tune的结果，发现效果很好(左图)</p><p>接着，作者尝试了当训练数据只有1%是的效果，发现比起其他模型，效果下降更少，说明泛化能力更强</p><p><img src="../files/images/I_JEPA/transfer.png"></p><p>然后作者还测试了迁移能力</p><blockquote><p>迁移是指：训练使用imageNet，因此imageNet测试集和训练集是一个数据分布。如果在别的分类任务上测试，数据分布不同，就需要模型有迁移能力</p></blockquote><p>同样是分类任务，和1%数据的分类任务，发现I-JEPA的性能依旧稳定</p><h3 id="scalability">scalability</h3><ul><li>作者测试了模型规模的影响，发现模型越大，效果越好，和transformer的特性一致</li><li>测试了训练数据规模的影响，发现训练数据更多，也会使得模型效果更好</li></ul><p><img src="../files/images/I_JEPA/dataset.png"></p><h3 id="消融实验">消融实验</h3><p><img src="../files/images/I_JEPA/mask_method.png"></p><p>作者尝试了不同的mask方法，发现这一套mask的效果最好。我感觉不只是最好的问题，其实这个方法好像对于mask方式非常的敏感，差一点效果会天差地别。(54.2&gt;&gt;15.5)。</p><p><img src="../files/images/I_JEPA/loss_dimension.png"></p><p>最后，作者还说了之前提到的用像素粒度还是representation粒度计算loss的实验，发现用pixel计算相似度(抛弃target-encoder部分)，会导致效果显著下降。不知道是不是因为绝大多数的训练资源花在了计算没有意义、不可预测的信息上。</p><h2 id="我的思考">我的思考</h2><ul><li>作者在提到几种范式的时候，说他们都能用energy-basedmodel的理论去解释。记得之前经典diffusion算法火的时候就有一批研究者和energy-model联系在一起了。我一直没看过这个理论，找个机会读读、讲讲energy-model吧</li><li>这一套讲故事的逻辑看起来很像T5：先说BERT编码模型xxx，接着说GPT模型xxx。最后来了个"看到一部分上下文基础上，预测剩下的部分"的prefixmodel</li></ul><blockquote><p>从这里出发，可以思考两个问题：</p><p>1.为什么后来的NLP还是decoder-only占了上风呢？</p><p>2.按作者的说法，NLP是在物体模态做loss。这种办法的主要问题是模态迁移能力比较差。</p></blockquote><ul><li><p>LeCun之前一直在推的"世界模型"理论，主要是认为GPT这一套不可解释，不能通向真正的智能。需要让模型有两个能力</p><ul><li>在多个模态间迁移，真正观察整个世界的能力</li><li>可以真正做逻辑推理的架构(auto-regressivetransformer架构没有显式地给逻辑推理留出空间)</li></ul><p>这套JEPA方法上是通过观察世界的一部分信息，去预测剩下部分的语义信息(而不是预测剩下部分)。故事上讲得通，但我没想清楚为什么这会给逻辑推理留出空间？？</p></li><li><p>最后，我们可以简单展望一下</p><ul><li>首先这个方法不受模态限制，只是这篇聚焦于图像模态(Image-basedJEPA)，后面显然有多模态版本的。联系一下几周前meta发的那个ImageBind一个模态看所有模态的论文。</li><li>另外，作者没细说那个mask的问题。我个人感觉，如果图像模态都对于mask方式这么敏感的话，其他模态想调试好估计挺难的。</li></ul><blockquote><p>不过这也符合人类学习的方式：顺序很重要、有效信息很重要？</p></blockquote><ul><li>再就是，作者这种三个参与训练的模型的方式，感觉有点不优雅，就像PPO一样？</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-RRHF: Rank Responses to Align Language Models with Human Feedback without tears</title>
      <link href="/feddc200.html"/>
      <url>/feddc200.html</url>
      
        <content type="html"><![CDATA[<p>论文介绍了一个非常简单的RLHF中PPO的替代品，昨天听了作者的报告，今天来仔细读读。我认为它的思路和calibration有一定的关系。</p><p><img src="../files/images/RRHF/intro.png"></p><span id="more"></span><p>作者来自清华和达摩院</p><p><img src="../files/images/RRHF/authors.png"></p><h2 id="introdution">introdution</h2><p>作者在introduction阶段主要介绍了一下传统的RLHF框架的过程，之前的博客也有讲过<a href="/5d2d4022.html" title="论文阅读[粗读]-强化学习和RLHF中的PPO算法">论文阅读-粗读-强化学习和RLHF中的PPO算法</a>。PPO算法虽然好，但有个巨大的问题，需要很多个模型放在一起：</p><ul><li>首先预训练模型的参数作为ref_model要存一份，计算全局的KL散度</li><li>每次更新参数时old_model要存一份，计算PPO内KL散度</li><li>训练的模型得有一份</li><li>计算Value的model也得有一份，这个可以和训练的模型共享参数</li><li>更新参数时的EMA model又得有一份</li></ul><p>这意味着什么呢？加入我们训练一个7B的LLaMA，按16位half精度存储，加载时大约需要14G显存。上面的模型只有训练model需要算梯度，那么显存就要翻倍，我们就当valuemodel共享参数不需要占显存。最后总体就要5倍，也就是70GB显存。这还不算前向时batch带来的开销。</p><p>假如只使用40GB显存的A100，甚至不能放到一张卡上。</p><p>另一方面，PPO需要调很多的超参，<span class="math inline">\(\alpha,\beta,\gamma,\lambda\)</span>等等，而且这些超参都很敏感，总体而言，PPO的训练还是很难的。</p><p>再有，PPO要求采集到的数据只能来自模型自己，甚至还必须要是top1解码出来的，如果用sample，可能训练就会崩溃。</p><p>因此作者提出了一种更省空间、需要训练的模型和需要调的超参都很少，并且在很多模型上做了实验</p><h2 id="method">method</h2><p>那么这个方法是什么呢？作者称为RRHF(rank responses to align humanfeedback)。具体的改进是这样</p><p>RLHF的前两步SFT、rewardmodeltrain不改变，因此我们已经有一个一个rewardmodel</p><p>对于第三步，正常按照actor-critic的算法，我们需要一个valuemodel，但这种manner其实破坏了语言模型的对称性，因为value基本都是从小到大的，就像是围棋某一局棋的胜率预测分布。但语言其实一个一个token就基本齐次的关系。再有就是valuemodel和费空间</p><p>因此作者要去掉这一部分，但是又要和rewardmodel形成一个训练对抗的关系，因此作者希望模型可以输出一个伪的reward。这里作者直接让model在输出输出nexttoken logits的同时，用所有有效位置的logprob的平均值代表reward，保证在[0,1]范围内 <span class="math display">\[p_i = \frac{\sum_t \log P_{\pi}(y_{i,t}| x, y_{i,&lt;t}) }{||y_{i}||}\]</span> 如果每次来的不是一个数据，而是一个数据对的话，rewardmodel可以先给所有candidate一个得分<spanclass="math inline">\(r_i\)</span>，然后模型对于所有candidate也会输出<spanclass="math inline">\(p_i\)</span></p><p>对于每个真实的偏序关系，作者定义一个loss <spanclass="math display">\[\mathcal{L}_{\text{rank}} = \sum_{r_i &lt; r_j} max(0,p_i - p_j)\]</span>也就是说希望模型输出的"reward"也可以满足这个偏序关系，但这里为什么要用0clamp，似乎没有解释。作者只是说借鉴了</p><blockquote><p>BRIO: Bringing Order to Abstractive Summarization</p></blockquote><p>另外，为了保证模型不炼崩，作者还引入了一个SFTloss，就是找到每组candidate里评分最高的 <span class="math display">\[i&#39; = \underset{i}{ \arg \max}(r_i)\]</span> 然后直接搞个crossentropy loss <span class="math display">\[\mathcal{L}_{ft} = - \sum_t \log P_\pi (y_{i&#39;,t} | x,y_{i&#39;,&lt;t} )\]</span> 最后的总体loss是加和，甚至没有加权重 <spanclass="math display">\[\mathcal{L} = \mathcal{L}_{\text{rank}} + \mathcal{L}_{ft}\]</span> 作者提到有试过给rank loss加权重，但加了都比不加效果更差</p><p>作者后面讨论了这种方法和传统RLHF的步骤的联系</p><ul><li>对比SFT：可以认为RRHF可以推导出SFT，只要把candidate固定，同时每组只有一个数据</li><li>对比rewardmodel train: 本方法让模型的logprob代表reward，而不是EOStoken过linear</li><li>对比PPO：RRHF可以使用任何模型生成candidate</li></ul><h2 id="experiment">experiment</h2><p>作者尝试了RLHF数据集，用以下几个模型做candidate</p><ul><li>LLaMA 7B</li><li>Alpaca: llama+instruction-tuning</li><li>AlpacaSFT: ALpaca+sft</li></ul><p>然后生成candidate时有不同的变种</p><p><img src="../files/images/RRHF/candi.png"></p><p>这个是指作者在训练RRHF时，每组都有6个candidate，<spanclass="math inline">\(\rho_1,..,\rho_6\)</span>他们的来源不同</p><ul><li>左边的四个candiate是模型自己生成的，右边的两个来源于数据集自带的好坏pair</li><li>由于不限制采样方法，作者使用了beam-search和一个更强的叫diversebeam-search的算法</li></ul><p>作者报告了reward model的得分。reward model是一个开源社区训练好的6BGPTJ</p><p><img src="../files/images/RRHF/results.png"></p><p>上面的图最上面的<spanclass="math inline">\(\emptyset\)</span>代表不训练，中间PPO是用PPO训练，下面的setting和对应图里面的解码方法。其实还是有很多好玩的点的：</p><ul><li>Alpaca不训练就比数据集中good response的平均值还要好</li><li>LLama直接跑PPO或者RRHF都会导致PPL爆炸</li><li>RRHF和PPO效果不相上下</li></ul><p>作者还对比了不同的解码方法</p><p><img src="../files/images/RRHF/decode.png"></p><p>可以发现BP和DP的效果差距不大。另外，这个方法也是可以迭代做的</p><blockquote><p>原始RRHF是可以先用原模型生成一堆candidate，然后算完reward，把rewardmodeldelete掉，再直接跑训练，这个称为一轮</p><p>第一轮训完，可以递归地做这件事。注意从第二轮开始可能就也得加kl了</p></blockquote><p>作者把多轮训练的版本叫做IP-1，IP-2...可以发现，持续的训练会导致reward进一步的衰减</p><p>另外，只有D的setting，也就是纯用model自己生成的数据效果也不错。</p><p>另外一个有趣的点是，有一个领域叫做learn fromBest-of-N,大概就是采样N个数据，直接拿最好的来finetune，那么这个和RRHF有啥关系呢？其实去掉rankloss就是learnfromBest-of-N。其实从上面的图可以大概看出来，因为训练完以后的reward大概相当于训练前所有candidate的平均reward</p><p>因此作者做了这个对比实验</p><p><img src="../files/images/RRHF/rank_loss.png"></p><p>发现这个rank loss其实是很重要的</p><h2 id="我的思考">我的思考</h2><ul><li>这个方法感觉有点像蒸馏，因为候选candidate里还有别的模型出的infer。不知道换chatGPT的会不会违反用户协议</li><li>感觉这个思路和pretrainloss更接近，毕竟也是按token一视同仁的优化，就相当于变了个学习率？</li><li>还有从calibration的角度理解的话，这个相当于显式地把calibration的偏序关系和实际reward的偏序关系定义进了训练任务中</li><li>如果引入一些对比学习领域的思路，也许直接avg logprob是不是不在一个隐空间中？也许可以试试加一个linear？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-5-6总结</title>
      <link href="/db37895b.html"/>
      <url>/db37895b.html</url>
      
        <content type="html"><![CDATA[<p>好久没写随笔了，今天一写突然发现好像博客快要更新一年了。这下子随笔的标题得把年份加上，和往年的时间加以区别了。笑死，让我想起了"千年虫"事件。这下我的博客要发生”一年虫“危机了。</p><blockquote><p>千年虫：曾经的计算机使用2位十进制数计年，所以到了横跨世纪的时候就会报错</p></blockquote><span id="more"></span><p>五一假期刚刚过完，今年学校放五一假期时间很长，具体我体感不太清楚，但似乎最少放了五天假，比起前几年疫情时期把五一、十一假期取消的离谱操作，可以说是充满了温馨。依我来看，这恐怕是西芹上台以来，做的最大的一件好事了。假期虽然长，朋友圈的足迹也基本踏破了大江南北、天南海北，但我并没有做什么，除了大吃大喝了几顿……好吧，无论有没有假期我都会出去大吃大喝，没什么所谓仪式感。仔细想来，和”劳动节“关系最大的事情可能就是把家里的马桶修好了，四舍五入也算是劳动了。</p><p>假期的生活和最近一两个月的生活其实都差不多，按部就班，一眼望到底：毕设、科研、街舞、LPL……来回周转，好像也没什么值得专门说的。最近和做强化学习的许哲华老师交流了一下，倒不是说有多么受益良多，主要是发现许老师的生活好多彩：弹唱、舞蹈、摄影、滑雪、调酒……这样恐怕才是破折号青年吧。我发自内心喜爱这样的生活，但好像又没有勇气把这些事情做起来。常言道万事开头难，做一万件事的开头就更是一万倍的困难，希望我不要望洋兴叹一辈子吧。</p><p>生活挺单调，但倒是挺幸福快乐的。最近感觉自己的街舞初窥门径，基本每天晚上8:30-10:00去舞房和队员们联系一个多小时，买了一顶大佬跳舞时都喜欢带的帽子，一个人练习的时候也逐渐知道自己该练什么内容。我相信这是个好的开始，嗯，SFT阶段结束进入RLHF阶段...？说到这方面，我最近还真有了点把街舞和LLM联系起来的哲学思考，等再多跳跳、多学学再分享给大家吧。</p><p>......刚才读了读自己写的东西，怎么有种意识流的感觉，上了四年大学，写作能力已经退化到”毫无技巧、全是感情“的地步了吗。可恶，终究还是不行吗？算了，就这样吧。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Are Emergent Abilities of Large Language Models a Mirage?</title>
      <link href="/f1e00e18.html"/>
      <url>/f1e00e18.html</url>
      
        <content type="html"><![CDATA[<p>前两天看论文解释了emergentability的出现原因猜想和复现，论文主要表达”涌现“没什么复杂的。我也聊聊我的看法。</p><p><img src="../files/images/emergent_ability_isnt_mirage/reason.png"></p><span id="more"></span><p>作者来自斯坦福大学</p><p><img src="../files/images/emergent_ability_isnt_mirage/authors.png"></p><h2 id="论文">论文</h2><p>其实这篇论文也没什么所谓的方法，都是基于一个假设。</p><p>我们之前也聊过emergent ability的博客：<a href="/9ece5113.html" title="大模型的Emergent-Abilities和最优传输">大模型的Emergent-Abilities和最优传输</a>。里面说到其实crossentropyloss是随着模型(指数)大小线性下降，符合power-law的。</p><p>作者就从这里出发，提到：我们现在的主流测试函数和loss不是线性关系，具体而言，主流有几种测试score:<span class="math display">\[\text{Multiple Choice Grade} \overset{\text{def}}{=} \left\{\begin{aligned}&amp; 1, \text{if highest probability mass on correct option} \\&amp; 0, \text{otherwise}\end{aligned} \right.\]</span></p><p><span class="math display">\[\text{Exact String Match} \overset{\text{def}}{=} \left\{\begin{aligned}&amp; 1, \text{if output string exactly matches target string} \\&amp; 0, \text{otherwise}\end{aligned} \right.\]</span></p><p>但我们的loss之前解释过 <span class="math display">\[\mathcal{L}_{CE}(N) = - \log P_N(a_j | a_{i&lt;j}) \propto\text{model-size}\]</span> 那么对于一个单独的token，其正确的概率大概是 <spanclass="math display">\[p(\text{single token correct}) = e^{-\mathcal{L}_{CE}(N)}\]</span> Accuracy-Lc可以视为二项分布，连续L个token都猜对 。stringmatch也是一个意思 <span class="math display">\[P_L \approx p(\text{single token correct})^L \propto (\log\text{model-size})^L\]</span>因此如果以模型大小为轴，看起来，再加上非常离散的采样方式，看起来就是emergentability</p><p>由此作者提出了现在emergent ability的三个原因：</p><ul><li>选取的metric和model size不是线性的，而且选取的模型size太稀疏了(GPT-3family就4个size)</li><li>测试数据集太小，有偏差</li><li>无论什么score，增加target string length都会使得表现下降</li></ul><p>下面作者做了几个实验</p><p><img src="../files/images/emergent_ability_isnt_mirage/exp.png"></p><p>解释下这几个图，作者在GPT3 family的API上做了实验：</p><ul><li>图中的列从左到右代表三个不同的任务：Mathematical Model, 2-Integer2-Digit Multiplication Task, 2- Integer 4-Digit Addition Task</li><li>图的行上下对照是不同的metric，上面是acc。下面是token editdistance，代表错误答案最少修改几个token就能做对。这个score可以近似看做和logmodel size 成正比，去掉了L的影响：</li></ul><p><span class="math display">\[\text{Token Edit Distance}(N) ≈ L \left(1 − p_N (\text{single tokencorrect})\right) = L(1 − \exp(N/c)^α)\]</span></p><p>从图中可以看到，换成了好的score之后，emergentability的现象基本就消失了</p><p>接下来作者探索了是不是增加测试集大小，普通的score也能看起来好很多</p><p><img src="../files/images/emergent_ability_isnt_mirage/test_size.png"></p><p>发现看起来确实好了很多，不过这里有个小细节</p><blockquote><p>我戴上赛文眼镜以后，发现这个图的纵坐标变成指数坐标系了？上面图的纵坐标是线性坐标系？</p></blockquote><p><img src="../files/images/emergent_ability_isnt_mirage/big_bench.png"></p><p>作者还尝试了在big-bench上测试LaMDA一系列的模型，发现出现emergentability的绝大多数都是这种非线性metric的</p><p>最后，作者在CV领域用卷积网络”复现“了emergentability。它定义了一种非线性score</p><blockquote><p>如果连续K个样本都分类正确，就算正确。K=1时就是图片分类accuracy</p></blockquote><p><img src="../files/images/emergent_ability_isnt_mirage/cv.png"></p><p>用MNIST数据集做实验，然后稀疏采样了model-size，形成最右侧的图。看起来和最左边GPT3在NLP领域的emergentability看起来一模一样。</p><h2 id="我的思考">我的思考</h2><p>我比较认可作者的思路，但这并不是说之前讨论emergentability就没有意义了。我由此想到了几个问题。</p><p>首先，我们在实际应用中，确实就是要用这种类似二项分布的accuracy。而且这个长度L很可能会更大，几十、几百、几千。所谓量变引起质变，这其实对预训练模型领域而言是个很好的现象：虽然模型指数增长，pertokenloss线性下降；但loss线性下降，对应的是metric的级数级别的的提高。所以pertoken优化的前景仍然广阔。</p><p>其次，现在的主流metric如果是如此的非线性，那用他来衡量模型的强度显然是很不合理的。虽然不合理，但是确实符合实际应用场景的。这里面就衍生出了一个bias：有没有什么更好的衡量办法呢？</p><p>最后，现在的模型训练显然都是在做NCE、crossentropyloss，这个loss训练很稳定。但这篇文章其实揭示了另一个方面：这种pertoken的训练和下游任务某种程度上是有偏差的。这接着聊就是老生常谈的teacher-forcebias和explosurebias。但其实我们从另一个角度思考一下，现在大火的RLHF其实就是跳出了这个怪圈，从generation级别进行优化，进而得出了很好的实际应用效果。我觉得，也许未来在pertoken的预训练基础上，还能发掘出更多per-generation级别的优化方式。如果设想的再张狂一些，把级别再次拉大，奖励变得更稀疏，是不是就和我们人类对自身的优化方式更像了呢？</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大模型的Emergent-Abilities和最优传输</title>
      <link href="/9ece5113.html"/>
      <url>/9ece5113.html</url>
      
        <content type="html"><![CDATA[<p>之前看了阿西莫夫的小说《最后的问题》，里面讲到了宇宙里最令人绝望的定律”熵增定律“。今天讲讲人工智能领域最让人绝望的规律”emergentabilities“，在结合最优传输说说我对这个现象的理解，最后聊几个有趣的话题。参考：</p><blockquote><p>Emergent Abilities of Large Language Models</p><p>Can LLMs Critique and Iterate on Their Own Outputs?</p><p>压缩下一个token通向超过人类的智能</p></blockquote><span id="more"></span><p><img src="../../files/images/emergent_ability/authors.png"></p><h2 id="定义">定义</h2><p>所谓的emergentability，就是说大模型的很多能力不是随着规模一蹴而就的，而是当到达一定程度以后，突然间”爆发“出来的，就像是诺奖得主PhilipAnderson说的</p><blockquote><p>Emergence is when quantitative changes in a system result inqualitative changes in behavior.</p></blockquote><p>这种现象是不符合scaling-law的，也没有什么合理的解释。</p><p>这个现象其实之前大家都有一个模糊的认知：</p><ul><li><p>prompt、few-shot-prompt方法为什么只对很大的模型有效？</p></li><li><p>类似toolformer为什么只有6B模型才效果好？</p></li><li><p>instruction tuning类的方法对于&lt;10B的模型是负作用？</p></li></ul><p>在Googleresearch的这篇文章中，作者详细探讨了这种现象。这是一篇分析性文章，我最喜欢读分析性文章了，读起来能感觉到一种人类智能(而不是人工智能)的魅力。</p><h2 id="实验">实验</h2><p>作者一共做了两大类的实验。</p><h3 id="few-shot-prompt">few-shot prompt</h3><p>这类是说对每一个任务，给一些样例输入输出对作为prompt，然后执行任务。具体来说，作者统计BIG-Bench，truthfulQA，Groundedconceptual mappings和MMLU等场景，有一种充斥着工作量的美。</p><p>最后得到了这张图</p><p><img src="../../files/images/emergent_ability/result.png"></p><p>这个图里面每个都是一种任务，纵坐标是metric值，横坐标是FLOP。</p><blockquote><p>FLOP是训练资源的表示，基本上正比于花的钱。模型size大，同样的前向就FLOP多。训练看的语料多，需要的FLOP也多。类似Chinchilla的论文里面详细探讨了同样FLOP下扩大size、提高看的token数量谁更赚。这是一个开放性的领域，不在本博客的讨论范围。</p></blockquote><p>然后红色虚线是随机猜测的结果，剩下的是几个经典模型的不同版本的结果。</p><p>从图中我们可以看到，基本上FLOP小的时候，模型在很多任务上都处于一种随机猜测、甚至不如随机猜测的水平。</p><p>但当训练规模增大到一定程度以后，表现突然就”涌现“了，变得很好。这就是emergence现象。</p><h3 id="augmented-prompting-strategies">Augmented PromptingStrategies</h3><p>除了最简单的few-shotprompt方法以外，学界还有一些进阶的效果增强方法</p><p><img src="../../files/images/emergent_ability/result2.png"></p><ul><li>chain ofthought(CoT)是说模型在做数学题的时候自己先生成一些思考、生成一些中间步骤，再一点点做</li><li>inistructiontuning是说模型要做一些参数更新，使得可以更好的follow人类指令。这个需要提前做出一些instruction的数据集</li><li>scratchpad是一种toollearning的方法。和CoT类似，也是生成中间步骤，但这个”中间步骤“是可以和tool交互的，比如写了python代码就可以真的执行</li><li>calibration是说模型在回答完问题以后，自己对这次回答评价一下。比如说可以用这个回答是不是"True"的token概率表示。一个牛的模型，它自己的回答信心和实际的正确率是强相关的。</li></ul><p>从图中的结果我们可以看到</p><ul><li>对于垃圾小模型而言，这些进阶方法不仅没用，还是副作用</li><li>当训练大到一定程度以后，突然间这些方法就变得特别好使</li></ul><p>这就是emergent ability</p><p><img src="../../files/images/emergent_ability/threshold.png"></p><p>这里作者还详细给出了每个实验产生emergent的阈值是多少。</p><blockquote><p>这里可以看到，toollearning方法是scratchpad的阈值比别的方法低几个数量级。这也许侧面说明使用工具是大模型的下一步方向？</p></blockquote><p>在这篇文章中，作者也探讨了emergent的原因。目前没有什么公认的原因，作者给出几点猜测和验证方法：</p><ul><li>也许和训练数据集memorization有关。这一点也许可以参考之前另一篇论文。大概说的是随着模型规模增大，模型记住只见过一次的数据的概率会增大(最多能到大概5%)，这可能和模型规模增大以后的稀疏性有关，这是另一个研究领域了。</li><li>可以尝试任务拆分，观察随着规模增大，子任务的效果有没有emergent，以及主任务emergent和子任务的关系</li></ul><p>这些都是很好的研究方法，我个人也对这个问题很感兴趣。我认为，理解大模型emergent的原因和逻辑，也许是我们找到四两拨千斤，用更小的资源实现更大智能的关键。毕竟，扩大训练规模肯定会撞到硬件瓶颈，撞到瓶颈以后还没有”emergent“出来的能力就寄了。</p><blockquote><p>1.值得一提的是，目前BIG-bench还有巨多任务至今模型也没emergent出来</p></blockquote><blockquote><p>2.另外，目前的国产大模型据我观察，都没有达到emergent的阈值。也许真的和训练数据有关，morecode data is more intelligent?</p></blockquote><p>此外，作者在附录中也做了cross-entropy loss和规模的关系</p><p><img src="../../files/images/emergent_ability/loss.png"></p><p>可以看到，cross-entropy-loss还是比较符合scaling-law。但performance为什么会emergence，就不好说了。</p><h2 id="我的理解">我的理解</h2><p>以下内容是我对于emergence的一点学术理解，没有正确性保证。</p><p>之前看了一篇文章，是讲最优传输的。举个例子，如果你要传100个token，每个token都是从256的词表里的，一个朴素的方式是用8位2进制数来存储一个token，对方打表恢复。最终需要<span class="math inline">\(8*100=800\)</span>个token</p><p>大模型(auto-regressive类型)是在做什么事情呢，其实是在压缩这个长度。举个例子，假如两边都有这个模型权重，然后模型输入前缀以后，是要给next-token概率分布的。</p><p>假如现在模型的平均cross-entropy loss是3，什么意思呢？ <spanclass="math display">\[\text{cross-entropy} = - \log \text{soft-max}(\text{hidden-state}(x_i,x_{&lt;i}))\]</span></p><p><span class="math display">\[\text{soft-max}(x_i) = \frac{e^{h(x_i)}}{\sum_{j \in \text{vocab}}e^{h(x_j)}}\]</span></p><p>如果把softmax以后的logits当做模型的真实输出的话，相当于有个概率分布，其中真实的那个token<span class="math inline">\(x_i\)</span>占了总体[0,1]区间的<spanclass="math inline">\(\frac{1}{e^3} \simeq0.05\)</span>的比例。这是什么意思呢？</p><p>假如说我们用一个二分查找的方式来存储token：从0.5出发，小了就变成0.75，大了就变成0.25这种，直到这个数字落到对应token对应的区域内。这个比例说明，二分查找大约需要3次就能落到正确的位置里。最后比如走的是”左右左“就能编码成”010“</p><p><strong>换句话说，一个token大约只需要3位就能传输</strong>。这比起之前的8位就优化太多了。</p><blockquote><p>用这个角度，我们其实还可以大概预判模型的初始cross-entropyloss。比如一个n分类问题(标签平均分布)，如果没啥意外的话，初始loss大概就是 <span class="math inline">\(\log n\)</span></p></blockquote><p>所以说，模型的Autoregressive训练任务(cross-entropy或者ppl)，其实就是在显示地进行模型对于数据集压缩率。没有所谓的”世界知识“学习的过程。</p><p>那么模型为什么能得到”世界知识“呢？我们先入为主的引入一个归纳偏置：只有本身理解世界知识，才能更好地压缩token。举个例子</p><blockquote><p>”中国的首都是北京“这个串，如果你提前知道这个知识，最起码"北京"这个词的logits肯定很大，很简单就能压缩。</p></blockquote><p>那么？压缩率有没有上限呢？有！因为数据集本身就是不确定的。比如说上面那句话，"首都"这个词上帝来了也预测不出来，谁知道你这里是要说”首都“还是”国旗“、”主席“或者什么其他别的词。这个例子也说明，只有更多的上下文，才能更好、更优的建模后文的token。</p><p><img src="../../files/images/emergent_ability/logits.jpeg"></p><p>用博客里的图就是上面那张：一个代表加法的串，只有结果是确定的，输入谁来了也预测不了。所以说我们的模型压缩是有上限的，这个上限和数据集有关，和模型、方法无关。</p><p>某种意义上，这个和密码学是相反的</p><blockquote><p>密码学研究怎么编码数据让别人猜不出来，LLM研究怎么编码数据自己可以更好的猜出来。</p></blockquote><p><strong>但这和emergence现象有什么关系呢？</strong></p><p>我们思考一下，假如”你现在假装是大模型“，你该怎么提高数据压缩率？</p><ul><li>最简单的方法先记住一些pattern，把一些词联系起来：比如”混元形意“后面大概率跟着”太极门“；”松果弹抖”后面大概率跟着“闪电鞭“。这个过程其实可以理解成”学会了世界知识“。</li><li>这个时候loss降一些了，但cross-entropy还要求你降，咋办？这个时候模型就必须找点别的办法了：比如逻辑推理。这在代码串里尤其有用，你只靠世界知识显然没法压缩代码数据。必须得学会一定的推理才可以。</li><li>想学会推理，是个随机事件。我们想想人是怎么学会推理的？你大概率也说不清楚，这是智能的一个附加产物。但对于transformer结构来说，最起码有一些点可以确定，如果模型层数少，那肯定不能实现推理，因为没有足够的空间去计算。</li></ul><blockquote><p>参考自动机理论：没有栈的只能做有限状态自动机DFA的任务；一个栈就能做下推自动机PDA；含有两个栈就能执行图灵机turingmachine任务。计算能力上限和空间关系很大。</p></blockquote><p>所以说我们对这个现象有一些理解：推理能力来源于预训练任务的数据压缩要求，一个必要条件是模型大小不能太小。换句话说，随着模型规模的扩大，到某个阈值，突然间就有能力做推理了，然后一下就emergence了。</p><p>为什么prompt方法会在大模型变得有效，也是一样的：prompt是添加token总数，对于推理模型，更多的上下文就相当于是给推理malloc空间，当然就有助于做任务了。而对于小模型，开多少空间都没法推理，所以prompt当然也没用了。</p><ul><li><strong>但为什么压缩率是scaling-law呢？</strong>这不冲突。因为cross-entropy loss <spanclass="math inline">\(\propto\)</span>压缩率。压缩率其实不是一个线性关系，你想象一下: entropy从3降到2是提升50%，但从2降到1是提升100%。<strong>压缩率线性下降其实就是对应能力的指数提升</strong>。</li></ul><p>接下来聊聊一个更哲学的话题：<strong>意识是怎么诞生的？通过这个“数据压缩”任务有可能诞生意识吗？</strong></p><blockquote><p>先叠个甲，我不懂哲学，里面用到的词我都是采取词本身的意思，我不知道会不会和哲学里某些已有定义冲突。</p></blockquote><p>这个问题我们可能得回到上个世纪人工智能“符号主义”、”逻辑主义“的一些理论，希望通过模拟人的行为达到真正的智能。但其实我个人不抱太大希望，因为学习模式是不同的：</p><ul><li>人是自顶向下的。我们虽然也是从世界经验中学习，但自己也说不好意识是怎么诞生的。而且驱动行为一般都是先有意志，再有行为。</li><li>LLM是数据建模，是自底向上的。是通过压缩大量的相似数据，总结出一些规律"推理、世界知识"，从而更好的压缩后面没见过的新数据。这个观点可以被认为是“泛化性”(generalization)。</li><li>意识的诞生对于数据压缩有没有帮助呢？我认为，如果答案是有，那意识有可能诞生。当然，意识本身是没有seq-length的，我觉得有长度上限的”意识“其实就是CoT那种推理。</li></ul><p>这衍生出来另一个问题，我们假设上一个问题的答案是”有“，那么就要实现一些”意识“的必要条件，比如无限的推理空间。想象一下，“人的思考是没有边际的”，这句话等价于说”意识不受到长度、空间的限制“。</p><p>这就是靠现在的seq-length架构肯定没办法，有一个想法其实就是接近CoT的：我们能不能在压缩数据的过程中额外开一些空间让模型在中间推理呢？举个例子，模型可以自由决定在压缩数据的某一个位置进入CoT模式，然后进行一些思考，接下来的token压缩就会变得更简单。通过这种”类似toollearning“的方式，其实实现了类似于”意识“的东西。我正在做一些类似的工作，感觉还是挺有意思的。</p><p>最后一个问题：<strong>”推理“的下一步是什么？</strong></p><ul><li>由于训练任务的特性，模型必须想一些额外的办法去进一步提升压缩率</li></ul><blockquote><p>LLM:谁懂呀家人们，我都想办法学会推理了，虾头cross-entropy还在那一个劲找optimizer搞我。</p></blockquote><ul><li>这个问题没有答案？也许LLM想到的下一个办法，是人类也想不到的呢？<ul><li>我的一点理解：现在的Autoregressive还是受限于从左到右的pattern，这个归纳偏置不一定只是人大多数时候的思考方式，但不一定就是对的。类似self-correct、reflection等论文都谈到了让模型多次输出的可能性。这种建模是一种另外的方式，也许还有更多的方式。总之，想要让模型做得更好，一个劲用optimizerpush模型是一方面，提升模型的表示能力的上限是另一方面。</li><li>这个研究方向，也许可以参考 meta learning研究领域？</li></ul></li></ul><p>以上。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Generative Agents: Interactive Simulacra of Human Behavior</title>
      <link href="/4a3bcbdb.html"/>
      <url>/4a3bcbdb.html</url>
      
        <content type="html"><![CDATA[<p>今天讲一篇很有意思的论文，如果让25个GPT假装NPC在游戏里自由生活、交流、发展会怎么样呢？</p><p><img src="../files/images/Generative_Agents/intro.png"></p><span id="more"></span><p><img src="../files/images/Generative_Agents/authors.png"></p><p>作者团队来自斯坦福和google research。其中percy liang是stanfordAI的代表人物，他挂名的论文一般质量都很高。</p><h2 id="introduction">Introduction</h2><p>整个introduction的写法大概相当于一个简单版的论文原文，这种写作手法其实是比较少见的，可能是这篇论文的格式瞄准的是nature子刊类，需要让外行也能看懂论文的内容。</p><p>introduction中，作者谈到想要真实地模拟出NPC的行为，大概需要有一个比较好的记忆模块，可以去进行长期的记忆。在此基础之上，作者额外设计了三个模块：记忆模块、反思模块、交互模块。</p><p>作者进行了对比实验，发现任何一个模块都对于结果非常重要。同时作者也做了一个社会实验，告诉其中一个模型要举行情人节派对，最后所有NPC都通过交谈得知了这件事情，每个人都决定自己要不要参加、或者因为什么事情不会出席。NPC社会衍生出了非常类似于人类社会的关系。</p><p>另外，我想额外说几件事情：</p><ul><li>搭建这样一个平台框架是一个很复杂的工程问题，作者其实为了这件事情应该是写了非常多的代码</li><li>prompt设计估计也比想象中麻烦</li><li>作者没有开源代码</li><li>作者在后文提到，即使这样一个平台，没有训练只是做实验，在ChatGPT现在打1折的情况下，仍然花了几千刀来完成实验。可见moneyis all you need</li></ul><h2 id="method">method</h2><p>下面我们来看看具体作者怎么创造了这么一个GPT的世界呢？</p><h3 id="world">world</h3><p><img src="../files/images/Generative_Agents/world.png"></p><p>作者首先设定了一个世界，里面有各种各样的场所，还有各种各样的对象可以交互，交互方式就是自然语言。</p><p>同时世界中有各种各样的人，每个人由一段prompt定义，比如说John：</p><blockquote><p>John Lin is a pharmacy shopkeeper at the Willow Market and Pharmacywho loves to help people. He is always looking for ways to make theprocess of getting medication easier for his customers; John Lin isliving with his wife, Mei Lin, who is a college professor, and son, EddyLin, who is a student studying music theory; John Lin loves his familyvery much; John Lin has known the old couple next-door, Sam Moore andJennifer Moore, for a few years; John Lin thinks Sam Moore is a kind andnice man; John Lin knows his neighbor, Yuriko Yamamoto, well; John Linknows of his neighbors, Tamara Taylor and Carmen Ortiz, but has not metthem before; John Lin and Tom Moreno are colleagues at The WillowsMarket and Pharmacy; John Lin and Tom Moreno are friends and like todiscuss local politics together; John Lin knows the Moreno familysomewhat well — the husband Tom Moreno and the wife Jane Moreno.</p></blockquote><p>GPT接受这个prompt作为人设开始活动</p><h3 id="memory">memory</h3><p><img src="../files/images/Generative_Agents/memory.png"></p><p>仅仅是GPT的32768的seqlength显然是不够的，作者需要一个记忆模块。记忆模块存储了过去智能体经历的所有事件。事件包括时间地点人物内容对话等等。每当模型发生一个新的事件，就把事件记录到记忆模块里。</p><h3 id="retrieve">retrieve</h3><p>为了让模型可以想起过去发生的事件，作者定义了一个专门搜索模块retrieve模块。</p><p><img src="../files/images/Generative_Agents/retrieve.png"></p><p>对于一个搜索的输入，对所有记忆中的事件都有三个部分的得分：</p><ul><li>时间得分：发生在最近的事件分数高。作者提到是一个底数为0.99的指数衰减算法</li><li>重要性得分：重要的事件分数高。这一部分是在事件发生时让模型自己打分。作者提到模型自己打的分挺好的，比如没啥意义的吃早饭就打1分，找暗恋对象去约会就打8分。</li></ul><blockquote><p>On the scale of 1 to 10, where 1 is purely mundane (e.g., brushingteeth, making bed) and 10 is extremely poignant (e.g., a break up,college acceptance), rate the likely poignancy of the following piece ofmemory.</p><p>Memory: buying groceries at The Willows Market and Pharmacy Rating:<fill in></p></blockquote><ul><li>相关性得分：和query相关的分数高。这里作者直接找了另一个LLM去编码句子向量，然后算cosine相似度。</li></ul><p>最终得分是三个得分的加权平均。模型每次遇到不知道的问题或者需要查看记忆的时候就调用这个retrieve模块。</p><h3 id="reflection">reflection</h3><p>仅仅是记忆和搜索，并不能让模型自己总结世界知识。作者希望模型可以在生活中自己总结出一些结论，这就是reflection模块。</p><p><img src="../files/images/Generative_Agents/reflection.png"></p><p>这一部分作者采用自顶向下的模式，每天进行一到两次reflection。每次reflection时输入最近的100个事件，然后通过prompt询问我们可以通过这些时间发现什么问题。要发现三个问题。</p><p>接下来，对于发现的问题，作者用retrieve模块搜索相关事件，然后让模型通过结果来回答问题。</p><blockquote><p>Statements about Klaus Mueller</p><ol type="1"><li>Klaus Mueller is writing a research paper</li><li>Klaus Mueller enjoys reading a book</li></ol><p>on gentrification</p><ol start="3" type="1"><li>Klaus Mueller is conversing with Ayesha Khan about exercising[...]</li></ol><p>What 5 high-level insights can you infer from the above statements?(example format: insight (because of 1, 5, 3))</p></blockquote><p>最终整体的reflection过程会当做一个事件存储到memory系统里。也就是说，随着生活的进行，模型会对于世界有更多的了解，比如说自己的邻居沉迷科研，或者xxx每天要花很长时间浇花之类的。最后NPC的对话会越来越精确，也能发现趣味相投的朋友。</p><h3 id="plan和交互">plan和交互</h3><p><img src="../files/images/Generative_Agents/plan.png"></p><p>有了记忆模块，模型最后是怎么和世界交互的呢？</p><p>作者定义了一个自顶向下的plan过程。每天先通过prompt生成当天的计划，大概分为5-8个部分。</p><blockquote><p>Name: Eddy Lin (age: 19) Innate traits: friendly, outgoing,hospitable Eddy Lin is a student at Oak Hill College studying musictheory and composition. He loves to explore different musical styles andis always looking for ways to expand his knowledge. Eddy Lin is workingon a composition project for his college class. He is also takingclasses to learn more about music theory. Eddy Lin is excited about thenew composition he is working on but he wants to dedicate more hours inthe day to work on it in the coming days On Tuesday February 12,Eddy</p><ol type="1"><li>woke up and completed the morning routine at 7:00 am,</li><li>[. . . ]</li></ol><ol start="6" type="1"><li>got ready to sleep around 10 pm.</li></ol><p>Today is Wednesday February 13. Here is Eddy’s plan today in broadstrokes: 1)</p></blockquote><p>接下来对于生成的每个部分，再生成更详细的计划。</p><blockquote><p>4:00 pm: grab a light snack, such as a piece of fruit, a granola bar,or some nuts.</p><p>4:05 pm: take a short walk around his workspace</p><p>[...]</p><p>4:50 pm: take a few minutes to clean up his workspace.</p></blockquote><p>接下来作者就会让模型真的去开始这一天，通过生成的今日计划。随着计划的进行，模型会遇到别的人，比如说都在餐厅吃饭之类的。或者遇到某些世界事件，比如在修路走不了。</p><p>对于这种，作者定义为observation，是一段自然语言描述，比如下面这样：</p><blockquote><p>[Agent’s Summary Description] It is February 13, 2023, 4:56 pm. JohnLin’s status: John is back home early from work. Observation: John sawEddy taking a short walk around his workplace. Summary of relevantcontext from John’s memory: Eddy Lin is John’s Lin’s son. Eddy Lin hasbeen working on a music composition for his class. Eddy Lin likes towalk around the garden when he is thinking about or listening to music.Should John react to the observation, and if so, what would be anappropriate reaction?</p></blockquote><p>接下来作者让模型自己决定要不要去和别的NPC交互一下，比如下文就是选择去说话。</p><blockquote><p>[Agent’s Summary Description] It is February 13, 2023, 4:56 pm. JohnLin’s status: John is back home early from work. Observation: John sawEddy taking a short walk around his workplace. Summary of relevantcontext from John’s memory: Eddy Lin is John’s Lin’s son. Eddy Lin hasbeen working on a music composition for his class. Eddy Lin likes towalk around the garden when he is thinking about or listening to music.John is asking Eddy about his music composition project.</p><p>What would he say to Eddy?</p></blockquote><p>另外的模型也用同样的方式回答：</p><blockquote><p>[Agent’s Summary Description] It is February 13, 2023, 4:56 pm. EddyLin’s status: Eddy is taking a short walk around his workplace.Observation: John is initiating a conversation with Eddy. Summary ofrelevant context from Eddy’s memory: Jonn Lin is Eddy Lin’s father. JohnLin is caring and is interested to learn more about Eddy Lin’s schoolwork. John Lin knows that Eddy Lin is working on a music composition.Here is the dialogue history: John: Hey Eddy, how’s the musiccomposition project for your class coming along? How would Eddy respondto John?</p></blockquote><p>同时，作者也允许模型在通过交互以后修改自己本来做好的plan:</p><blockquote><p>[Agent’s Summary Description] Eddy Lin is currently in The Linfamily’s house:</p><p>Eddy Lin’s bedroom: desk) that has Mei and John Lin’s bedroom, EddyLin’s bedroom, common room, kitchen, bathroom, and garden.</p><p>Eddy Lin knows of the following areas: The Lin family’s house,Johnson Park, Harvey Oak Supply Store, The Willows Market and Pharmacy,Hobbs Cafe, The Rose and Crown Pub.</p><p>* Prefer to stay in the current area if the activity can be donethere. Eddy Lin is planning to take a short walk around his workspace.Which area should Eddy Lin go to?</p></blockquote><p>通过上述的所有模块，作者最终让智能体可以在小世界自由的生活、交谈、理解、反思。</p><h2 id="experiment">experiment</h2><p>这一部分作者做了两个实验。</p><p>首先是相似性实验，作者让human在模型同样的处境下选response。和模型自己的response掺杂在一起，然后让别的human打分看看局的哪个好(打分人不知道哪个是human写的)。</p><p><img src="../files/images/Generative_Agents/clone.png"></p><p>最后发现，其实模型自己生成的response比human写的还要好，尤其是在有所有模块的情况下。</p><p><img src="../files/images/Generative_Agents/party.png"></p><p>另外，作者还做了个社会实验：通过修改某个NPC的行为，让它想要开个情人节派对。最后通过NPC的对话传播，最终12个人得知了情况，决定来参加。模型们自己讨论了派对时间地点，最终成功举办了派对。细思恐极呀。</p><h2 id="我的思考">我的思考</h2><ul><li><p>现在模型都能无监督自己生活了，这会不会有伦理问题呀w</p></li><li><p>期待一手未来的游戏，难道GTA6里面的NPC要变成真正的智能NPC、决定自己的行程和发展了吗？</p></li><li><p>25个ChatGPT跑了2天的模拟人生，就花了几千刀的token钱，还是在打1折的情况下。哎</p></li><li><p>作者说还有很多的改进空间，感觉提到的改进空间大多数都是需要少更多钱才能做的。</p></li><li><p>我自己挺看好这个方向的，而且我觉得肯定很多做心理学、社会学的人会对这个很感兴趣，比如说</p><ul><li>真正的human和24个ChatGPT生活会怎么样？</li><li>10个人和15个ChatGPT生活两天后投票谁是AI，能不能选出真正的人？</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-强化学习和RLHF中的PPO算法</title>
      <link href="/5d2d4022.html"/>
      <url>/5d2d4022.html</url>
      
        <content type="html"><![CDATA[<p>今天讲讲强化学习里的经典算法PPO，也是现在Gym库里默认的强化学习算法，最后再讲讲RLHF中的PPO算法是怎么算的。参考</p><blockquote><p>Proximal Policy Optimization Algorithms</p><p>Trust Region Policy Optimization</p><p>A (Long) peek into Reinforcement Learning</p></blockquote><span id="more"></span><p><img src="../files/images/PPO/authors.png"></p><p>这是2017年的论文，作者团队是openAI。讲PPO，必须先说明一大堆前置知识。我先简单说说前置知识，不保证正确。</p><h2 id="前置知识">前置知识</h2><p>强化学习领域就是让智能体agent和环境e一直交互，最终强化智能体。</p><p>强化学习的一次交互为多个state，action链式连接。</p><p>强化学习优化的对象叫做return。环境对于每个action给出的反馈叫做reward。return是所有未来state的reward指数衰减<span class="math display">\[return_i = \sum_{t=i}^T \gamma^{t-i} Reward_t\]</span></p><ul><li>return是个未来函数，只有跑完后面才知道前面的reward。我们只能用模型模拟或者求期望</li><li>在很多情况下，环境也不能给出密集的reward，很有可能只有交互结束才能给一个reward(比如下棋)，这个领域叫sparsereward</li><li><spanclass="math inline">\(\gamma\)</span>是时间衰减函数，说明未来的收益要衰减。这是因为未来有不确定性，所以未来的回报没有现在的重要。至于为什么用指数衰减，这是未了后面的数学推导更简单</li></ul><p>强化学习分为两大类方法，基于动作的policy method和基于评估的valuemethod</p><h3 id="value-method">value method</h3><p>这是指用一个模型来拟合value。要知道value，要先知道Q(s,a)</p><p>Q(s,a)是衡量模型在状态s做出动作a到底好不好，值就等于未来状态return。<spanclass="math inline">\(Q_\pi(s_t,a) = return_{t+1}\)</span></p><p>Q(s,a)包含动作a，不好。把a积分掉(或者离散情况下就是<spanclass="math inline">\(\sum\)</span>)以后得到<spanclass="math inline">\(V_\pi(S) = \sum_{a} \pi(a|s)Q(s,a)\)</span></p><p>另外，<span class="math inline">\(\text{Adavantage}_{a} = Q(s,a) -V(s)\)</span>专门表示采取动作a带来了多少额外收益</p><p>经典的Q-learning用模型去学习Q(s,a)，怎么学？通过贝尔曼方程，根据Q的定义<span class="math display">\[V(S_t) = R_{t+1} + \gamma V(S_{t+1}) \\...\\V(S_{T-1}) = R_T\]</span></p><p><span class="math display">\[Q^*(s_t,a_t) = R_t + \gamma \max_{a\in A} Q^*(s_{t+1},a)\]</span></p><ul><li><span class="math inline">\(Q^*\)</span>是指最好的Q，就是说<spanclass="math inline">\(s_t\)</span>采取动作<spanclass="math inline">\(a_t\)</span>以后最好的结果就是<spanclass="math inline">\(Q^*(s_t,a_t)\)</span>了</li></ul><p>这衍生出了TD-error(temporaldifference):等式右边的V估计更准确，因为右边计算用到了真实的回报<spanclass="math inline">\(R_{t+1}\)</span>。通过这个归纳偏置就可以学习<spanclass="math inline">\(Q^*\)</span>了:</p><ul><li><p>用一个模型拟合<spanclass="math inline">\(Q_\theta(s_t,a_t)\)</span></p></li><li><p>在状态<span class="math inline">\(s_t\)</span>中通过<spanclass="math inline">\(\epsilon-\text{greedy}\)</span>采样一个动作。采样方法是指有<spanclass="math inline">\(\epsilon\)</span>概率采取一个随机动作，剩下的时候按照<spanclass="math inline">\(Q_\theta(s_t,a_t)\)</span>最大的<spanclass="math inline">\(a_t\)</span>采取动作</p></li><li><p>执行<span class="math inline">\(a_t\)</span>，进入<spanclass="math inline">\(s_{t+1}\)</span>并得到回报<spanclass="math inline">\(R_t\)</span></p></li><li><p>对于<spanclass="math inline">\(Q_\theta(s_t,a_t)\)</span>的更好的估计是<spanclass="math inline">\(R_t + \gamma \max_{a\in A}Q_\theta(s_{t+1},a)\)</span>。用做target，然后用mseloss优化就行</p></li></ul><p>Q-learning里的最经典的算法叫做deep-Q Network，大概做了几个改进:</p><ul><li>经验重放：就是说实际在环境里跑交互太慢了，我可以提前跑，跑完把状态转移对<spanclass="math inline">\((s_t,a_t,s_{t+1},R_t)\)</span>存下来，然后再多次更新、多次利用</li><li>periodically update target，就是我算loss的时候那个target里的<spanclass="math inline">\(Q^*\)</span>不用现在的参数，而是用一段时间之前的参数，每隔一段时间同步一次。这是为了减小loss尖峰带来的影响，让训练更稳定</li></ul><h3 id="policy-method">policy method</h3><p>前面提到Q-learning等方法学习Q，然后采取动作是用<spanclass="math inline">\(\epsilon-\text{greedy}\)</span>解码。这里的思路是说我不学Q，我学习一个agent<spanclass="math inline">\(\pi(a|s_t)\)</span>输入一个状态，我返回在这个状态下做所有动作的概率分布。</p><p>优化的目标当然就是让好的动作概率更大，坏的动作概率小 <spanclass="math display">\[\mathcal{J}(\theta) = \sum_{s\in S} \left( \sum_{a\in A} \pi(a|s)Q_{\pi(s,a)} \right)\]</span> 经典的REINFORCE算法就是一大堆数学推导，最后使得 <spanclass="math display">\[\Delta \mathcal{J}(\theta) = \mathbb{E}_{\pi \theta} \left[ \Delta \ln\pi(a|s) Q_\pi(s,a) \right]\]</span> $() <spanclass="math inline">\(是\)</span>$的函数，可以看做loss，用梯度上升来做参数更新</p><p>很优雅，用了log换元。但是上面的式子要求期望，还是有两个<spanclass="math inline">\(\sum\)</span>，显然没法直接算。一个简单的方法就是用蒙特卡洛近似，就是在环境里跑完一大堆状态转移，然后倒着算回来所有的<spanclass="math inline">\(Q_\pi(s,a)\)</span>(这里视为这个是数字，和<spanclass="math inline">\(\theta\)</span>无关，是一个近似)。再拿上面的式子就能训练<spanclass="math inline">\(\pi\)</span> 了</p><p>上面的reinforce算法有个问题：上面的梯度受到环境影响大，方差大不稳定，要找个东西减小方差，这叫做baseline方法。</p><p>一个思路就是用上面提到的advantage<span class="math inline">\(=Q(s,a)- V(s)\)</span>代替Q(s,a)。</p><h3 id="actor-critic">actor-critic</h3><p>上面说我们蒙特卡洛模拟，然后反着求Q，这个Q的估计显然是不准确的，有解决办法吗？我们搞ai就是workingis all you need：我们用另一个模型拟合<spanclass="math inline">\(V_\theta(S)\)</span>。这种两个模型的方法就是actor-critic。一个演员采取行动，一个裁判进行打分。</p><p>经典的actor-critic是用另一个模型来拟合<spanclass="math inline">\(Q(s,a)\)</span>。Q的参数更新就用上面讲到的Q-learning里的temporaldifference (TD-error)，然后<spanclass="math inline">\(\pi\)</span>的参数更新用模型输出的Q来算<spanclass="math inline">\(\mathcal{J}(\theta)\)</span>。注意这里要把Q的输出的梯度detach掉，就是不要反向传播到Q的参数里。</p><p>这种双模型的方法是现在RL的最常用的方法。简单区分几种方法，来自知乎：</p><blockquote><p>actor就好比是你，critic就好比你妈。你做一件事情，比如抓蜜蜂，结果被蜇疼了，下次你再抓蜜蜂的概率就减小了，这个就是policygradient。你刚手伸出去要去抓蜜蜂，你妈就说，别抓，十有八九会被蜇疼。你听了后停止了抓蜜蜂，并且下次抓蜜蜂的概率减小了，这个就是actor-critic。你每次看见蜜蜂的时候都问你妈，抓蜜蜂好还是不抓蜜蜂好？你妈说不抓蜜蜂好，通常你听你妈的话就不抓蜜蜂了，偶尔心情不好的时候（以<spanclass="math inline">\(\epsilon\)</span>的概率）还要去抓蜜蜂，这个就是Q-learning。那么妈妈是怎么知道抓蜜蜂会疼的？当然她也是抓过蜜蜂的（Q-valueupdate）</p></blockquote><h2 id="ppo">PPO</h2><p>下面正式开始PPO讲解。PPO基于更早的一个叫TRPO(trustregion)的算法。可以理解成一个actor-critic方法，大概做了几个改进。</p><p>Trust region是一篇数学很多的论文，大概讲的事情是 <spanclass="math display">\[\mathcal{J}(\theta) = \sum_{s\in S} \left( \sum_{a\in A} \pi(a| s)A_t\right)\]</span>上面的估计是很不稳定的，要想办法用另一个让它变稳定：用一个叫做的方法，可以用另一个分布来估计原始的分布。<span class="math display">\[\mathcal{J}(\theta) \sim \mathbb{E}_{\sim \pi_{\theta_{old}}} \left[\frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}A_t \right]\]</span> 这样这个分布的采用使用<spanclass="math inline">\(\theta_{old}\)</span>就在数学上成立了。然后训练时我们每隔一段时间更新一次<spanclass="math inline">\(\theta\)</span>，在这段时间内用之前跑出来的状态转移对进行训练就行。极大地提高了训练效率。</p><p>我们希望两个分布不要差太远，这是因为上式可以一阶等价于<spanclass="math inline">\(\mathcal{J}(\theta)\)</span>，因此需要分布很接近才行。因此可以再给loss加一个KL散度惩罚。<span class="math display">\[\mathbb{E}_t \left[ \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t| s_t)}A_t - KL(\pi_{\theta}(·| s_t), \pi_{\theta_{old}}(·|s_t))  \right]\]</span> 这就是论文里说的代理训练目标</p><p>PPO把这个分式重新命名了一下 <span class="math display">\[r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t |s_t)}\]</span> 把TRPO中的loss称为 <span class="math inline">\(L^{CPI} =\mathbb{E} \left[ r_t(\theta) A_t  \right]\)</span></p><p>然后提出了两个变体 <span class="math display">\[L^{CLIP} = \mathbb{E} \left[ \min(r_t(\theta) A_t, clip(r_t(\theta,1-\epsilon, 1+ \epsilon)A_t ) \right]\]</span></p><p><span class="math display">\[L^{KLPEN} = \mathbb{E}_t \left[ r_t(\theta) A_t - \betaKL(\pi_{\theta}(·| s_t), \pi_{\theta_{old}}(·| s_t))  \right]\]</span></p><p>这两个变体都能使得训练更稳定。其中变体二的超参数<spanclass="math inline">\(\beta\)</span>是自适应的:</p><ul><li><span class="math inline">\(\beta \rightarrow\frac{\beta}{2}\)</span> 如果 $本次KL/1.5 &gt; 平均KL $</li><li><span class="math inline">\(\beta \rightarrow 2\beta\)</span> 如果$本次KL*1.5 &lt; 平均KL $</li></ul><p>除此之外，模型的更新就和actor-critic方法没什么别的区别了。</p><ul><li><p>它使用了经验重放，交互用的链可以更新多次(4 epoch)再同步参数<spanclass="math inline">\(\theta_{old}\)</span></p></li><li><p>他的更新用的不是Q,而是Advantage <spanclass="math inline">\(A_t\)</span>，这是为了减少方差，稳定训练。</p></li></ul><p><span class="math display">\[A_t = \delta_t + (\lambda \gamma) \delta_{t+1} + ... + (\lambda\gamma)^{T-t+1} \delta_{T-1}\delta_t = r_t + \gamma V(s_{t+1})  - V(s_t)\]</span></p><p><span class="math display">\[\delta_t = r_t + \gamma V(s_{t+1})  - V(s_t)\]</span></p><ul><li>其中<spanclass="math inline">\(V(s)\)</span>是需要训练的第二个模型。然后训练流程是先用<spanclass="math inline">\(\pi\)</span>跑完一整轮，得到<spanclass="math inline">\(V(s_1),...V(s_T)\)</span>再倒过来算出所有的<spanclass="math inline">\(A_t\)</span>，再把<spanclass="math inline">\(A_t\)</span>拿过来训练<spanclass="math inline">\(\pi\)</span></li><li>V的值同样通过TD-error的mse-loss进行更新<spanclass="math inline">\(V_\text{target}\)</span>注意计算要用到那个慢更新的优化，用的是一段时间以前的参数算的V</li></ul><h2 id="rlhf中的ppo">RLHF中的PPO</h2><p>上面讲完了PPO，那RLHF中的PPO又是怎么算的呢？这一部分我是根据代码阅读的结果来讲的，实际上可能每边都有每边的实现，我讲其中一种。</p><p>RLHF算法分3个大块：SFT，reward model training,RLHF。我这里讲最后一个部分，也就是说我们已经有了一个rewardmodel可以给任何一个(query, response)对输出一个 <spanclass="math inline">\(r \in[-1,1]\)</span>的讲理，越高说明越好越符合人的期望输出。这个就是假的"环境"，可以给反馈</p><h3 id="what-is-model">what is model</h3><p>首先，PPO明显是需要两个模型policy model， valuemodel。怎么实现呢？</p><ul><li>policymodel其实就是语言模型自己，我们把一次response生成的每个token生成视为一个状态转移，然后action就是对应的tokent。</li><li>value model这里和语言模型是共享参数的，我们在最后语言模型的hiddenstate层后面加一个 model_dim -&gt; 1的映射（正常是model_dim-&gt;vocab_size-&gt;softmax映射到词表），所以所有的状态下都有一个float的输出，把这个当做<spanclass="math inline">\(V(s_t)\)</span></li><li>至于两个模型要不要共享参数，是个实验问题。我只能说，如果不共享参数的话，首先需要存两份，然后每次交互都要跑两次前向，反向传播也是，这个在尤其是大模型场景下，对算力的消耗是多很多的。</li></ul><p>我们需要记载一个最开始的模型的参数，这个开始是指原始的LLM语言模型的参数<spanclass="math inline">\(\pi_{start}\)</span>，这是为了让RLHF阶段的更新不要太多，不要丢失语言模型原本的语言、推理能力</p><h3 id="advantage">advantage</h3><p>然后就是reward的计算</p><ul><li>对于正常的token，除了最后一个token以外，reward就是引入模型分布和原始分布的KL散度<spanclass="math inline">\(KL(\pi_\theta(r|q),\pi_{\theta_{start}}(r|q))\)</span></li><li>对于最后一个token，额外引入最终response的reward，就是reward和KL散度的和。</li></ul><p>KL散度怎么算？其实就是<span class="math inline">\(\logP(·|s)\)</span>的差值。其实就是你的模型的输出logit <spanclass="math inline">\([seq-length,vocab-size]\)</span>。过完log-softmax层，然后把你实际取的token的对应位置的值取出来，得到一个logprobs <spanclass="math inline">\([seq-length,float]\)</span>。把现在的参数<spanclass="math inline">\(\theta\)</span>和之前参数<spanclass="math inline">\(\theta_{start}\)</span>的对应logprobs都取出来，然后直接做减法就行，得出来seq-length长度的序列，就是对应每个位置的KL散度reward</p><p>有了所有未知的reward以后，就可以用PPO的方法倒过来算出所有位置的advantage<span class="math inline">\(A_t\)</span>，注意这里的<spanclass="math inline">\(A_t\)</span>计算需要用到Value-old输出<spanclass="math inline">\(V_{old}(s_t)\)</span>而不是现在的<spanclass="math inline">\(\theta\)</span>的输出，这是为了训练更稳定 <spanclass="math display">\[A_t = \delta_t + (\lambda \gamma) \delta_{t+1} + ... + (\lambda\gamma)^{T-t+1} \delta_{T-1}\]</span></p><p><span class="math display">\[\delta_t = r_t + \gamma V_{old}(s_{t+1})  - V_{old}(s_t)\]</span></p><h3 id="value-model-loss">value model loss</h3><p>接下来计算Value的loss，使用<span class="math inline">\(loss_{V} =||V(s_t) -A(s_t)||^2\)</span>作为loss，这里可以进行一下clip使得更新不要太大 <spanclass="math display">\[loss_{V-clip} = ||clip(V(s_t), V_{old}-\epsilon_1, V_{old}+\epsilon_1 )- A(s_t)||^2\]</span></p><p><span class="math display">\[loss_{V} = \frac{loss_{V} + loss_{V-clip}}{2}\]</span></p><h3 id="policy-model-loss">policy model loss</h3><p>接下来计算policy的梯度。首先是openAI定义的那个<spanclass="math inline">\(r_t(\theta)\)</span>。因为是除法，相当于lo概率的减法再e指数。直接用之前算KL时那个log-softmax的输出减法再指数就行，得出来也是seq-length长度的链。<span class="math display">\[L^{CLIP} = \mathbb{E} \left[ \min(r_t(\theta) A_t, clip(r_t(\theta,1-\epsilon_2, 1+ \epsilon_2)A_t ) \right]\]</span> <span class="math display">\[loss_{pg} = - L^{CLIP}\]</span></p><p>然后要注意！这里的L本来按policygradient算法是要最大化的，所以在AI框架实现中最后要取个符号作为loss(或者就直接用负的，然后换成<spanclass="math inline">\(\max\)</span>)</p><p>最终得到最终的优化目标 <span class="math display">\[loss = loss_{pg} + \alpha · loss_{V}\]</span></p><h3 id="训练框架">训练框架</h3><ul><li>1.初始化模型<spanclass="math inline">\(\theta\)</span>，用一个预训练LLM来初始化。同时初始化valuemodel,其实就是加个linear层。</li><li>2.用现在的<spanclass="math inline">\(\theta\)</span>跑一堆response数据。query就是采样自你的query数据集</li><li>3.把repsonse送进reward model打分。注意rewardmodel要锁参，然后这里记得detach。</li><li>4.存下来2，3出来的 (query,response,reward)对，记录现在的<spanclass="math inline">\(\theta\)</span>为<spanclass="math inline">\(\theta_{old}\)</span></li><li>5.对4的数据做几个epoch的更新(比如4个)，每个epoch<ul><li>按上述方法算出 <span class="math inline">\(loss_{pg},loss_V\)</span></li><li>不管是不是分离的policy、value model，反正进行梯度更新</li></ul></li><li>6.删除刚才的跑出来的数据集4，节省空间。回到2</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning</title>
      <link href="/8622e2d1.html"/>
      <url>/8622e2d1.html</url>
      
        <content type="html"><![CDATA[<p>今天来讲一个和方法toolformer很像的文章：如何让模型左脚踩右脚学会做题？</p><p><img src="../files/images/STaR/intro.png"></p><span id="more"></span><p>作者团队来自stanford和google，</p><p><img src="../files/images/STaR/authors.png"></p><h2 id="introduction">Introduction</h2><p>introduction是一个很经典的论文写作套路。</p><p>作者首先讲了讲做题这个领域。在做题领域，大模型直接做表现基本很差，few-shot版本也很差。(这里的few-shot是指只有问题-答案 对的情况)。最近有一类方法叫chain-of-thought(CoT)，让模型thinkstep by step，先生成一些思考链再做题，就能极大提高准确率。</p><p>这里作者就来思考：既然模型可以自己创造思考链，那能不能提高模型“创造思考链”的能力呢？</p><p>于是提出了所谓的self-taught reasoner。最后总结了一下贡献：</p><ul><li>提出了一个简单的提高模型能力的方法。（无需检测生成的思考链是否正确）</li><li>提出了让模型对着答案学的方法</li><li>在很多数据集上做了详细地实验</li><li>是首次提出让LLM自己提升自己的方法</li></ul><p><img src="../files/images/STaR/method.png"></p><h2 id="method">method</h2><p>那么是什么方法呢？这里要按照是否让模型自我提高分出两类方法。</p><p><img src="../files/images/STaR/algo.png"></p><h3 id="star-without-rationalization">STaR Without Rationalization</h3><p>这个方法就是algorithm里没有蓝字的部分</p><p>假设我们已经有了CoT，然后接下来就是把CoT转换成自动的过程：</p><ul><li><p>先让人生成一些CoT，然后按照in-context的方式拼在前面</p></li><li><p>对于数据集里的所有数据，都可以生成一个CoT再做</p></li><li><p>对于生成的所有CoT，如果做出了答案，说明是“好的CoT”。将对应的(问题-CoT-ans)返回给模型进行训练</p></li><li><p>持续这个过程</p></li></ul><p>这里之所以省掉了人类的大量的CoT标注。其实是有个偏置信息在里面：“生成正确的答案的CoT比正常的CoT质量高”。</p><p>这里其实就是和单步reward的RL有一些相似的观点在里面：不管过程是怎么做出来的，如果最后做对了，就认为过程里的所有步骤都是好的。</p><h3 id="rationalization">Rationalization</h3><p>作者又思考了上面算法的问题：这里其实是没有引入额外知识的，只是在激发模型本身理解问题、思考问题的能力。换句话说，如果模型不能再生成新的数据了，上面算法的上限就到了。</p><p>那么怎么引入额外信息呢？就是要关注聚焦于没做出来的题目，一个思路是让人标CoT，这个显然太“贵“了。作者提出的方法就是rationalization。</p><ul><li>对于做不出来的题目，同时喂给模型问题和答案，让模型在看到问题和答案的情况下生成CoT。</li><li>然后认为这个CoT就是好的CoT，拿来训练模型</li></ul><p>这里其实也是有一个偏置信息在里面：”看到答案的情况下，模型生成的CoT会比之前更好“。</p><p>上面两种方法的结合，就是STaR。</p><h2 id="实验">实验</h2><p>作者在三个场景测试了STaR：</p><ul><li>多位数加法，这个里面的CoT其实就是scratchpad</li><li>CommonsenseQA(CQA)：这个任务是一些根据尝试回答问题。基本上CoT就是自己说一说问题里对应的常识</li><li>Grade School Math (GSM8K)：全是高中数学题</li></ul><p>和toolformer类似，作者也选用GPT-J 8B作为训练的基础模型</p><p><img src="../files/images/STaR/CQA_result.png"></p><p>可以看到，</p><ul><li><p>对于CQA任务来说,STaR+8B模型就能达到finetune+175B的效果。</p></li><li><p>另一个关键点是，即使没有左脚踩右脚，仅仅通过自己生成CoT的方式，效果也很不错</p></li><li><p>注意，这里说的fine-tune指的是直接生成答案。</p></li></ul><p>上面的结果可以看出来，在训练中加入CoT是可以让模型更轻松理解题目、提高效果的。作者也做了casestudy，发现模型经过训练可以生成更合理的reason，做出原来做不出来的题目。</p><p><img src="../files/images/STaR/more_reasonable.png"></p><p>另外几个实验场景也有类似的结果。</p><p>对于数学题，作者还有另外的发现</p><p><img src="../files/images/STaR/steps.png"></p><p>作者罗列了答案的解题步骤的计算步骤和模型生成的解题目步骤的计算步骤，发现</p><ul><li>计算步骤数量比较接近</li><li>数量不同时，一般是模型的步骤更少。作者在这里做了casestudy，发现其实是模型找到了另外的、比正确答案更好的解法。有点神奇……</li></ul><blockquote><p>作者额外提到了一点，这个方法想要有效必须模型本身够强。同样的方法在GPT-2就没有效果。</p></blockquote><h2 id="我的思考">我的思考</h2><ul><li><p>感觉他toolformer类似，都是通过一个已有标注的数据集开始，让模型自己去”看着答案给步骤“。这样相当于对答案做reverse-engineer，让模型自己生成符合自己习惯的办法。</p></li><li><p>也许对于强的模型来说，学简单的例子没什么用处，in-context足以。只有自己做不出来的”困难例子“，才需要模型想办法去学习。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Reasoning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3-17总结</title>
      <link href="/4e410eb8.html"/>
      <url>/4e410eb8.html</url>
      
        <content type="html"><![CDATA[<p>转眼又是一周过去了，时间过得越来越快了。今天被同学提醒：开学已经第四周了，感觉好像才开学一样。</p><span id="more"></span><p>记得之前听过一个说法：人对时间的感知是按<spanclass="math inline">\(\log\)</span>分布的，最开始4-8天的体验和8-16天的体验感知是相同的。这样算下来我本科都读到第四年了，可能最后一学期的体验都没有新生入学开学第一周来得多。这个说法还是有些道理的，毕竟见到一个之前没见到的新事物的概率，大概就是一个负二项分布吧。</p><p>话虽然这么说，但这周还是过得挺快乐的。今天参加了popping的考团，过几天就是团员了。不只是新成员，老团员也都跳了自己的free-style。比起老团员，我的技术不能说帮臭，只能说烂成一团。不过所谓近朱者赤，想必过一年、过几年，我也能和大家一样强吧。</p><p>我的生活平淡如水，但这周世界上倒也发生了不少大事：GPT4、office-copilot……和人的时间感知正好相反，怎么感觉科学的发展是e指数的：蒸汽时代用了75年，电气时代用了50年，互联网用了20年，大模型从transformer诞生到现在只用了5年，模型与数据规模的增长更是超出了e指数的速度。今天实验室也讨论了新形势、新技术，感觉未来的研究生涯会过得很充实、很有参与感。</p><p>你说得对，但是周末又到了，明天是贵系的学生节。想起之前在学生节参与的节目——每当想起这些事，就觉得自己仿佛更年轻了，又和本科生涯连接了起来(现在虽没毕业，但自我感知和本科已经没什么关系了)。今年虽然没节目，但倒是把我的电子琴借了出去。这算什么，黑默丁格吗？我死了，但我的淘炮台还在输出？</p><p>毕业倒计时一百天了，看往年朋友圈，大家毕业100天的时候都是樱花开放；今年没等到樱花，倒是等来了沙尘暴。我不算喜欢樱花，也不算讨厌沙尘暴。就是这样吧，反正都比毕设中期答辩好。</p><p>好吧，毕设中期答辩就要来了。所以我的答辩怎么样了呢？我只能说，我不好说。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT4技术报告解读:应用报告</title>
      <link href="/1a72430d.html"/>
      <url>/1a72430d.html</url>
      
        <content type="html"><![CDATA[<p>今天openAI发布了GPT-4。直接把PaLM卷到开放API了，相信google是真的被卷麻了。</p><p>GPT4附赠了一个98页的报告(没有论文)，报告前面是性能和应用报告，中间是一些附录，最后是技术报告。我就按顺序来给大家讲讲，先讲应用报告。相信很多公众号大概都吹了一波GPT4，我希望我的讲解可以更深入一些，把问题、难点和领域更多地呈现给大家。</p><p><img src="../files/images/GPT4/exam_score.png"></p><span id="more"></span><p>作者团队来自openAI，比较神奇的是前面没写作者。报告里把贡献名单放到了最后，足足有3页，按照职能划分。一般的电影里的名单都很长，但大家都不会看完，但这次大家都看得很仔细，试图通过人员分布来发现openAI技术投入的侧重点。值得深思</p><p><img src="../files/images/GPT4/authors.png"></p><h2 id="introduction">Introduction</h2><p>整个应用报告部分其实没有什么技术，有点像是发布会吹牛逼部分的文字版，带有非常多的实验与结果。总体而言，GPT4比GPT3的区别可以总结为以下几点：</p><ul><li>GPT4支持KOSMOS类的图片输入</li><li>GPT4有更好的多语言能力</li><li>GPT4比GPT3.5有更好的事实能力，并且极大地提高了”安全性“</li><li>在简单问题上，GPT4和GPT3.5区别不大，只有问题复杂性达到一定地步以后，才能拉开差距</li></ul><p>除此之外，关于，模型细节，报告没有任何提供细节，只说了这样一段话：</p><blockquote><p>This report focuses on the capabilities, limitations, and safetyproperties of GPT-4. GPT-4 is a Transformer-style model [33] pre-trainedto predict the next token in a document, using both publicly availabledata (such as internet data) and data licensed from third-partyproviders. The model was then fine-tuned using Reinforcement Learningfrom Human Feedback (RLHF) [34]. Given both the competitive landscapeand the safety implications of large-scale models like GPT-4, thisreport contains no further details about the architecture (includingmodel size), hardware, training compute, dataset construction, trainingmethod, or similar.</p></blockquote><p>我为大家提取一下关键点：</p><ul><li>transofmer-based，但没说一定是transformer结构。</li></ul><blockquote><p>关于这一点，如果读了最近很多论文linearAttn等等，就会发现什么都能和transformer-based扯上关系，连CNN四舍五入也算transformer。所以这个说了等于没说，也不知道openAI做了什么优化使得大模型训练表现得稳定而且优异。</p></blockquote><ul><li>基础模型按照auto-regressive方式训练：嗯，唯一的关键点就是，一次生成一个token</li></ul><blockquote><p>这一点上，虽然报告没说，但我们得知GPT4的sequence-length打到了32768，比之前3.5提高了4倍。在这一点上，如何节省显存，如何构造寻找真正符合长度的、好的训练数据，都是很难、很值得研究的方向。</p></blockquote><ul><li>没提训练数据的任何信息，也没提代码数据的来源</li><li>没提模型规模，和模型结构</li><li>pretrain结束以后，使用了RLHF。但RLHF的过程、数据规模没提</li></ul><p>模型规模一直是大家期待的一个点，之前有人说大到1000B，也有人说小到10B，结果最后一点没提。</p><h2 id="predictable-scaling">Predictable Scaling</h2><h3 id="power-law">power-law</h3><p>这一部分很有意思，作者提到经典的大模型训练很难调参，因为经典的一次run就已经开销非常大了。GPT4开发出了一种基于power-law的方法可以在低至10000倍的训练时间下，就很吻合地预测出最终在下游任务上的表现。使用如下公式：<span class="math display">\[L(C) = a C^b + c\]</span> 对于复杂的pass@k之类的指标，使用更复杂的拟合等式： <spanclass="math display">\[\mathbb{E}_p[\log (\text{pass-rate}(C))] = a * C^{-k}\]</span></p><p>大概是用前面ckpt的测试结果拟合a、b、c，不知道之前大家有没有做个这件事情。</p><p><img src="../files/images/GPT4/1_10000.png"></p><p>我相信openAI内部一定是尝试了transformer的各种变体，对于各个任务做了这么一个工作流，然后每个变体都试了试，拟合一个最终loss，最后决定了具体使用怎样的模型结构、训练超参。</p><p>另外，这个拟合结果这么好，足以说明transformer真的是非常符合power-law的，以及现在的所有模型还都没有撞到性能墙，可以继续扩大规模、扩大epoch地训练下去。</p><h3 id="inverse-scaling-prize"><strong>Inverse ScalingPrize</strong></h3><p>这一部分，作者还额外提到了一个任务类型：探索一个随着模型大小增长，表现越来越差的任务。</p><p><img src="../files/images/GPT4/inverse_scale.png"></p><p>之前有一类工作专门找到了这样的数据集，作者提到：GPT4在这里表现出了相反的情况，效果打到了100%。这里我想提两个关键点</p><ul><li>首先这个”相反趋势“不是首创的，我记得之前PaLM里面好像就提过这个事情，说到了"U型"的趋势</li><li>最起码在这里的语境下，表明GPT4不是一个”小模型“，之前10B的论断可能不太准确。</li></ul><h2 id="capability">capability</h2><p>这一部分，作者系统性地吹了一波GPT4的表现，我就找一些比较好的实验来说一下吧。</p><h3 id="exam-result-academic-benchmark">exam result &amp; academicbenchmark</h3><p>这里包含三个实验，</p><ul><li>作者首先在比较有名的人类考试里召集实验者和GPT4，GPT3.5进行了笔试，发现GPT4基本可以排在前10%，而3.5只能排在后10%。</li><li>后者，academicbenchmark就是指传统的score。作者发现GPT4在几条样本few-shot的情况下，基本可以战胜所有对任务做了额外优化的训练方法</li><li>对于RLHF的性能，由于训练是human，评测也只能human。作者说基于GPT4的RLHF比ChatGPT的版本，人类有70%的概率觉得更好。</li></ul><p><img src="../files/images/GPT4/benchmark.png"></p><h3 id="多语言">多语言</h3><p>为了评测多语言性能，作者把认可度比较高的MMLU多类别选择题数据集翻译成了26种语言，并且输入给GPT4进行学习。发现在这些语言上做的都非常好。</p><blockquote><p>这其实是一个很神奇的地方，我们虽然不知道数据集情况，但可以想见某些小语种的数据一定不是很多。但模型随着某种语言(英语)的学习，竟然就能表现出很好的泛化到多语言的能力。这是不是说明人类的语言内在的相关性其实是很好把握的？</p><p>另一个思考的层面是，这个实验没有做GPT3.5以及PaLM等的版本，不知道这种“对于多语言的理解泛化能力”是不是和in-contextlearning能力类似，是随着模型基础性能的提升才“突然地”激增出来的。这也是值得我们探索的问题。</p></blockquote><p><img src="../files/images/GPT4/multi.png"></p><p>这里可以看出一个趋势，对于大模型来说，传统的academicscore其实并不能说明什么，大家已经刷的很高了，而且score高和humanprefer其实没有很大的相关性。最后主试验还是要通过”在humanexam上比较排名“的方式，某种程度上已经说明了评测领域的不足和缺陷。</p><p>想必openAI也注意到了这一点，专门提到他们开源了所有测试的代码(OpenAIEvals)，已经GPT4在每条数据上的结果，来帮助大家对自己的模型进行评测，后续也会完善这个框架。估计评测的问题应该会吸引更多的关注吧。</p><h3 id="图像">图像</h3><p>这就是作者的另一个卖点，输入图像。其实用Autoregressive做图像输入，openAI早有先见。最开始的DALL.E1,就是把pixel表示直接通过Autoregressive输出来实现图像生成的。</p><p>作者虽然没有提具体的细节，但现在主流的方法PaLM-E等都是用一个专门的图像编码器来编码图像，然后作为"tokenembedding"注入到sequence里面去的，不知道openAI是不是在自己DALL.E2那个不开源的超级text2image数据集上整了个新的图像编码器。但看起来能力确实很强。</p><p><img src="../files/images/GPT4/visual.png"></p><p>不只是输入，在这里我还是觉得以后可以像versatilediffusion一样做出图像模态的输出来，直接套一个类似DALL.E 2的unCLIPprior模型大概就能有模有样。我甚至怀疑，他们已经做了，只是效果不尽如人意，或者“存在潜在的偏见和风险”，所以没有放出来。</p><p>另一方面，关于这种图像输入的模式，不知道是谁先发明的。虽然Kosmos说他们是the firststep，但我们从GPT4技术报告可以看到：<strong>其实GPT4早在去年8月份就已经训练完成了</strong>。</p><blockquote><p>走向多模态、通用性，才是未来大模型的发展方向呀。</p></blockquote><h2 id="limitation">limitation</h2><p>这一部分，作者分析了事实性错误、幻觉现象、偏见、危险内容等等信息，大致得出了两个结论：</p><ul><li>GPT4的安全性比GPT3.5大大提升</li><li>但是GPT4也没有“很安全”，大家谨慎使用</li></ul><p><img src="../files/images/GPT4/factual.png"></p><p>上面这种图我不是想说score的高低，我只是想吐槽：什么时候chatgpt都有v2,v2,v3,v4了，我怎么我感觉我火星了……</p><p>作者在MMLU做了一个很有意思的自检测实验：</p><blockquote><p>模型自己认为置信度高的回答，很可能最终的置信度就是真的很高。</p></blockquote><p>这个结论比较符合之前的研究结果。某种意义上，这说明模型自身对于所谓“安全性”是有一定认知的。但作者走的更进一步，也检测了RLHF后的模型，发现这种偏序关系基本消失了。</p><blockquote><p>我认为，也许RLHF或者类似的方法，更重要的是通过模型参数一些小小的迭代，激发出来模型对于自身输出回答的可靠性的认知。所以RLHF模型自己丧失了偏序能力。这也许侧面说明，我们有可能通过更简单的方法，比如prompt等，达到RLHF的效果（当然，RLHF的单步RL假设已经够简单了……</p></blockquote><p><img src="../files/images/GPT4/believe.png"></p><p>最后作者总结了文章的贡献，并且很自信的说了一句：</p><blockquote><p>Though there remains much work to be done, GPT-4 represents asignificant step towards broadly useful and safely deployed AIsystems.</p></blockquote><h2 id="我的思考">我的思考</h2><p>一篇看下来，感觉最大的震撼是：openAI的一切基本都是自己的，而且所有层面都是领先的</p><ul><li>自己提出的Autoregressive训练方法</li><li>自己研发的高效训练平台</li><li>自己制作的闭源数据集</li><li>自己创造的评测框架</li><li>自己聘用的人工评测、安全性评估专家团队</li></ul><p>回过头想想，之前GPT1被BERT迎头痛击以后，openAI还是没有放弃Autoregressive，一直在迭代版本，不知道积累了多少idea和失败的实验经验，几年下来，已经和我们形成了技术壁垒了。98页的报告其实说了很多，但也什么都没说。我们想要赶上，最起码得在最少一个层面做出自己的突破呀。</p><p>当然，我们也有一些自己的优势，比如最起码，我们不用饱受“安全性、偏见”的困扰。也许对我们来说，只用技术报告最后一句话的前半句更合适一些：</p><blockquote><p>There remains much work to be done</p></blockquote><p>是机遇，也是挑战，可能挑战更多一些。与君共勉。</p><p><del>以上内容由GPT4生成(bushi</del></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Toolformer: Language Models Can Teach Themselves to Use Tools</title>
      <link href="/78ff6196.html"/>
      <url>/78ff6196.html</url>
      
        <content type="html"><![CDATA[<p>上次讲了多模态大模型，今天讲另一个有意思的大模型工作：toolformer——如何让大模型自己学会去使用工具。在使用工具的情况下，6.7B的GPT J效果远胜175B的GPT 3.5</p><p><img src="../files/images/toolformer/intro.png"></p><span id="more"></span><p>作者团队来自Meta AI</p><p><img src="../files/images/toolformer/authors.png"></p><h2 id="introduction">Introduction</h2><p>作者在这里和Kosmos类似，讲到(语言)大模型能力越来越强，可以理解人的需求，理解人的问题。但是，对于一些精确计算，或者涉及到事实性的地方，模型的表现总不是太好、或者说不稳定，即使这是一个很简单就能解决的问题。</p><p>一个比较好的解决这个困难的办法就是引入外部的工具。让模型去调用外部的工具。这就是本文的主要贡献toolformer:</p><ul><li>一个self-supervised，不需要人工标注的学习方法，让模型自动的学习使用工具的方法。</li><li>大模型应该有能力去自己决定要不要使用工具，使用哪个工具。</li></ul><h2 id="method">method</h2><p>下面看一下到底是怎么做到的吧。</p><p><img src="../files/images/toolformer/method.png"></p><p>toolformer首先要从一个“强”的大模型出发，本文用的是GPT-J，一个6.7B的预言模型。第一步，是生成一个带有tool标注的数据集。作者使用通过in-context的方式，现在prefix里写一些人使用工具的prompt，接下来让模型对训练数据集里面自己生成很多的工具使用示例。</p><p><img src="../files/images/toolformer/prompt.png"></p><p>对于一个工具调用，作者使用token来代表</p><p><img src="../files/images/toolformer/api.png"></p><p>LM模型会生成很多带有工具调用的实例</p><p>对于一个好的工具使用事例，理论上通过观察工具的数据，降低后面正确答案的loss，作者通过这个偏置信息。计算使用和不使用工具时对于后文的loss。作者定义了给定前缀z和权重w的情况下的loss算法<span class="math display">\[L_i(z) = - \sum_{j=1}^n w_{j-1 }·\log p_M(x_j | z, x_{1:j-1})\]</span> 里面的weight w，希望更靠近工具调用的部分，选用 <spanclass="math display">\[w_t = \frac{\tilde{w_t}}{\sum_{s\in \mathbb{N} }\tilde{w_t}} \text{with}\quad \tilde{w_t} = \max(0, 1 - 0.2 t)\]</span></p><p>也就是说越靠近工具结果t的地方权重就越大。</p><p>对于给定一个API call，<span class="math inline">\(e(c_i,r_i)\)</span>,算出不同的loss <span class="math display">\[L_i^+ = L_i(e(c_i,r_i)) \\L_I^- = \min (L_i(\epsilon), L_i(e(c_i,\epsilon)))\]</span> 如果使用工具以后，loss比原来降低超过阈值 <spanclass="math display">\[L_i^- - L_i^+ \geq \tau_f\]</span>就认为是有效的工具调用，保留这次生成。最终制作出一个带有有效工具调用的数据集。然后让LM在这个数据集上fine-tune一下，就学会了工具调用。</p><p>在实际使用时，如果模型生成了工具调用token&lt;API&gt;等到工具输入结束生成token→以后，就执行工具，然后把结果直接贴过去。</p><h2 id="experiment">experiment</h2><p>作者在实验中实际使用了以下工具，使用方法如下：</p><ul><li>计算器</li><li>日历</li><li>LM-based问答工具</li><li>Wikipedia Search</li><li>翻译系统</li></ul><p><img src="../files/images/toolformer/tools.png"></p><p>作者在CCNet预训练数据集上进行了工具数据集生成，使用GPT J6.7B作为LM基础模型。</p><p>生成的带标注的数据大概在几十万这个量级，其实不是很大。</p><h3 id="主试验">主试验</h3><p>在很多任务上做实验做实验</p><ul><li>SQuAD, Google- RE, T-REx几个事实性任务</li><li>ASDiv等几个数学任务上做评测</li><li>QA任务</li><li>翻译任务</li></ul><p>作者选定了一下baseline，</p><p><img src="../files/images/toolformer/baseline.png"></p><p>分别是阳性对照、阴性对照，还有主试验模型和他的阴性对照。</p><p><img src="../files/images/toolformer/result.png"></p><p><img src="../files/images/toolformer/result2.png"></p><p>可以看到</p><ul><li><p>基本上toolformer远胜没有tool的方法，即使是175B的 GPT 3text-davinci</p></li><li><p>工具调用的输入也会带来表现得提高。我理解有可能正确的工具输入会给模型一种类似”chain-of-thought“的效果？</p></li></ul><p>另外，作者也做了些对比实验。</p><h3 id="规模实验">规模实验</h3><p><img src="../files/images/toolformer/size.png"></p><p>作者探索了不同模型大小对于工具学习的能力，可以看出：当模型小的时候，toolformer和基础模型拉不开差距。但随着模型增大，toolformer和基础模型的差距越来越大。也就是说，只有模型”大“起来之后，内部的推理、理解能力变强，才逐渐拥有了学会调用工具的能力。</p><h3 id="k实验">k实验</h3><p>比较有意思的是k的实验</p><blockquote><p>解码的时候有个小trick，如果token&lt;api&gt;的概率排在前k，就直接认为输出了api调用。k越大，模型的调用就越频繁。</p></blockquote><p><img src="../files/images/toolformer/k.png"></p><p>可以看出，当k小的时候其实效果并不好，也就是说模型对于输出api调用是很不自信的。</p><blockquote><p>假如我是研究员，很可能最开始就是设定k=1。然后发现效果拉胯，这个时候，其实是很考验发现问题的能力的。值得深思</p></blockquote><p>最后，作者谈到了toolformer方法的局限性</p><ul><li>不能链式调用工具，一个输出作为另一个输入，只能一次一个工具</li><li>不能zero-shot学习，最起码要人进行一些prompt的辅助，但人其实是可以通过看说明书、--help等方法进行zero-shot学习的</li><li>对于k很敏感</li><li>数据集生成效率很低：过了几千万的文档，最后就生成了几十万的有效调用</li><li>模型对于工具本身的开销没有计算</li></ul><h2 id="我的思考">我的思考</h2><ul><li>感觉让大模型使用工具，也是基础模型能力增强以后的一个进化方向。</li><li>理论上，多模态大模型也可以算作工具的一种：比如一个diffusiontext2image，可以看做一个文生图的工具。OCR也可以看做一个图生文的工具</li><li>GPT J + tool都这么强，如果是davinci-003 + tool呢？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> tool-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我对ChatGPT和近期NLP新形势的想法</title>
      <link href="/728d9a40.html"/>
      <url>/728d9a40.html</url>
      
        <content type="html"><![CDATA[<p>过完新年，时间过去不长，没想到NLP领域却出现了新的风暴——ChatGPT(GPT-3.5-turbo)。ChatGPT自身是instructGPT+GPT3.5的结合体，大概在去年12月，就是我得新冠那时候推出。我记得我当时还顶着高烧听n+e学长做分享，但没想到后来会到火出圈的程度。</p><span id="more"></span><p>昨天回高中宣讲，感觉现在ChatGPT的火热程度甚至超过了当时的AlphaGO,很多高中小朋友不知道大学的专业划分，就来问问ChatGPT属于哪个专业，叉院是时候成立一个“chaì班”了，笑死。</p><p>新学期开始，感觉很多NLP的工作一下就显得不那么重要了，听说美国高校甚至很多NLP相关的项目直接被撤资了。我仔细回想了一下：这一切是怎么发生的，为什么NLP领域发展这么久，ChatGPT就能一下获得这么高的关注度？</p><h2 id="align">Align</h2><p>传统的NLP模型、任务，一般是定义好了很多的metric，然后大家通过score的高低去评判好坏。不过在文本生成领域，经典的score比如BLEU，ROUGE-L等等其实都有很多的问题，所以比较好的论文一般都会引入人工评测：让人去评baseline和新方法的生成质量高低。</p><p>ChatGPT其实就是更进一步：既然我们引入人工评测，那干脆把人工评测当成一个信号，指导模型生成。这就是RLHF里的HF(humanfeedback)。</p><p>说回到刚才的文本生成。任何的机器学习模型都是在进行拟合，或者说在align到某个指标。之前的指标是align到score，而ChatGPT是align到“人”。这就不难理解为什么”人“会更喜欢ChatGPT了。</p><p>就应用性和宣传性来说，human align比起scorealign是具有显著优势的，毕竟冰冷冷的score-SOTA看起来也没什么大不了。”想要让工作有好的宣传，一定要有一个更符合humanalign的展示形式“，这是值得我们思考的第一个点。</p><h2 id="why-chatgpt">Why ChatGPT?</h2><p>ChatGPT的RLHF思路的推导其实是比较正常的，但反过来想想，之前大家竟然没想到这么做？</p><p>其实也不是没想到，只是之前的pretrainmodel效果没有那么好。举个例子：</p><blockquote><p>最近metaAI新挂出来一个LLAMA-65B模型宣称达到了新SOTA，但在某数据集上有69%的AUC，但GPT-3.5-002模型的score是77.4%，何况现在GPT-3.5已经迭代到了003。</p></blockquote><p>类似的例子还有很多，无不说明GPT-3.5预训练模型冠绝世界，甚至技术代差似的zero-shot语义理解和生成能力。还记得两年前GPT3刚发布的时候，也很火，不过由于还是scorealign，所以引入的关注度可能相对没有这么高。但这几年，openAI一直在默默的迭代基础模型的能力：</p><ul><li>引入代码训练</li><li>提高数据质量</li><li>增强训练效率</li><li>修改backbone表示能力</li><li>...</li></ul><p>其实，GPT2开始，openAI一直在宣传”in-contextlearning”，到GPT3宣传“zero-shot”概念，都是逐渐感受到了预训练模型能力膨胀带来的新趋势。很可惜，学术界到一直到2022年，关注度一直不是很高。主要原因是开源的预训练模型可能并没有达到这种效果，甚至fine-tune、delta-tune的效果还比不上人家的in-contextone-shot效果。</p><p>有一些工作曾经提出：随着预训练模型的能力叠加到了一定限度，模型才能逐渐理解人的需求，理解人给出的feedback指标高低“到底意味着什么”。关于这一点，其实我之前使用了我们实验室的CPM3进行过类似HF的实验，发现模型训练非常不稳定，会把得分高的语句的相关词语学成“高平分的关键因素”。所以GPT-3.5+RLHF,可以说是如虎添翼，一拍即合了。</p><p>之前的发展先不谈，现在ChatGPT一出现，所有研究者都真正意识到back-bonemodel的技术代差了。洛阳纸贵，年后国内的算力供应商租卡价格都涨了大概50%。估计未来一年到几年，中国几家大公司应该要开始卷大模型训练的算力、宣传了。</p><p>很多人说技术代差追不上，我到没有那么悲观，我倒觉得这个领域，钱比技术更重要。我们反而应该感谢ChatGPT，让大家更愿意在这个领域投钱了，毕竟之前一个run就要1000万的价格，没有多少人愿意出。</p><p>预训练模型是一个很烧钱的事情，并且创新性不是很多，学术名声上也不是特别好，前几年一般都用“军备竞赛”之类的词形容，现在这个词也没人提了。openAI一直在持续的做这件事，理由其实很简单：它是卖大模型inferenceAPI挣钱的。学术界一方面不挣钱，一方面大家不愿意投钱，产生差距很正常。</p><p>“为什么openAI可以从一个纯学术组织发展到现在自给自足盈利也很多，经济技术双丰收；其他高校和学术组织就像是啃老族，总是等着别人救济”，是值得我们思考的第二个问题。</p><h2 id="rlhf-and-super-pretrain-model-is-all-you-need">RLHF and "super"pretrain model is all you need?</h2><p>回到RLHF技术本身。那么以后大家就都做RLHF了吗？其实instructGPT自己也发现了：align到任务和align到人某种意义上是冲突的。随着RLHF，模型对于基础NLP任务的能力反而会下降。对这种现象，我的理解是：</p><blockquote><p>随着预训练模型能力的增强，模型不是变得更强，而是变得更“flat”，会更容易泛化到下游任务上。由fine-tune变成prompt、instruction-tuning。</p></blockquote><p>之前几年，很多NLP工作的开展形式是：</p><ul><li>发现问题，具体化定义问题</li><li>制作数据集、定义metric</li><li>跑实验，做分析</li></ul><p>我觉得其中的第二步、第三步未来可能会越来越不重要，因为对于instructiontuning来说，你很可能只需要用自然语言描述你的需求，模型就可以理解你的需求了。ChatGPT令人着迷的点，正在于此：即使再抽象的需求，模型也可以理解，并且在只有几个或者没有example的情况下，做出相对不错的结果。</p><p>在这种情况下，在所有的研究的最开始，我们都要先提出两个问题：</p><ul><li><p>你的需求，为什么要用你的方法，而不是ChatGPT？</p></li><li><p>你的需求，ChatGPT能做到多好，现在还不够好吗？</p></li></ul><p>如果回答不出来，那么这个研究大约就是要被淘汰的。作为研究者，我们大约会衍生出两种研究选题：</p><ul><li>如果把大模型更好的align到下游的需求上去</li><li>如何不用大模型或者在用不了大模型的场景下，提出更有优势的方法</li></ul><p>“在NLP的新形势下，如何想出真正对时代有意义、有价值，让大家愿意顺着你的思路做下去的工作”，这是值得我们思考的第三个问题。</p><p>总之，无论愿不愿意，我认为NLP新的领域已经来临了，未来会有更多的投资、更多的关注度。作为研究员，既要思考ChatGPT成功的关键因素，也要想清楚新形势下的研究必须要满足什么条件。</p><ul><li>为什么我做不出来ChatGPT？</li><li>我可以做出来“下一个ChatGPT”吗？</li></ul><p>战战兢兢，如临深渊，如履薄冰。敌方回合结束，现在压力来到了我这边，希望我不会成为最先被淘汰的那个职业。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Language Is Not All You Need: Aligning Perception with Language Models</title>
      <link href="/cf5a0a9e.html"/>
      <url>/cf5a0a9e.html</url>
      
        <content type="html"><![CDATA[<p>今天来讲一篇前两天刚挂上的多模态预训练的论文,这个模型KOSMOS可以直接输入文本、图像模态的信息，也可以平滑的进行多模态的任务，甚至是智商测试。值得一提的是，模型甚至只用1.3B</p><p><img src="../files/images/KOSMOS_1/intro.png"></p><span id="more"></span><h2 id="introduction">Introduction</h2><p>作者团队来自微软，主要参考了他们的之前一篇工作</p><blockquote><p>MetaLM：Language Models are General-Purpose Interfaces</p></blockquote><p><img src="../files/images/KOSMOS_1/authors.png"></p><p>从标题就可以看出，这是在碰瓷transformer的热度，并且点出了关键点：</p><blockquote><p>他们要做above language的LLM工作</p></blockquote><p>这就是本文的核心概念：MLLM。</p><p><img src="../files/images/KOSMOS_1/intro2.png"></p><p>所谓MLLM(multimodal large languagemodel)，就是说要输入多模态的内容，然后完成语言模型的任务。其实这也是现在一个新的趋势，从之前versatilediffusion论文出来开始，就能看出预训练大模型现在有能力去处理多模态的内容了。</p><p>在这篇文章中，作者仿照一般大模型论文的写法，做了下面的事情：</p><ul><li>占坑了MLLM这个概念</li><li>作为概念，整了个多模态的预训练数据集</li><li>作为例子，炼了一个<strong>1.3B</strong>的丹KOSMOS-1，并且说明比起之前的工作效果更好</li></ul><p>这里可以看出，作者心是很大的，一般上来就叫xxx-1的，就是想要比肩GPT系列，形成类似行业规范的大模型。</p><p>下面就来看看本文主打的方法吧</p><h2 id="from-llms-to-mllms">From LLMs to MLLMs</h2><p>现在的预训练语言模型LLM的水平极高，并且有很强的任务理解能力，或者说通用性的语言理解和建模水平。拥有成为“general-purposeinterface”的潜力或者说实力。</p><p>然而所有的LLM有一个绕不开的问题：只能输入文本。因此很多任务“不得不”对齐到文本模态，比如表格生成等等。真正的通用人工智能显然不能只有语言模态：需要输入图像、文本、声音、视频，输出文本、图片图像、乃至动作空间等等。这就意味着我们需要更进一步。</p><p>作者在这里迈出了一小步：先把输入变成多模态的，输出还是只有文本模态，称之为multimodallarge language model</p><p><img src="../files/images/KOSMOS_1/intro3.png"></p><p>这样，很多多模态任务就可以很轻松的转换成MLLM模型的输入格式，比如说：</p><ul><li>文字识别任务不需要专门定义，就输入一个图片，然后问“文字写的是什么”就可以了，甚至还可以针对回答进行一些进一步的对话</li><li>图像描述也不需要专门定义，还是把图片喂进去然后进行一个类似对话问答的场景即可</li><li>甚至还有很多开放性的图像问答类任务，“图片背景里的咖啡店是什么店？”，这种开放性问题之前是没法处理的，但对于MLLM都是可以做到的</li></ul><p>可以想见，这种MLLM的通用性比一般的LLM更好，现在的LLM能力也宣誓着MLLM确实可以做得很好。接下来，作者就真的训了一个1.3B的MLLM出来</p><h2 id="kosmos-1">KOSMOS-1</h2><h3 id="表示方法与训练目标">表示方法与训练目标</h3><p>想要输入图片，就要表示图片，这里有两种思路：</p><ul><li>用类似ViT的格式把图像和文本直接表示在一起</li><li>用一个专门的图像编码器编码图像，再把类似图像latent的东西喂给LLM</li></ul><p>作者使用的第二种方法，并且用了一个锁参(除了最后的linear层)的ViT-CLIP模型作为图像编码器</p><p>图像编码完了以后就直接把latent当做token喂给模型。模型对于这种图像、文本一起的内容进行auto-regressive的学习</p><h3 id="数据集">数据集</h3><p><img src="../files/images/KOSMOS_1/data.png"></p><p>数据集是所有预训练模型任务里最重要的部分，也是基本上大多数论文阅读者都会跳过的内容。我们就来详细看一下作者的训练数据是怎么来的。</p><ul><li><p>首先是纯文本部分，作者和已有的模型GPTxxx、OPT、PaLM、Chinchilla、LLaMa等等类似，关键点是加入了代码数据训练</p></li><li><p>接下来是混合模态部分,这里有两大类：</p><ul><li>imagecaption：用了text2image的数据集，最大的那个LAION-5B里面2B的英文子集</li><li>Interleaved Image-TextData：这个是带图片的网页，作者从2B网页里筛出来了71M的网页，没有公布筛选的办法，但是作者提到删掉了一半只有一个图片的网页</li></ul></li></ul><h3 id="模型结构">模型结构</h3><p>主体是transformer encoder+casual mask，总体大小1.3B：</p><ul><li>24 layer</li><li>32 head</li><li>2048 hidden dim</li><li>8192 FFN dim</li></ul><p>这里就只说和transformer改进的地方：</p><ul><li>加了更多的layerNorm，然后是采用postNorm</li><li>MEGANETO初始化方式</li><li>采用的是相对位置编码，还是xPos变体，这个其实挺少见的，现在大多都是rotaryembedding了</li></ul><h2 id="评测">评测</h2><p>这一部分就说一些好玩的结果吧</p><p>首先是智商测试</p><p><img src="../files/images/KOSMOS_1/IQtest.png"></p><p>这是现在唯一一个能做智商测试的模型，虽然结果不怎么样，但是这个任务定义真的让我感觉我们又向着通用人工智能走近了一步</p><p><img src="../files/images/KOSMOS_1/IQresult.png"></p><p>剩下的很多任务作者做的评测还是比较完善的</p><p><img src="../files/images/KOSMOS_1/tasks.png"></p><p>我们大约需要关注几个结论：</p><ul><li><p>作者的评测baseline都是类似大小的模型，不过大的也没有</p></li><li><p>引入多模态输入的训练，对于任何一个模态都是很有帮助的</p></li><li><p>对于纯文本任务，表现没有相同训练成本的LLM baseline好</p></li><li><p>模型确实可以理解语义复杂的多模态任务定义，并且向着正确的方向做，即使模型只有1.3B</p></li></ul><p><img src="../files/images/KOSMOS_1/compare_with_LLM.png"></p><h2 id="我的思考">我的思考</h2><ul><li><p>虽然大小只有1.3B，但由于是多模态输入，其实训练成本并不低，这点要注意</p></li><li><p>之前看过一个说法叫“最小发布单元”，就是把一个工作的贡献拆成多个论文，最后凑出多篇论文的发表。感觉这篇论文作者就有很多后手等着：</p><ul><li><p>模型能不能再大点？</p></li><li><p>既然是多模态输入，哪能不能输出图片？(现在Autoregressive直接输出clipembedding了，接个残血的DALL.E 2就行)</p></li><li><p>图像和文字端可以联动，训练数据也有代码，是不是代码和图像模态也可以联动一下？比如来个什么代码可视化</p></li><li><p>训练输入是网页，那应该天生做网页生成很在行</p></li><li><p>以上这些估计就是作者在KOSMOS-23456里想做的了</p></li></ul></li><li><p>我觉得这种MLLM，乃至MLM(multimodal largemodel)可能是未来几年的趋势，毕竟现在网页数据的commoncrawl，图像描述的LAION系的数据量这么大，搞一堆几百B的大模型应该不会缺数据。尤其是chinchilla之类的工作还说了现在的训练量不够</p></li><li><p><del>Language Is Not All You Need</del> money is all youneed</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3-4总结</title>
      <link href="/b6f8ec41.html"/>
      <url>/b6f8ec41.html</url>
      
        <content type="html"><![CDATA[<p>看了一下，竟然已经三个月没有更新了。寒假回了家之后，那边的环境不是特别方便写笔记，但没想到返校以后，还是没有复更。</p><p>不能再这样下去了！从今天开始，恢复正常的更新频率，最少一周要有两篇阅读笔记。</p><p>最近看了“苏剑林”老师的博客<ahref="https://spaces.ac.cn/">科学空间</a>。同样是论文阅读，很多论文也是我之前读过的，但感觉我的理解深度还有很多欠缺，还有很多可以深挖的地方我没有发现。</p><p>想想还是很感慨：为什么我就想不到这其中很多的细节呢。也许随着继续的、更多的阅读和学习，我也能逐渐提高我的思考深度吧。希望以后我的论文阅读笔记，也能让大家读起来能有向科学空间一样的收获感。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12-13总结</title>
      <link href="/34250cd3.html"/>
      <url>/34250cd3.html</url>
      
        <content type="html"><![CDATA[<p>发现好久没更新随笔了，今天更一篇。首先，我前几天阳了：头疼、发烧、嗓子疼三件套循环一轮。我属于无症状感染者，就是等于得了一次大号流感。</p><span id="more"></span><p>说起来还挺好玩，我前一天晚上可能是因为得了，所以有点失眠睡不着觉，就通宵写了一晚上代码。第二天有点头疼，我还以为是因为通宵就没管，结果第二天夜里马上就难受了，第三天醒来一看，38度。烧到了下午，退烧了，第四天也没复发。我看着觉得好了，就洗了个澡……结果出来马上又发烧了，这下烧到第五天，晚上退烧了。第六天我也没敢再洗澡……阴差阳错，也算是多体验了两天新冠。</p><p>抛开致死率不谈，我还挺喜欢阳的。毕竟可以光明正大的给科研请请假，躺在床上睡会，好像还挺爽..?病好了以后，我又出去吃了顿饭，感觉生产生活确实在恢复，大家也都从家里出来了。一个城市，总要经历几个阶段：</p><ul><li>"杨过"：就是阳性过境，大家都开始感染</li><li>“杨康”：大家感染完康复了</li></ul><p>我觉得保定现在已经步入杨康阶段了，快递外卖小商店都在开业，街上的人口罩越来越少，笑容越来越大：后疫情时代的中国生机，可能正从保定萌发而出。</p><p>最后，最近在家里装机，记录配置如下：</p><ul><li>13代i5 13600：你想要的，这里都有；你的下一个i9，何必是i9</li><li>华硕b660m主板：是的，13代酷睿竟然兼容b660m。我有考虑用新的z790，想了想我又不超频也没啥用</li><li>32g内存 3200Hz，ddr4：现在ddr5还是有点贵，下次一定……</li><li>1T固态：看起来现在固态好像降价了？我记得之前吹固态涨价吹的挺厉害的呀</li><li>3060丐：反正破发了，我赚了，黄仁勋亏了</li><li>s4 120风扇、m2机箱、650电源：标配..?</li><li>32寸带鱼屏：上次买的</li><li>azzurro配色键鼠垫三件：天空之境、G304。实物上手巨好看。不过红轴的声音只能说，居家办公。只能说，不是青轴就是成功</li></ul><p>配件还在路上，比较期待，对了，我还得做个启动盘。</p><p>现在身体和精神基本恢复了，后面多读几篇论文、多写几篇分享。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control</title>
      <link href="/49dce4ad.html"/>
      <url>/49dce4ad.html</url>
      
        <content type="html"><![CDATA[<p>这篇文章是最近一个月新挂arXiv的文章，看格式是打算瞄准ACL2023。作者用一个diffusionbased和GPT2大小一样的模型打败了GPT-2。让我们一起看看是怎么做到的吧。</p><span id="more"></span><p><img src="../files/images/SSD_LM/authors.png"></p><p>作者团队来自华盛顿大学和CMU。</p><h2 id="introduction">introduction</h2><p>作者上来先说了diffusion textgeneration虽然很新、很controllable，但并不能赶上auto-regressive的效果，本文就是要解决这个问题。</p><p>接下来，作者谈到已有的diffusion文本方法基本都是在latent层面做，而已有的classifier基本都是在token(wordpiece) level做训练的，这有一个mismatch。</p><ul><li>需要重新训练classifier做guidance</li><li>对于不同的t，甚至要训练不同的classifier-t</li></ul><p>而作者的方法直接在离散token层面搞，可以直接把已有的classifier拿过来用。</p><p>最后作者拍出来结果：</p><ul><li>我们用GPT-2一样的训练数据，基本一样大的模型</li><li>在无条件生成上打败了GPT-2</li><li>在可控生成上打败了很多可控的baseline(基本都是Autoregressive的)</li></ul><h2 id="method">method</h2><p>background部分讲了传统diffusion和传统Autoregressive的方法，这里我就跳过了，感兴趣的同学可以看看前面的笔记。</p><p>method部分我重点讲作者的两个改进:</p><h3 id="block-level-diffusion">Block-level diffusion</h3><p>这一部分说的是传统的文本diffusionmodel都是对于一组输入进行加噪去噪，导致文本的语义非常难以保持，而本文作者想到一种所谓semi-Autoregressive的方法，每次在一个block内进行全block的去噪，然后在block间采取Autoregressive的方法。</p><p><img src="../files/images/SSD_LM/model.png"></p><p>在这种情况下，模型的生成其实是给定前文的，没有噪声的之前blcok的信息也传入模型内。这样单独block内的去噪其实有比较好的语义性，因此流畅度和一致性就会更好。</p><h3 id="continuous-data-representation">Continuous datarepresentation</h3><p>这里作者告诉我们如何把离散token转换成logits。作者用了一个所谓almost-one-hot的方法，就是把w转成一个和词表大小一样的embedding，其实每个维度的定义如下<span class="math display">\[\tilde w_{i} = \left\{\begin{aligned}+K, \text{when}\quad  w = V_i \\-K, \text{when}\quad  w \neq V_i\end{aligned}\right.\]</span> 这其实相当于把单词的wordembedding手动指定成了这个形式。在传入atomic模型时，就先把离散文本变成这个logits表示即可。</p><p>注意，这种情况下，随机噪声的方差不是1而是<spanclass="math inline">\(K^2 I\)</span>,因此祖先采样的最终公式是这种形式<span class="math display">\[\tilde w^{C:C+B}_0 = \text{logits-generation}(w^{C:C+B})\]</span></p><p><span class="math display">\[\tilde w^{C:C+B}_t = \sqrt{\overline{\alpha_t}} ·\tilde w^{C:C+B}_0 +\sqrt{1 - \overline{\alpha_t}} · \epsilon_t\]</span></p><p><span class="math display">\[\epsilon_t \in \mathcal{N}(0,K^2I)\]</span></p><p>仔细对比一下，和之前LisaLi那篇比是不是就相当于把所谓的rounding层给手动指定了wordembedding。我个人认为好处是所有的单词针对embedding空间更平均，因为之前的end2endwordembedding其实是希望类似单词更接近，这样可能造成embedding空间非常稀疏。</p><p>那么解码该怎么办呢？因为模型输出的是logits，如果要用classifier，其实是希望先回到token层面的。作者对标Autoregressive的解码方法提了三个变体，这相当于在LisaLi的基础上(她是相当于用greedy)更进一步:</p><h4 id="greedy-projection">Greedy projection</h4><p><span class="math display">\[\tilde w_{i} = \left\{\begin{aligned}&amp; +K, \text{if}\quad  i = \text{argmax}(w_{\text{logits}}) \\&amp; -K, \text{else}\end{aligned}\right.\]</span></p><p>就是取概率最大的意思</p><h4 id="sampling">Sampling</h4><p>就是把概率按照top-p方法采样一个结果，p是超参数。</p><blockquote><p>top-p采样：相当于选取概率最大的一些token，保证选到的token的概率之和&gt;p，然后剩下的score置为负无穷，最后再按照softmax以后每个token的概率大小采样</p></blockquote><p><span class="math display">\[\tilde w_{i} = \left\{\begin{aligned}&amp; +K, \text{if}\quad  i = \text{top-p-sampling}(w_{\text{logits}})\\&amp; -K, \text{else}\end{aligned}\right.\]</span></p><h4 id="multi-hot">Multi-hot</h4><p>这个意思就是top-p留下的所有token都有可能选到 <spanclass="math display">\[\tilde w_{i} = \left\{\begin{aligned}&amp; +K, \text{if}\quad  i \in \text{top-p-all}(w_{\text{logits}}) \\&amp; -K, \text{else}\end{aligned}\right.\]</span> 采样时每个中间状态要先用一个解码策略回到token level，作者称为logits-projection，再加噪声变成下一次的噪声样本。当一个block解码完成后，就和之前的拼接在一起，然后进行下一个block的解码。</p><h2 id="总体的训练和采样">总体的训练和采样</h2><p><img src="../files/images/SSD_LM/algorithm.png"></p><p>如果要做classifier guidance的话，只需要拿出一个tokenlevel的提前训练好的classifier对中间状态进行指导。 <spanclass="math display">\[\tilde w^{C:C+B}_t = \tilde w^{C:C+B}_t + \lambda \Delta_{ w^{C:C+B}_t}f_\phi(y | \tilde w^{C:C+B}_t, w ^{&lt; C})\]</span> 作者使用的训练语料就是GPT-2 OpenWebText，有9Btoken，总体模型作者使用RoBERTa-large，是一个transformerencoder。其中训练用的几个超参定义如下：</p><ul><li>L=200。一次采样的所有长度</li><li><spanclass="math inline">\(B_{train}=25\)</span>，就是说前文最多175个token</li><li>T=5000，diffusion step，看起来很大</li><li>K=5。almost-one-hot概率用的超参，这应该和词表大小有关系</li></ul><p>然后作者在32 V100并联跑了6天，100K steps ，batch_size=6144,lr=1e-4</p><h2 id="result">result</h2><p><img src="../files/images/SSD_LM/result.png"></p><p>作者在多个无条件生成的指标上报告了SSD-LM的记过，可以看出和Autoregressive模型不相上下</p><blockquote><p>值得一提的是，纯的PPL不是越低越好，可能是重复，然后作者也报告了和goldenreference的log差值ppl。差值更小就更接近人的输出。</p></blockquote><p>接下里，作者实验了几种解码方法的影响</p><p><img src="../files/images/SSD_LM/decode.png"></p><p>可以看出：</p><ul><li>p越小ppl越低，但是作者提到可能会有很多重复</li><li>multi-hot效果不好，可能是因为和训练时的输入形式不一样</li></ul><blockquote><p>也许可以考虑在训练时变成那种MASK的格式？</p></blockquote><p>作者后面又做了可控生成的实验，就是sentiment可控的实验。作者首先调用了一个已有的classifier做guidance。同时为了评测，作者在yelpreview上额外训练一个classifier得分(C-Ext)，来测试输出的语句的情感得分。</p><p><img src="../files/images/SSD_LM/controllable.png"></p><p>可以看到，SSD-LM很好的平衡了diversity和PPL。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
            <tag> 文本生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12-1总结</title>
      <link href="/7109427b.html"/>
      <url>/7109427b.html</url>
      
        <content type="html"><![CDATA[<p>昨天我被封控了，楼里有十混一阳性，盲盒还真开到我了。封控对生活的影响其实也不大，虽然出不去楼，但盒饭会自己长腿跑到我的身边。四菜一饭，加上发的饮料和小食，算是比较高质量的午餐了。但人一被关，就会悲观，就会开始在各个群里刷刷刷，看看大家pyq都在吐槽什么：这个喊着要放开，那个又说要失控……没人逼着我在宿舍如何如何，但就不能沉下心来做事情了。浮先生和躁先生，可能就是疯先生的两个保镖吧。</p><span id="more"></span><p>庆幸下午同学的复检结果出来是阴性，楼里解除了封控。我是被封怕了，当场就申请了离校，买了晚上的火车跑了。大家都被封控了，外面的人很少，紫荆的人也很少，寒风吹在脸上刮的挺疼，人也就低落了下来。和室友在紫四吃了最后一顿晚餐，涨价后的紫四一顿要吃30块钱，实际看看东西也不多。仔细想想，好像最近几个学期，每次离开清华都是灰头土脸，去也匆匆，大家把这叫"跑毒"。我倒觉得，这学期不算是跑毒，更像是跑封。</p><p>回到家，家里倒是没什么两样，小区该修的路还是要修，我种的几棵树也还是那样在长。无论有没有疫情，世界其实没有什么变化；无论我开心还是低落，该做的事情也还是要做完。老爸说我就像是爱因斯坦从德国逃出来。一时之间，我也不知道是该说我不是爱因斯坦还是我没有逃……</p><p>今天是12月1号，2022年寒冷的最后一个月开始了，不过大家都知道后面可能就是温暖的春天。这两天的报道和之前有了比较大的转变，开始宣传放开和疫情致病力的降低了。”动态清零“这个词在发布会上一直都没有出现，也许会和”食堂不许堂食“的制度一样，逐渐湮没在时间之中。但我被封在学校里的几个学期呢？因为疫情而被破坏的旅行计划和畅想呢？还有那些被损失的经济和人呢？</p><p>这些我都不知道，就像我也不知道现在放开的消息到底是传闻还是下一个陷阱。但时间一直以恒定的速度向前，再伟大的人也会逝去，好和不好的政策也终会有个结果。可能对于无知的我来说，跑远点才是最好的选择吧。</p><p>也许爱因斯坦早就懂了，他总是什么都清楚。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11-29总结</title>
      <link href="/f5edef7d.html"/>
      <url>/f5edef7d.html</url>
      
        <content type="html"><![CDATA[<p>最近北京疫情非常严重，疫情以外可能也比较危险，美国大使馆都开始通知公民小心、撤离了。我这两天分配了个工位，不过第二天就封楼了，第三天解封，不知道第四天会不会封回去。最近每天都得有几管阳性，封几栋楼，每天睡觉就像开盲盒，也像狼人杀，醒来就有惊喜，或者说惊吓。</p><p>学校昨天开了疫情防控专题会，虽然满口在说"研判"、”上级“，但我觉得学校防疫政策其实还好，比起北京其他高校……起码学校最近通知可以返乡了，但我没有返。只能说，家里疫情更严重：学校是盲盒，家里是潘多拉魔盒……不过话虽这么说，但还是有点想回去的，再几天吧。也不知道到时候还能不能回去了。</p><p>最近几天发生了很多事情，也出现了很多不同的声音，同学们的精神状态一天比一天疯狂了，每天都有新破防的同学。我现在也就是靠着为数不多的活动和社交吊着了，可能等街舞训练被叫停，我也就彻底疯狂了吧……仔细想想，也算一段特殊的经历吧。</p><p>以上。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11-24总结</title>
      <link href="/943a8ebd.html"/>
      <url>/943a8ebd.html</url>
      
        <content type="html"><![CDATA[<p>最近两天系里联系着我们做毕设的事情，真的感觉到要毕业了。</p><span id="more"></span><p>这两天写了上次推研分享的总结的问卷。写着写着发现自己其实还有挺多可说的上次都没讲，看来还是需要大家去启发我一下……</p><p>这两天看了两场世界杯，一场是沙特2:1阿根廷，一场是日本2:1德国。笑死，纯外行，是不是亚洲才是第一赛区呀</p><p>最后，记录周二健身情况：</p><ul><li>卧推 30kg 4组 + 25kg一组 + 20kg一组</li><li>夹胸 35kg 4组</li><li>三头推举 10kg 4组</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11-20总结</title>
      <link href="/6075aaae.html"/>
      <url>/6075aaae.html</url>
      
        <content type="html"><![CDATA[<p>最近疫情形势又严重了，封校已经三周了，今天又决定进一步严格管控，把"四大"(实习、科研、就医、上课)也改成审批制，可以说彻底关了。</p><span id="more"></span><p>可以想到，今天学校里爬爬流应该要重出江湖了。我和以往一样，还是期待着解封，期待着出去乱逛，期待着放开的一天。可惜，三年过去了，病毒进化出了第九版，防疫政策倒还是变化不大。</p><p>今天去给同学们做推研的分享，总体还算顺利。低年级的小朋友们倒也挺有特点，一脸凝重看着推研数据的大概就是零字班，不管不顾看着电脑的可能就是一字班，听分享听得津津有味的大概就是二字班。进入园子一年又一年，也不好说到底是变好了还是变坏了……我总觉得小朋友们，尤其是低年级小朋友们，还有好多的机会可以把握，但他们也许不会像我这么认为。</p><p>最后，今天又去健身了，特此记录：</p><ul><li>挺举20kg 4组</li><li>侧平举、前平举 7kg 4组</li><li>二头弯举8kg 4组</li><li>小臂特训10kg 12个 * 4组</li></ul><p>希望综体别关门，我就这点盼头了……</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11-15总结</title>
      <link href="/bcc282ea.html"/>
      <url>/bcc282ea.html</url>
      
        <content type="html"><![CDATA[<p>虽然标题是15日的总结，但其实是16日凌晨写的。主要是前一天睡了午觉，晚上理所应当地失眠了。既然失眠，那就稍微讲讲最近一段时间吧。</p><span id="more"></span><p>还记得去年11-15的时候是ACL rollingreview提交，我在截稿前最后2h和组长对了最后一版论文，转眼一年过去了。今天开组会的时候还在想，倒是一年不如一年了，去年还有finding，今年干脆啥都没了。</p><p>科研没有产出，做的事情倒也不少。最近一段时间几个题目、几组同学、几个服务器来回转，做了很多的实验，但好像还真没有前几年感觉那么的累和焦虑。仔细想想，大二大三，每天一堆课程作业，恐怕还真玩不来现在的状态；研究生，全职研究，写代码之余还能看论文，可能思路优势比我之前想的还大。</p><p>话又说回来，低年级同学科研进展慢也不只是水平的问题。华子175学分的培养方案，只能说功不可没。</p><p>今天组会上大家对我的选题挺感兴趣，后面又讨论了很多。但我现在总有点奇妙的恐惧感，觉得自己做的工作总会高开低走，后面卡在某些瓶颈上。</p><p>虽然害怕，但工作还得做下去。其实我最近看了很多论文，尤其还看了不少CV和ML理论的论文，越看越觉得大家的工作比前几年的更新颖了，也更经常有了所谓触类旁通的感觉。人总是从一个极端走向另一个极端。一年前我极度抵触看论文，必须得是沐浴更衣、糖水到位才能勉强看两页。现在好像完全反过来了，只想多看几篇论文不想写代码做作业，每天扫扫arXiv上的工作觉得都很新颖，但又没办法一篇篇看全……</p><p>也许我之前并不是怕看论文，现在也不是怕写代码，只是不想突破舒适圈罢了。说是舒适圈，可能更像是一种惯性，就像是给Adam加了一个momentum，不干之前一直干的事就觉得心里发慌。看来我还需锻炼呀。</p><p>今天接了一个给低年级同学分享保研经验的锅，现在又有点后悔了：我的经验好像也无法复制到零、一字班同学身上去，同学们来听我的分享好像也不能得到一些经验上的帮助，没准还更焦虑了。也许TLE比我更适合这个分享。</p><p>不过锅既然接了，那就得尽力干好。可能我更得从低年级同学的角度出发，想想同学们希望了解什么，而不是我想要分享什么。想到这些技术性的东西，好像马上就困起来了……果然，我终究还是不擅长这些吗……希望往届PPT能来点作用吧。</p><ul><li><p>最后，我在上周天11-12开完转正会，变成了正式党员。</p></li><li><p>以及，今天亚马逊宣布裁员10000人，距马斯克Twitter裁员1天，引以为戒。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-LANGUAGE MODELING VIA STOCHASTIC PROCESSES(1)</title>
      <link href="/a50a8741.html"/>
      <url>/a50a8741.html</url>
      
        <content type="html"><![CDATA[<p>今天来讲一篇我很喜欢的工作，ICLR2022上通过对比学习建模长文本生成过程的文章。</p><span id="more"></span><p><img src="../files/images/long_text_generation_bridge/authors.png"></p><p>作者团队来自斯坦福大学</p><h2 id="动机">动机</h2><p>这篇文章瞄准了现在长文本生领域的一个问题：coherence。正常Autoregressivemodel做长文本生成其实面临两个困境：</p><ul><li>由于teacher forcing的训练方式，会造成explosurebias，在inference时很难真的把长文本的所有位置的生成都做好，尤其是一步错步步错。</li><li>在长文本生成中，受限于显存，序列长度不能太长。模型只能有所取舍，比如每次只看到前面500个token是什么，这就从本质上上模型不能感知到前面生成了什么。</li></ul><p>作者提到对于前面一个问题</p><ul><li><p>目前有一些基于plan的方法先一些plan，最后再去做真正的生成。然而，这样就会带来的方法的效果其实非常依赖于一些人工的对plan的标注。</p></li><li><p>另一方面，作者也提到有一些通过对比学习等方式尝试对一个长文本的生成过程做一个更好的建模，但这类方法在结果上都不好。</p><blockquote><p>这类方法，还有一些关于长文本生成过程面临问题的本质探索的分析性文章，我后面也会阅读一些，可能为大家做一些分享。</p></blockquote></li></ul><p>在本篇工作中，作者着眼于第一个问题，提出了一种基于随机过程布朗桥的建模方法，通过新颖的对比学习loss设计巧妙利用了正例和负例，对长文本生成的atomicprocess做了更好的表示。最终得到了更好的长文本生成效果。</p><h2 id="方法">方法</h2><p>那么，作者是怎么看长文本生成过程的呢？</p><p><img src="../files/images/long_text_generation_bridge/bulang.png"></p><p>在这里作者想到了随机过程中的布朗桥。就是一个粒子从一个点到另一个点，中间可能会有一个偏移的过程，离起止点越远，这个偏移就越严重。</p><p>作者引入了一个归纳偏置：文本生成也遵循这样的道理。如果把每一句话的句表示都看做一个"点"，一段文本中的很多句话的句表示正是一个句表示空间的布朗桥。这样，最终生成的一整段话的语义应该有比较好的一致性和承接性。<span class="math display">\[p(z_t| z_0, z_T) = \mathcal{N} \left( (1-\frac{t}{T})z_0 +  \frac{t}{T}z_T, \frac{t(T-t)}{T}\right)\]</span> 其中这个t可以认为是中间隔了一些句子。</p><h3 id="布朗桥sentence-encoder">布朗桥sentence encoder</h3><p>上面的说法很有道理，但是你该怎么训练这个sentenceencoder呢？作者很巧妙，在这里就想到了contrastive learning的方法。</p><p>对比学习一定要有正例和负例，作者这里是怎么考虑正例和负例的呢？</p><ul><li>要判定是不是在布朗桥里，需要最少三个样本。</li><li>正样本明显可以直接从数据集里一段话采样三句</li><li>负样本呢？作者认为，起止点还要是正确的起止点，但负样本应该要求中间的句子来自于一条其他的数据。</li></ul><p>假如说我现在训练一个encoder $ f()<spanclass="math inline">\(,对于一个三元组，我们就能算出偏移量\)</span>$d(x_0,x_t,x_T; f_) = - ||f_(x_t) - ((1-)f_(x_0) + f_(x_T)) ||^2_2 $$对于正样本希望它绝对值小，负样本希望它绝对值大。</p><p>接下来通过采样真实的<spanclass="math inline">\(x_t\)</span>和负例<span class="math inline">\(\hat{x_t}\)</span>就能算出对比学习的cross entropy loss <spanclass="math display">\[\mathcal{L}_N = - \log \frac{e^{d(\text{正样本})}}{ \sume^{d(\text{所有样本})} }\]</span>通过继续不断地采样，最后encoder就会学到怎么把一个段中的句子编码成符合布朗桥过程的句表示。</p><p>具体实现上，作者使用了GPT2freeze，然后在最后一层的句子EOS表示的基础上叠加了4个可学习的MLP层进行预测</p><p>在这个层面上，作者也类比已有方法的一些setting，论证了布朗桥建模的正确性。我后面可能也会看看他引的其他论文。</p><h3 id="sentence-embedding-decoder">sentence embedding decoder</h3><p>当第一阶段的sentenceencoder训练结束以后，作者就会开始第二阶段的实际文本生成过程。在这里，其实有一个sentenceembedding和之前生成的内容作为指导信号。作者直接fine-tune了一下GPT2做这个decoder。</p><p><img src="../files/images/long_text_generation_bridge/plan.png"></p><p>作者在这里fine-tune时不仅要求模型可以做next-tokenpredicition，同时要求可以同时predict出来下一句话的sentence embedding</p><p>具体的decoder结构作者在正文中没有提，但在附录里说了一下：</p><ul><li><p>假如一个段落分成两个句子 [SOS] [s1] [s1] [s1] [ . ] [s2] [s2][s2]。其中[.]是句子开始符号，[SOS]是段落开始符号。s1是第一句，s2是第二句，分别有z1,z2两个句表示。接下来作者直接把句表示z贴在positionembedding上z1, z1, z1, z1, z2, z2, z2,z2，然后当正常的GPT2来做fine-tune</p></li><li><p>采样时就是你先有生成的z1和s1tokens，然后有个z2。你直接在后面加position embedding然后Autoregressivedecode到下一个[.] token就算完成了。</p></li></ul><p>对于一个长文本的解码过程，</p><ul><li>就是先随机采样一个Z_0和Z_T,都是从训练集的分布中采样的。</li><li>接下来，定义一个T，比如说数据集平均句子长度，然后按1000token算一下有多少句子。</li><li>接下里就是采样一个布朗桥z0,z1,...,zt。然后上面的方法一句一句来做，第i句就加第i句的positionembedding。如果后面还没有结束，就用z_T一直加直到结束。</li></ul><p>以上就是完整的方法、训练和测试流程，后面的实验和附加实验分析部分留到下次再写吧。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 文本生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-LANGUAGE MODELING VIA STOCHASTIC PROCESSES(2)</title>
      <link href="/a50a8741.html"/>
      <url>/a50a8741.html</url>
      
        <content type="html"><![CDATA[<p>继续讲一篇我布朗桥建模长文本生成的实验部分。之所以单拎出来，是因为他的实验部分的论文书写方式比较有意思，不像是mehtod类的论文，反倒像是分析性文章。我很喜欢这种写论文的思路，就单独拿出来说说。</p><span id="more"></span><p><img src="../files/images/long_text_generation_bridge/authors.png"></p><p>作者团队来自斯坦福大学</p><h2 id="实验">实验</h2><p>在这一部分，作者瞄准动机，想要设计实验回答下面四个问题：</p><ul><li>可以很好的encode句子级别的语义吗？</li><li>可以很好的做句子级别的生成吗？</li><li>可以很好的编码全局信息吗？</li><li>可以很好的做全局的长文本生成吗？</li></ul><p>作者在主模型的基础上做了几个消融模型：</p><ul><li>Implicit Dynamics(ID)：专门用一个autoregressive模型来预测下一个sentenceembedding，给定前面的embedding</li><li>Brownianmotion(BM)：不用布朗桥的三个句子训练，而是直接用两个句子的布朗运动用<span class="math inline">\(z_t | z_s \sim \mathcal{N}(z_s,t-s)\)</span>作为随机过程的编码oracle</li><li>VAE：用VAE做句子编码，这里不考虑句子间的关系，只是对单独句子要求做0-1标准分布的对齐loss</li></ul><h3id="可以很好的encode句子级别的语义吗">可以很好的encode句子级别的语义吗？</h3><p>这一部分就是在衡量sentenceencoder的能力。作者在这里用了一个实验性的定义：</p><ul><li>如果我们能从embedding中恢复句子的前后关系，说明句表示的学习有很好的承接关系</li></ul><p>作者因此用encoder跑出来的句表示数据集训一个MLPclassifier区分句子的先后关系，其实有点像对抗……这个实验最差的结果应该随机给，正确率50</p><p><img src="../files/images/long_text_generation_bridge/classifier.png"></p><p>可以发现，正常的GPT2、VAE、BM基本都是random来搞。而TC最好地保持了这种句子顺序的关系，说明这种由3个句子形成的对的建模效果很好。</p><blockquote><p>尤其是BM这种单向布朗运动的结果是random。这也许说明这种建模其实不能让模型学到先后关系。</p></blockquote><p>同时，ID方法其实在conversation数据集上的效果很好，说明Autoregressive建模其实也还不错。</p><p>最后，作者有一个小细节带过了，但我们可以想一想：采样的句子举例k=1时，所有的模型最后都是瞎猜。这是不是说明相邻句子的编码关系其实没有那么好？但其实人做分类时，相邻句子是可以做的很好的，这一点上，我们stillhave a long way to go。</p><h3id="可以很好的做句子级别的生成吗">可以很好的做句子级别的生成吗？</h3><p>这一部分，作者使用了in-filling作为代理任务，因为需要同时考虑前面和后面的内容才能保证生成的内容一致性好。在这一部分，作者对比了一个非常强的baselineILM，就是做in-filling的SOTA。注意，TC并不是一个专门做in-filling的模型</p><p><img src="../files/images/long_text_generation_bridge/in_filling.png"></p><p>作者在BLEU上甚至战胜SOTA。这强烈证明了布朗桥建模确实非常有利于建立一个连续地语义的变化过程。</p><h3 id="可以很好的编码全局信息吗">可以很好的编码全局信息吗？</h3><p>在这一部分，作者思考了另外一个setting。通过生成一个wikipage和原始page的长度之间的差距来衡量全局建模能力。作者在这里对比了GPT2。</p><ul><li>为了加强GPT2的能力，作者还给了GPT2某些oracle的能力，提前告诉GPT2对应哪些段落，然后段落名的embedding，再让GPT2对应生成段落。</li></ul><p><img src="../files/images/long_text_generation_bridge/mismatch.png"></p><p>在这种情况下，TC仍然战胜了所有竞争者。说明TC方法确实很好的把握了全局的语义信息，生成出了正确的长度。作者在这里也提到别的模型的错误主要来源于对于单个段的长度有错误的判断。</p><blockquote><p>然而，这里可能有个bias的问题，就是训练的时候我们其实采样过了wiki的语料，这说明TC其实对于wiki的段落长度有个大概的感知，下游任务段落长度判断准确是可以理解的。因此这里我觉得可能涉及有循环论证的问题。</p></blockquote><h3id="可以很好的做全局的长文本生成吗">可以很好的做全局的长文本生成吗？</h3><p>最后，作者目标指向了最终的全局长文本生成问题。叫做forced long textgeneration。对于传统Autoregressive模型这很难，因为一旦生成EOStoken你即使让它接着生成，后面也会开始说一些怪话，表现会很差。</p><p><img src="../files/images/long_text_generation_bridge/forced_generation.png"></p><p>但矛盾就在于，模型正常说几十个字就会出现EOS，这不是我们能决定的。在这里作者对于传统GPT2就直接forcedlonggeneration。对于所有plan系的模型，都是按照之前讲的那一套方法进行进行生成</p><p><img src="../files/images/long_text_generation_bridge/long_generation_result.png"></p><p>应该说，不管是在哪个机械评分还是human评分上，TC方法都赢了。说明这种建模方式对于长文本段语义保持的有效性。</p><h2 id="思考">思考</h2><ul><li>作者这个实验部分挺有意思。用回答问题的方式设计实验，而不是先设计实验再做分析。不知道作者在完成工作的时候用的是什么顺序，但读起来就会让大家觉得故事线非常完整</li><li>同时作者的对比实验和各种附加实验以及人工评测都做的很详细。这样的完成度才像是ICLR的体量</li><li>有一个有意思的点是作者用的句表示维度其实只有最多32维，效果就已经不错了，并且也可以明确分出来句子表示的先后顺序。这也许说明句子中实际含有的语义可能没有我们想象的那么难以表达。</li><li>布朗桥的思路非常新，引入的这个归纳偏置值得考究。但是文本生成真的就是想布朗桥一样吗？比如所谓的起承转合，或者欲扬先抑的语义在这套体系下就不能表示。</li><li>我们能不能想到更一般性的表示法，有更强的表征能力，然后模型甚至去重新发现所谓的归纳偏置。不过这样可能有点回到了作者之前提到的formerwork的感觉。</li><li>最后，就是感觉这种摆一个strong specific SOTAbaseline然后用一个一般性模型打败它，这种实验好酷啊……</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 文本生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Do Vision Transformers See Like Convolutional Neural Networks?</title>
      <link href="/158670f0.html"/>
      <url>/158670f0.html</url>
      
        <content type="html"><![CDATA[<p>今天来和大家读一篇非常有趣的分析性文章，卷积网络和Vit学的隐层表示有什么区别？模型眼中的世界是一样的吗？</p><span id="more"></span><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/author.png"></p><p>这是一篇NeurIPS 21的文章，作者团队来自GoogleBrain。这个作者好像是LSTM的作者？</p><h2 id="动机">动机</h2><p>文章主要探索一件事情：CV领域的ViT，其实没有引入卷积网络的归纳偏置：</p><ul><li>ViT没有对临近的图像区域做专门的处理，conv其实是按局部去滑动。</li></ul><p>在这种情况下ViT的效果依然很好，那么他们两种网络学到的图像表示有没有什么联系呢？ViT有没有重新学到conv给定的归纳偏置呢？</p><p>这个问题是很好的问题，但是所谓的”图像表示之间的关系、相似度“该怎么衡量呢？我们可以实验性的定义”linearprobe的结果“代表图像表示的好坏，但相似度又该怎么衡量？毕竟两个模型、甚至一个模型的两次初始化学到的表示空间可能就完全不同，图像表示只有针对不同图像的差值、夹角、模长等可能有意义。</p><blockquote><p>linearprobe:锁定整个模型，只是把图像表示拿过来后面跟一个linear层，然后做下游任务，比如分类。这个调节的参数很少，但效果基本远不如fullsize fine-tune</p></blockquote><h2 id="实验">实验</h2><p>作者在这里引入了一种非常有趣的衡量方法：CKA, centered kernelalignment</p><p>这个方法是这样，对于两个模型，先扔进去m个样本得到m个表示： <spanclass="math display">\[X \in \mathbb{R}^{m\times p_1} \quad Y \in \mathbb{R}^{m\times p_2}\]</span> 接下来算Gram矩阵 <span class="math display">\[K = XX^T,L = YY^T\]</span> 接下来计算<strong>Hilbert-Schmidt independencecriterion</strong> ,构建出来一个中心矩阵 <span class="math display">\[H = I_m - \frac{1}{m} \vec{1}\vec{1}^T\]</span> 用中心矩阵来做中心化 <span class="math display">\[K&#39; = HKH,L&#39; = HLH\]</span></p><p><span class="math display">\[HSIC(K,L) = \frac{vec(K&#39;) ·vec(L&#39;)}{(m-1)^2}\]</span></p><p>有了HSIC算子以后，对两个矩阵用HSIC算子做cosine就行了 <spanclass="math display">\[CKA(K,L) = \frac{HSIC(K,L)}{\sqrt{HSIC(K,K) · HSIC(L,L)}}\]</span> 这个变换</p><ul><li>对于两个矩阵分开的正交变换具有不变性</li><li>对于矩阵表示的模长好像也是无所谓的</li><li>我理解CKA值越大，好像代表两个矩阵通过的变换距离的模比较小？</li></ul><p>作者有了CKA方法以后，都是用一个minibatch来模拟真实的CKA结果</p><h2 id="两个模型表示的区别">两个模型表示的区别</h2><p>接下来作者先看了模型自己层和自己层的表示的CKA对比</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/self_CKA.png"></p><p>就是说单拿出某一层的表示和另一层的表示，当做两组表示计算CKA。这个值高(亮)就代表两个层出来的表示具有更强的相关性</p><p>可以发现：</p><ul><li>对于ViT，图像基本是一样的，也就是每一层学到的表示都很像，随着层数距离变大，表示慢慢均匀地变不像，但总体相似度还是挺高的</li><li>ResNet有明显的两个块，底层和上层学的表示差距很大</li></ul><p>接下来作者做跨模型对比，把两个模型不同层的表示做CKA，得到下面的结果</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/cross_CKA.png"></p><ul><li>ViT前1/4的层表示和ResNet前1/2的表示接近</li><li>ViT中1/3的表示和ResNet后1/2表示接近</li><li>ViT后1/3的表示和ResNet任何地方都不同。作者认为这些层可能主要在优化CLS的表示，毕竟ViT是有监督的训练</li></ul><p>总结一下，作者得到了以下结论：</p><ul><li>ViT下层和ResNet下层表示有较大区别</li><li>ViT表示在层间有很强的继承关系</li><li>ViT的上层表示和ResNet有很大区别</li></ul><h2 id="全局与局部信息">全局与局部信息</h2><p>接下来作者想要探索ViT是不是重新发现了conv的局部性归纳偏置，具体作者这么算的：</p><p>transformer layer里面要先算key *query，作者直接把5000条数据里的query矩阵乘积算出来，就是模型希望去询问的东西，由于selfattention，其实最后就是这些query互相乘。然后作者算两个query的像素距离。</p><p>就如果两个图片patch离的近，他们的query像素距离也近，说明模型学到了local信息</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/local_global.png"></p><p>作者发现，对于低层，确实距离近的patch,query也相似。但对于高层的block，这个相似性就消失了。对比之下，conv在底层由于实现，只能关注更局部的feature.</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/small_data.png"></p><p>接下来，作者做了更有趣的实验，作者只在imageNet上炼了一个ViT(小的pretraindataset)，发现这种重新发现的局部性原理竟然消失了，说明ViT其实需要从非常多的数据中总结出这种规律。</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/cross_local.png"></p><p>ViT这种局部性学到的feature和conv的一样吗？接下来，作者试了局部性最好的head和ResNet里面做CKA，发现结论是肯定的。</p><h2 id="skip-connection">skip connection</h2><p>接下来作者衡量了skip connection对ViT这种学习的影响，定义了一个量<span class="math display">\[\frac{||z_i||}{||f(z_i)||}\]</span> 代表从skipconnection出来的表示的模长除以long-branch算子出来的表示的模长。假如这个值很大，说明long-branch算子的影响很小，结果以skip-connection继承之前的东西为主</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/div.png"></p><p>作者发现CLS token(token 0)和其他token不一样:</p><ul><li>CLS在前面基本都是继承，直到后面才开始使用long-branch里面的东西</li><li>正常token都比较接近，前面要看互相的关系，到后面以继承为主</li></ul><p>右边的图对比了ResNet，可以发现conv就比较平均，每一层都在看互相的量</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/without_skip_connection.png"></p><p>接下来，作者尝试在ViT某一层去掉skipconnection，发现ViT马上出现了这种明显的分块。也就是说，ViT的这种不同层表示之间的CKA相关性，基本就是由skip-connection带来的。</p><h2 id="空间位置信息">空间位置信息</h2><p>作者继续探索了ViT在高层还能不能保持图像之间的空间关系，毕竟上层基本都是看全局信息</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/spatial.png"></p><p>对比之下可以看出，ViT在高层对于空间关系的保持性比ResNet好很多。作者提出一个猜测：</p><ul><li>ResNet训练任务是用averagepooling，而ViT是用CLS分类头。显然前者需要高层的表示比较接近</li></ul><p>作者做了个对比实验，训练了用average pooling的ViT</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/average_pooling_vit.png"></p><p>一下子，ViT也不行了，这佐证上述猜测。</p><p>接下来作者又探索了使用别的token能不能做linear probe。</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/token_num.png"></p><p>左图作者试着用不同比例的token一起做linearprobe，右边是单独ViT的对比</p><p>可以看出来，用CLS做训练的ViT下游任务一但没有CLS token，就寄了</p><p>最后，作者做了一个数据规模实验：</p><p>首先试了在不同训练数据规模下，用不同层的表示做实验。发现即使只有3%的数据，在低层的时候学到的表示也差不多了。只是在高层的表示需要更多的数据来学习</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/scale_with_layer.png"></p><p>后面，作者常规地试了试数据规模和迁移能力的实验，得到了多就是好的常规结论</p><p><img src="../files/images/do_vision_transformers_see_like_convolutional_neural_networks/scale.png"></p><h2 id="思考">思考</h2><ul><li>这篇文章是一个经典的分析性文章。作者的实验设置值得学习，尤其是很多对比实验的思路，selfcompare，cross compare的实验设计</li><li>有没有人试过MAE和普通ViT的对比呢？感觉这种自回归的方式很可能更接近ResNet？</li><li>进一步，这种思路是不是也可以对比NLP这边encoderbased和encoder-decoder based模型得到的表示</li><li>在新的模型结构出来以后，比起直接在下游任务用起来，这种对模型表示的探索也许更值得学习</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>S12世界赛总结(2)</title>
      <link href="/6243510f.html"/>
      <url>/6243510f.html</url>
      
        <content type="html"><![CDATA[<p>上次聊完了入围赛、小组赛，今天再来聊聊淘汰赛。</p><p><img  src="../files/images/s12总结/keria.png"></p><span id="more"></span><p>上次讲完四个小组的第一第二分别是：</p><ul><li>A组: T1, EDG</li><li>B组: JDG, DK</li><li>C组: DRX, RGE</li><li>D组: GEN.G, RNG</li></ul><p>淘汰赛是单败淘汰制，第一轮是小组第一打小组第二，四个小组第一优先选边。这里额外说一下，其实优先选边很重要，选到蓝色方可以第一手选。对于很多版本，红色方的ban位很确定，蓝色方能选到优势阵容。因此往往蓝色方胜率更高。像s11决赛，s12MSI决赛，都是在bo5中有5把蓝色方获胜。LPL夏季赛蓝色方胜率更是高达60%。</p><p>抽签结果：</p><p><img  src="../files/images/s12总结/round1.png"></p><h2 id="四强赛">四强赛</h2><p>第一轮的几场BO5里：</p><ul><li>JDG打RGE是纯纯碾压局3：0获胜。RGE还是那几板斧，真拿不出别的东西了，只能说，可惜</li><li>T1打RNG是第一轮里最有看点的比赛，在美国收票网站更是炒到了原价三倍的价格。但真的打起来发现RNG局部多打少捡尸的体系被T1破掉了。T1先是全图流，Faker聪明瑞兹几波单带和偷大龙的节奏让RNG晃花了眼；再是奶妈卡莉斯塔，导致RNG非常被动。Ming和Gala的下路组合又掏不出别的东西，最后被3:0拿下了。</li><li>GEN.G打DK。非常精彩，鏖战5局。Canyon在野区几乎每一把都碾压了Peanut，甚至掏出了凯隐打野，可以说是尽力中的尽力。不过Peanut也非常灵性，转变很快，不让凯隐变身，叫队友帮忙开野保野。决胜局Showmaker拿出绝活辛德拉，配合Canyon凯隐绕后击飞四个一波0换5。可惜最终还是败下阵来。Dokdem，队伍永远滴送子，6神装厄斐琉斯带闪现狂风直接被秒没打出伤害……这场比赛，是最让人意难平的比赛。canyon，可惜了。</li><li>DRX打EDG，也很精彩。DRX让二追三。第二把Deft选出ez爆c，点基地差最后一下，平a都出手了，EDG小水晶复活了，最终被EDG反一波。后三把Zaka爆种，绝活阿卡丽和塞拉斯把把有作用。决胜局甚至4次单杀Scout。向我们展现出了韧性。</li></ul><p>最终JDG，T1，GEN.G, DRX进入四强。JDG拿到了去年EDG的剧本。</p><h2 id="半决赛">半决赛</h2><ul><li>JDG打T1，T1摇骰子拿到优先选边权。JDG第一把前期劣势，但先锋团一波神奇0换4直接打赢，最终赢下第一局。后面T1连赢三把。gumayusi稳定发挥，把把爆hope线，后三把一共死了两次。真的是，hope在干嘛。JDG的上中野其实没输。值得一提的是，UZI坐镇2路解说，直接带入hope角色，几次锐评hope站位……真的是，LPL的下路只能拿出厄斐琉斯一个英雄了吗……</li><li>DRX打GEN.G，GEN.G拿到优先选边权，赛前没人看好DRX，赔率一度来到2.7。第一把GEN.G直接碾压，全程绝望运营，展现一号种子实力。第二把DRX绝地反击，Pyosik拿出绝活千珏，怪翻盘，GEN.G状态直接不对了。从第三把开始，Ruler表现成迷，几次没吃到先锋的钱。同时Chovy重回刷子，直接隐身。反倒是Zeka，哪里都有它。最后一把更是放出了Zeka塞拉斯，不给你搞有的没的直接18层杀人书；同时BeryLcounter选出布隆辅助对位Lehands泰坦作用拉满。DRx3:1拿下。说实话，GEN.G状态很怪，决策很犹豫就像是吵架了。Chovy更是全程隐身，作为顶级中单，这太不应该了。</li></ul><h2 id="决赛">决赛</h2><p>决赛11-6北京时间早8点，在旧金山打响，T1拿到优先选边权。赛前同样没人看好DRX。LPL和LCK所有解说赛前预测只有王多多认为DRX赢，外网DRX赔率来到2.3，T11.7。</p><p>第一把T1拿到GEN.G剧本，稳定发力，运营碾压局，DRX全程没还手之力。Gumayusi维鲁斯甚至一箭抢到一条小龙……</p><p>第二把换成DRX蓝色方，T1还是前期优势，但是先锋团打输变成DRX经济领先。后面Oner抢到一条大龙又追回了经济。最终龙魂决战T1打输，DRX险胜。</p><p>第三把T1蓝色方，Gumayusi又选到维鲁斯稳定发力，助力团队赢下。</p><p>第四把Kingen选到剑魔并且ban了zeus的永恩，zeus拿出剑姬应对，可惜被抓烂了。Kingen天神下凡直接拿下MVP。</p><p>第五把Kingen又拿到剑魔并且ban了zeus的永恩，zeus拿出葛温应对。同时Pyosik选出人马打野，BeryLcounter选出巴德辅助。Kingen再次爆种直接单杀，DRX经济领先。但是Gumayusi维鲁斯竟然一箭抢了大龙追回了经济。最后运营拉扯龙魂团Pyosik拼惩成功，T1直接选择偷家，无奈DRX回城很快。</p><p>最终DRX 3:2拿下冠军，Kingen靠两把巨大作用剑魔拿下FMVP</p><p>DRX这支队伍，LCK夏季赛第6，季后赛一轮游。冒泡赛靠Deft爆种险胜LSB才进了世界赛。在世界赛上从资格赛开始打，一路上Deft，Zeka，Pyosik，Kingen先后爆种，一黑到底，可以说是虚假的不破不立与真正的不破不立……</p><p><img  src="../files/images/s12总结/keria.png"></p><p>Keria哭的巨伤心，只能说，T1下路赢了5把，但输掉了bo5。总结来看，其实可能是bp的问题，太过侧重于下路，导致上路Zeus拿不到英雄压力太大，有点类似于wayward。不过Gumayusi抢了两条大龙一条小龙真的是尽力了。感觉和去年决赛一样，都是怪局……T1几个小将，加油吧。大飞老师，今年可能是夺冠希望最大的一年，第六次进决赛，第三次拿亚军，这次换成队友泣不成声了</p><p>值得一提的是，BeryL赛后选择艾希做冠军皮肤。他是三连决赛，并且是第一个换队拿s赛冠军的选手。我的BeryL，难道才是隐藏boss？冥想训练法，才是版本答案？</p><p>总之，S12已经落幕，大魔王倒在了最后一关，黑马DRX最终笑到了最后。故事仍在最后，转会期刚刚开始：</p><p>TES能否凑齐三叉戟？Deft要不要退役？Tian在干嘛？Ning的试训怎么样了？Faker留队吗？Viper真要去助力Canyon夺冠？</p><p>我们，敬请期待！</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>11-8总结</title>
      <link href="/dd1c3b6d.html"/>
      <url>/dd1c3b6d.html</url>
      
        <content type="html"><![CDATA[<p>反思一下，已经快半个月没写总结了。</p><p>上一周是期中周，说是期中周，其实我并没有期中考试。预期中的课业压力没有如期而至，主要是因为我会出手，连续退了好几门课。虽然是期中，但又回到了专业课随便水水，别的课根本不听的日子。</p><p>上周做了解刨小白鼠的实验，我因为害怕没有做，帮TLE打了打下手。虽然过了一周，但还是感觉自己好残忍，小鼠好无辜好可怜。后面没有解刨实验了，都是细胞实验，应该会好一些。</p><span id="more"></span><p>最近科研任务逐渐增多，有了研究生的感觉。不过上一周服务器坏了，科研进度暂停。只能说，深得我心。最近又开新坑，读了好多篇新的论文，真的感觉现在读论文速度快了很多，逐渐有了读中文那种一目十行的感觉。神奇的是，英语水平好像并没有提高呀。</p><p>另外，最近几天学校上新了小闸蟹，15元一只，挺好吃，除了小以外没什么其他的缺点。前两天专门去桃李吃了一次，发了个朋友圈，文案竟然被选入了《清华小五爷园》的推送。反思了一下，不知道是我的问题还是桃李的问题，除了这次，我竟快一年没去过桃李了……去食堂也基本是去清芬、丁香、紫荆、荷园。后面有时间的话，可以写一篇食堂的评测，其实我还是吃了食堂不少好东西的。</p><p>嗯，和胎死腹中的火锅评测，两个一起咕咕咕……</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>S12世界赛总结(1)</title>
      <link href="/1bbf726e.html"/>
      <url>/1bbf726e.html</url>
      
        <content type="html"><![CDATA[<p>S12决赛在11-6打完结束。正好我几乎看了s12的所有比赛，我也就放在一起，来总结一下s12发生的情况吧。</p><p><img src="../files/images/s12总结/drx.webp"></p><span id="more"></span><h2 id="入围赛">入围赛</h2><p>入围赛12个队伍分成两个小组，一个小组六个队伍，出线4个队伍。小组内单循环，小组第一直接出线，然后小组2,3分别和另外小组的3，2打bo5，赢的两个出线。</p><p>在入围赛中，值得关注的就是RNG和DRX，他们被分到了一个小组。正在第一轮的bo1碰撞中，pyosik掏出了一个大树打野，几波团战和视野都碾压了rng，最终赢得比赛。</p><p>最后drx全胜小组第一出线，rng小组第二和日本队dfm打了一个bo5。甚至还输了一把(日本队第一次赢lpl小局)，最后以3:1战胜DFM出线。</p><h2 id="小组赛">小组赛</h2><p>加上入围赛进来的4个队，s12一共是16个队进行比赛，lpl和lck都有4个队。</p><p>小组赛分4各小组，打双循环，小组第一第二晋级淘汰赛。首先就是小组抽签。</p><p><img src="../files/images/s12总结/小组分组.png"></p><p>下面我一个小组一个小组的讲：</p><h3 id="第一组a">第一组A</h3><p>第一组其实传统强队T1和EDG分到了一起，梦回s7。然后在赛前大家其实不太看好T1，因为t1在lck夏季赛决赛被gen.g3:0碾碎。当然EDG在lpl夏季赛成绩也一般，冒泡赛上3:2赢了RNG拿到的3号种子。</p><p>不过在小组赛上，gumayusi一转颓势，下路出手双杀了EDG，两把都是碾压获胜。值得一提的是，第一轮FNC战胜了T1，不过在第二轮FNC惨遭edg和t1暴打。最终T1和edg分别以小组第一第二出线。</p><p>第二轮的比赛中，T1下路选出了奶妈卡莉斯塔阴间组合，互相保护，天神下凡辅核一奶四。可以说是非常恶心。</p><h3 id="第二组b">第二组B</h3><p>B组在赛前被认为是看点最多的组。大家不清楚G2的真正实力，觉得DK和G2可能要争小组第二了。不过真的打起来以后，发现G2可以说是一碰就碎，谁都没打赢。</p><p>JDG打DK可以说是强队碰撞。第一轮canyon掏出大树打野对着Kanavi的野区乒乓一顿反野，可以说是意识拉满。不过最后在yagao的沙皇神奇开团下逆转游戏结局。第二轮Nuguri爆c赢下一把，但是在加赛中Nuguri直接犯病，拿出刀妹counter位对位369剑魔被爆杀。</p><p>DK用小组第二为我们带来两个教训：</p><ul><li>yagao的沙皇不能放</li><li>别拿刀妹打剑魔</li></ul><p>JDG最终以小组第一出线</p><h3 id="第三组c">第三组C</h3><p>第三组就是搞子来了。赛前大家都觉得第三组最没看头，TES稳稳拿下。结果在第一轮RGE会出手，上来三板斧，3:0横扫c组，欧洲观众直接站起来了，都觉得RGE可以夺冠。</p><p>TES不负众望，分别在对阵RGE和DRX的比赛中落败。值得一提的是，JackeyLove选出德莱文装杯被杀穿。第一轮结束，TES位居小组第三。</p><p>第二轮RGE现出原型，原来还是原来的三板斧：中单战士，上单肉墩，打野节奏怪。被研究清楚以后RGE直接被DRX和TES爆杀。</p><p>第二轮，比赛从凌晨三点TES对阵越南队GAM开始，大家都订好了5点和8点的闹钟看TES打DRX和RGE。最终在3点TES输了GAM，确定无缘淘汰赛。值得一提的是，TES哥在对面水晶前没有a基地而是打人，最终被GAM水晶丝血翻盘。</p><p>粉丝们5点钟起来看TES哥输越南队表明态度。无所谓，knight会出手，TES接连大优势碾压DRX和RGE表明实力，嘻嘻哈哈游回家。wayward世界赛6维图如下：</p><p><img src="../files/images/s12总结/wayward.jpeg"></p><p>虽然大家都在喷wayward，但我倒是不这么认为。主要是TES的战术体系是保下路牺牲上路，小Tian刷野抓人也偏重于下路。wayward本身被放弃：bp拿不到英雄，游戏见不到打野，后期把线让给队友，当然发挥不出来了。关键在于，下路拿到优势了吗？这一点可能才是TES游回家的关键。</p><p>当然，在c组的小组赛中，虽然输了两小局，但deft和BeryL的精彩表现，尤其是大头艾希的下路组合。向我们展现出了LCK下路的战术储备和英雄池深度。</p><h3 id="第四组d">第四组D</h3><p>D组的看点主要是GEN.G和RNG。GEN.G在赛前被认为是夺冠大热门。不管是分均经济差还是人头差，gen.g都领跑全场。从以往几年经验来看，这个图高的最后都夺冠了。</p><p><img src="../files/images/s12总结/summer数据.jpeg"></p><p>然而，第一轮GEN.G就被RNG打败了。RNG选出赶尸中野冰女破败王直接找小规模多打少，邦邦两拳GEN.G还真没挡住。当然，也有人说是第一轮lehands直接搞了个怪的炼金塞纳组合，连刀都补不到。</p><p>不过，第二轮GEN.G调整状态，双杀RNG最终还是小组第一出线了。不过看起来，没有泽丽的ruler确实有点没内味了。最终RNG小组第二出线。</p><p>小组赛完了就是淘汰赛，淘汰赛下次接着聊。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-24总结</title>
      <link href="/7bf8e583.html"/>
      <url>/7bf8e583.html</url>
      
        <content type="html"><![CDATA[<p>转眼又是1周过去了，这周有个新变化：</p><span id="more"></span><ul><li>我没去跳周二的Hip-hop了。主要是人太多了，根本看不到镜子，感觉这样子还是挺难进步的，还不如回来科研，以后可能还是以周四的popping/locking为主了。其实上周末还去了一次popping的特训，虽然人比较少，但收获倒是挺大，有队长基本一对一的指导</li></ul><p>另外，计划这两周去看看：</p><ul><li>国家博物馆 意大利之源——古罗马文明展</li><li>在小动物们冬眠之前，去一次北京野生动物园</li></ul><p>另外，今天又去健身了，本来想练胸的，结果由于没找到卧推的器械的位置，练的肩和三头，重量如下：</p><ul><li>挺举 15kg 10个 * 4组</li><li>侧平举 5kg 12个 * 4组(想用6没找到器械，7感觉有点沉动作变形)</li><li>前平举 7kg 8个 * 4组</li><li>三头 10kg * 4组(赶紧涨到12kg，这样可以用健身房的宽的哑铃，姿势更舒服，更不容易借力)</li></ul><p>最后，今天吃了玉树的蛋黄酥，还挺好吃的：最外层的酥皮的香，中间层的豆沙的甜，里面的咸蛋黄的咸鲜分离感很好，吃起来就是味道很丰富。回口稍微有一点干，但更多的还是甜。主要问题就是9块钱有点太贵了，鲍师傅的蛋黄酥才卖8.5元……</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-19总结</title>
      <link href="/94a083a0.html"/>
      <url>/94a083a0.html</url>
      
        <content type="html"><![CDATA[<p>今天去健身了，感觉力量比前几周增长了很多，特此记录:</p><ul><li>平板卧推 25kg 两组 + 20kg 两组</li><li>斜上卧推 20kg 两组</li><li>器械夹胸 30kg 两组 + 25kg两组</li><li>三头 9kg 4组</li><li>面拉 12.5kg 一组 + 10kg 3组</li></ul><p>我的目标是到期末的时候卧推可以到40kg4组，然后可以增重到60kg。但愿如此</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Exploring Universal Intrinsic Task Subspace via Prompt Tuning</title>
      <link href="/3e784f99.html"/>
      <url>/3e784f99.html</url>
      
        <content type="html"><![CDATA[<p>这一篇文章可以看做对IntrinsicDimension的又一个推广，用了比LoRA更强的假设。通过在多任务间找到统一的子空间表示，证明了任务间的相关性，提升了prompttuning的稳定性和效果</p><span id="more"></span><p><img src="../files/images/unifed_subspace_pt/author.png" ></p><p>作者是THUNLP实验室</p><h2 id="介绍">介绍</h2><p>这篇文章主要是探讨这个问题：</p><blockquote><p>已有研究发现预训练可以降低下游任务的IntrinsicDimension，也就是模型可以压缩下游任务的表征</p><p>那么不同的下游任务需要的空间是否是关联或者统一的呢？</p></blockquote><p><img src="../files/images/unifed_subspace_pt/introduction.png" ></p><p>由此作者进行了一系列的探索，发现竟然可以只用250个参数统一子空间上就能在100个可见任务和250个不可见任务上达到FT的97%和83%的效果</p><p>全文的实验设计很复杂，对比也很多，我感觉文章的组织形式和结论更像是一种探索、分析性的文章</p><h2 id="方法">方法</h2><p><img src="../files/images/unifed_subspace_pt/arch.png" ></p><p>具体怎么实现统一的子空间呢？作者分为了两个步骤：</p><ul><li>Multi-task Subspace Finding (MSF)</li><li>Intrinsic Subspace Tuning (IST)</li></ul><h3 id="multi-task-subspace-finding-msf">Multi-task Subspace Finding(MSF)</h3><p>这个过程实际上就是在寻找任务统一的子空间，首先要确定“任务”是什么意思：</p><blockquote><p>对于一个任务而言，就是一个<spanclass="math inline">\(&lt;X,Y&gt;\)</span>的集合，其中X是输入语句，Y是一个类别(分类任务)或者一些句子(生成任务)。</p></blockquote><p>所谓的PT就是在任务输入X前面加上一个可学习的prompt，实际模型是这样的:<span class="math display">\[\mathcal{L}_{PT} = -\frac{1}{|\mathcal{Y}|} \prod_{j=1}^{|\mathcal{Y}|}P(y_j | p_1,p_2,...,p_n,x_1,x_2,...,x_{|\mathcal{X}|}, y_1,...,y_{j-1})\]</span> 保持只有<spanclass="math inline">\(p_1,...,p_n\)</span>是可学习的，别的参数都锁定，然后参数p是从某个分布采样来进行初始化的</p><p>作者首先找到了120个已有任务，把他们随机分成了100个可见任务和20个不可见任务</p><p>由此可见，PT正常的搜索空间是<span class="math inline">\(D = n\timesd(\text{modeldimension})\)</span>的，对于不同的任务，模型可以加上不同的prompt来进行表示。对于100个任务，一共有100组prompt（在每个任务的训练集上训练得来的）</p><p>那么怎么找到统一的表示空间呢？</p><p>可以先把D维降到d维，再来一个反向变换变换回D维。如果锁定矩阵的话，中间的d维就是所谓的统一表示空间，矩阵就是空间的变换矩阵。在具体执行中，使用重建loss进行学习:<span class="math display">\[\begin{aligned}P_i^* &amp; = \text{Proj}_b(\text{Proj}(P_i)) \\\mathcal{L}^i_{AE} &amp; = || P_i^* - P_i ||^2_2 \\\mathcal{L}^{MSF}_{\theta_{proj}} &amp; =\frac{1}{|\mathcal{T}_{train}|} \sum_{i=1}^{|\mathcal{T}_{train}|}(\mathcal{L}_{PT}(P_i^*) + \alpha \mathcal{L}^i_{AE})\end{aligned}\]</span>通过这公式，可以学习到前后的变换模型的参数。实验中，变换模型就是简单的MLP</p><h3 id="intrinsic-subspace-tuning-ist">Intrinsic Subspace Tuning(IST)</h3><p>一旦学习好变换矩阵<spanclass="math inline">\(Proj,Proj_b\)</span>以后，对于新的任务，我们就可以只保留<spanclass="math inline">\(Proj_b\)</span>矩阵，然后从统一的子空间采样prompt<span class="math display">\[\mathcal{L}^{IST}_{\theta_{d}} = \mathcal{L}_{PT} (\text{Proj}_b(V_i))\]</span> 其中，只有d维变量<span class="math inline">\(V_i \in\mathbb{R}^d\)</span>是可学习的</p><h2 id="实验">实验</h2><p>这一部分作者做了各种实验来佐证文章的观点</p><p>作者找到了120个任务，随机分成100个可见任务和20个不可见任务；也可以按是否生成任务来分</p><blockquote><p>可见任务是指训练proj时用到的prompt的任务</p></blockquote><p><img src="../files/images/unifed_subspace_pt/split.png" ></p><p>首先所有实验是用BART执行，然后所有任务都报告了相比于直接prompttuning/fine tuning的结果的相对值，分类任务用F1，生成任务用BLEU。</p><p>所有任务都是few-shot场景下的：</p><ul><li>生成任务用32条数据</li><li>分类任务用16条数据</li></ul><p>其中模型用到的指标的解释如下：</p><ul><li><spanclass="math inline">\(\mathcal{T}_{\text{train}}(\text{MSF})\)</span>：在100个任务上训完<spanclass="math inline">\(Proj,Proj_b\)</span>，然后直接报告重建的prompt的表现</li><li><spanclass="math inline">\(\mathcal{T}_{\text{test}}(\text{MSF})\)</span>:在20个不可见的任务上直接报告重建的prompt的表现(用不可见任务的训练集先做PT训练，把出来的prompt跑一遍重建)</li><li><spanclass="math inline">\(\mathcal{T}^{\text{same}}_{\text{train}}(\text{IST})\)</span>:拿训好的<spanclass="math inline">\(Proj_b\)</span>，在可见任务上从零开始做d维的 ISTPT</li><li><spanclass="math inline">\(\mathcal{T}^{\text{diff}}_{\text{train}}(\text{IST})\)</span>：unseen-datachallenge，对于可见任务在保持测试集不变的情况下，换用另外的训练集(和之前训正常prompt不一样的训练集)做ISTPT</li><li><spanclass="math inline">\(\mathcal{T}_{\text{test}}(\text{IST})\)</span>：在20个不可见任务上做IST PT</li><li><spanclass="math inline">\(\mathcal{T}^{in}_{\text{test}}(\text{MSF}),\mathcal{T}^{out}_{\text{test}}(\text{MSF})\)</span>：如果用是否用CLS划分任务的话，测试loss分为两组（就是生成和非生成任务）</li></ul><p>实验结果如下:</p><p><img src="../files/images/unifed_subspace_pt/result.png" ></p><p>这个结果其实能说明很多问题，作者用了两页来讲这个结果，大致有以下几个关键点：</p><h3id="do-plms-really-reparameterize-various-task-adaptations-into-a-universal-task-subspace">DoPLMs really reparameterize various task adaptations into a universaltask subspace?</h3><p>首先就是主试验的结果，发现<spanclass="math inline">\(\mathcal{T}^{\text{diff}}_{\text{train}}(\text{IST})\)</span>和<spanclass="math inline">\(\mathcal{T}_{\text{test}}(\text{IST})\)</span>效果都很好，这说明用一个统一的subspace确实可以得到很不错的效果，有力佐证了共同子空间假说</p><h3 id="what-limits-ipt">What limits IPT?</h3><p>作者进一步分析IPT效果的提升空间</p><ul><li><spanclass="math inline">\(\mathcal{T}_{\text{train}}(\text{MSF})\)</span>和<spanclass="math inline">\(\mathcal{T}_{\text{test}}(\text{MSF})\)</span>之间的差距不小，说明模型对于不可见任务的泛化能力弱小，这可能和变换模型的结构有关系</li><li><spanclass="math inline">\(\mathcal{T}_{\text{train}}(\text{MSF})\)</span>和<spanclass="math inline">\(\mathcal{T}^{\text{same}}_{\text{train}}(\text{IST})\)</span>的差距，说明模型从统一子空间学习的能力没有在高位空间学习强</li><li>受限于prompttuning方法本身的限制。这一点作者做了用adpter找统一子空间的实验ISTAdapter和用随机参数找子空间的实验(fine-tune相当于这个的d推广到所有参数)，发现adpter的方法确实比pt更好</li></ul><p><img src="../files/images/unifed_subspace_pt/adpter.png" ></p><h3 id="how-is-the-influence-of-task-types">How is the influence of tasktypes?</h3><p>对比<spanclass="math inline">\(\mathcal{T}^{in}_{\text{test}}(\text{MSF}),\mathcal{T}^{out}_{\text{test}}(\text{MSF})\)</span>可以说是惨不忍睹，说明生成任务和理解任务的差距还是非常远的</p><h2 id="附实验">附实验</h2><p><img src="../files/images/unifed_subspace_pt/few-shot.png" ></p><p>作者尝试增加训练集中的数据量，发现基本上就是数据越多，效果越好。当d小的时候增长更为明显</p><p><img src="../files/images/unifed_subspace_pt/tasks.png" ></p><p>也探索了选用的可见任务多少带来的影响，说明可见任务越多，统一子空间的表征能力越强，或者说变换矩阵越准确</p><p><img src="../files/images/unifed_subspace_pt/stability.png" ></p><p>最后作者对比了稳定性，就是连续跑10次不可见任务每次score的方差：</p><ul><li>fine-tune的方差还行</li><li>直接做PT结果不稳定，或者说训练不好训</li><li>IPT是指先用IPT PT跑出来一个softprompt，然后作为起始点来进行真正的PT，这样结果最稳定。同时效果比单纯PT更强(103.4%<span class="math inline">\(E_{rel}^{PT}\)</span>)</li></ul><p>因此从统一子空间出发更有利于PT训练的稳定性</p><h2 id="我的思考">我的思考</h2><ul><li>实验设计很精妙，想想作者的几个观点和对应的实验方法</li><li>所谓的统一子空间，有没有可能就是token空间？不知道是不是upsample出来的结果更接近于模型本身的某个word embedding</li><li>能不能结合预训练任务？比如说用预训练任务做proj,proj_b的训练，看看在下游任务能不能更好的进行低维表征</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> delta tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记-Black-Box-Tuning相关</title>
      <link href="/3544a1a8.html"/>
      <url>/3544a1a8.html</url>
      
        <content type="html"><![CDATA[<p>今天来讲一下Black Box Tuning这个方向，主要是基于:</p><blockquote><p>Black-Box Tuning for Language-Model-as-a-Service</p></blockquote><blockquote><p>BBTv2: Pure Black-Box Optimization Can Be Comparable to GradientDescent for Few-Shot Learning</p></blockquote><span id="more"></span><p><img src="../files/images/black_box_tuning/author.png"></p><p><img src="../files/images/black_box_tuning/author2.png"></p><p>这两篇的作者是一波人，来自复旦大学。这两篇都是今年的文章。</p><h2id="language-model-as-a-servicelmaas">Language-Model-as-a-Service(LMaaS)</h2><p>所谓的黑箱训练，就是说如何在不能获取到模型的导数，只能有一个loss或者accuracy的情况下优化模型。</p><p><img src="../files/images/black_box_tuning/introduction.png"></p><p>之所以提出这个概念，是因为目前随着模型的规模越来越大，很多模型现在都是不开源参数的，只能开原一些inferAPI，因此也不能获取到梯度。在这种情况下，下游任务的研究者首先进行fine-tune不现实(没有梯度,然后存不下)。已有的一些deltatuning的手段也不现实，因为没有梯度。作者把这个场景叫做Language-Model-as-a-Service(LMaaS)</p><p>作者希望提出一种方法，可以在这种情况下微调模型。</p><h2 id="derivative-free-optimization-dfo">derivative-free optimization(DFO)</h2><p>这是另一个方向，专门在只有loss的情况下优化参数，有点像RL的场景。有一些已有的工作，我后面可能也会分享到。但大体面临几个问题：</p><ul><li>训练很慢</li><li>当需要调整的参数多的时候，基本不可能实现</li></ul><h2 id="black-box-tuning">Black-Box-Tuning</h2><p><img src="../files/images/black_box_tuning/bbt.png"></p><p>接下来作者就要讲本文的方法了。作者首先借鉴了Deltatuning领域，想要减少模型需要调整的参数。</p><p>作者用了soft prompt的方法。不过参数还是太多(length *dimension)。因此作者又掏出来了LoRA那个假设</p><blockquote><p>fine-tune has a low intrinsic dimension</p></blockquote><p>然后把prompt表示成了矩阵增多那个方法(Intrinsic原始论文那个搜索):<span class="math display">\[z* = \mathop{\text{argmin}}_{z \in \mathcal{Z}} \mathcal{L}(f(Az + p_0;X),Y)\]</span>这个公式的意思是，f函数是一个黑箱函数(模型输出)，通过一个promptp和输入x返回一个y，这里y指的是类别概率(做的分类任务)</p><p>然后作者把<span class="math inline">\(D = len \timesdim\)</span>维的prompt表示成了d维的$z,A ^{Dd}$,然后p0是一个预先设计的prompt</p><p>其中A是一个uniform分布的矩阵，选定就不变了。<spanclass="math inline">\(z \in[-5,5]^d\)</span>才是DFO算法实际优化的东西。</p><p>实际执行中，作者用的优化函数L是cross-entropyloss。然后DFO优化算法是经典的CMA-ES (Covariance Matrix Adaptation Evo-lution Strategy)算法</p><h2 id="black-box-tuning-v2">Black-Box-Tuning V2</h2><p>既然是两篇，那肯定是有改进的。第二篇说了BBT算法的缺点:</p><ul><li>需要预先设计P0。可能是任务相关的，不算是完全的黑箱</li><li>bbt算法主要是在拟合训练集，在测试集上并没有真的优化</li><li>对于entailment类的任务表现不加</li><li>和fine-tune比效果还是不够好</li></ul><p><img src="../files/images/black_box_tuning/compare_self.png"></p><p>因此作者做出了写改进，具体如下：</p><p><img src="../files/images/black_box_tuning/algo2.png"></p><h3 id="分治">分治</h3><p>首先作者参考prompttuning的改进方法在每一层都添加了prompt，然后由于多了层数L倍的参数，DFO肯定会下降。但作者观察到:</p><blockquote><p>由于残差链接的原因，模型的前向可以认为是分治的。最终的输出可以解递归变成每一层输出的和</p></blockquote><p>作者就用分治的手段，自底向上一层一层优化。然后A矩阵也是有L个</p><h3 id="a的生成">A的生成</h3><p>接下来，作者探索A矩阵的生成，原论文说是uniform分布更好，但作者觉得需要让A乘完了以后和真实的token或者模型的真实中间hiddenstate更接近，作者推导了一下矩阵乘的方差、均值，总结出来了下面的分布：<span class="math display">\[\begin{aligned}\mu &amp; = \frac{\hat \mu}{d - \hat\sigma^2} \\\sigma &amp; = \frac{\hat \sigma}{\sqrt{d - \hat\sigma^2}} \\\end{aligned}\]</span> 这个高斯分布采样出的A可以最大程度使得Az和真实分布接近</p><h2 id="实验">实验</h2><p><strong>注意，作者做的所有实验都是few-shot场景下的，也就是训练集每一类给了16条数据，然后测试集用了全数据(<spanclass="math inline">\(\gg\)</span> 训练集)</strong></p><p>经过下面的改进之后，作者在BERT,RoBERTa，CPM-2都进行了实验，效果还是很好的</p><p><img src="../files/images/black_box_tuning/compare_delta.png"></p><p><img src="../files/images/black_box_tuning/result.png"></p><p><img src="../files/images/black_box_tuning/cpm2_result.png"></p><p>对baseline的一些解释:</p><ul><li>BitFit, Adapter, LoRa, Prompt Tuning之前都讲过了</li><li>P-Tuning v2大概就是在每一层都有prompt</li><li>Feature-MLP, Feature-BiLSTM：有梯度的方法，是在CLStoken上面叠加了新的模型，前面大模型freeze</li></ul><p>后面作者还做了有意思的附实验:</p><p><img src="../files/images/black_box_tuning/ablation.png"></p><p>大概说了几件事情：</p><ul><li>搜索空间d越大，虽然变慢，但效果会变好</li><li>在d &gt; 100以后基本上就触顶了</li><li>表现随prompt length先增后减，大概50的时候表现最好</li><li>换了新的A的采样方法以后，训练开了很多，并且测试集convergence增强了很多</li></ul><h2 id="我的思考">我的思考</h2><ul><li>总体而言，这个文章提出的LMaaS场景感觉很大的实际应用空间，我觉得闭源大模型是个趋势;尤其是，大概率以后前向本机的小破卡也跑不起来……</li><li>作者没说多数据的场景表现怎么样，但我感觉应该是有梯度的方法远胜BBT。这个可能是妨碍应用范围扩大的一大问题……</li><li>我觉得有一个重要指标，就是query次数作者不是很重视，这个在实际应用中很重要吧——闭源大模型你花的钱好像是要正比于query次数……</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> delta tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-16总结</title>
      <link href="/62e8f349.html"/>
      <url>/62e8f349.html</url>
      
        <content type="html"><![CDATA[<p>最近几天在忙开题的项目，看了很多论文，感觉收获很大。上次这么集中看论文，还是在暑假刚开始的时候。感觉多读一读别人的工作就会觉得自己在做一些很有意思的研究，进而提升一些动力。</p><span id="more"></span><p>今天听了摄影协会的培训课，讲了讲基础知识和相机选择，还算有收获。总算是有课了，泪目了。</p><p>另外，今天在树洞看到了22届各个大厂的招聘薪资，精神内耗又加重了。</p><p>今天看pyq有同学发澜园超市的板栗很好吃，精神内耗有所减轻。我下定决心去买一点，目前在学校附近买过万人二层的板栗和盒马鲜生的板栗，不知道澜园的是不是更有含金量。</p><p>最后，TES今天小组淘汰，特此记录。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记-Intrinsic-Dimension相关</title>
      <link href="/476791e1.html"/>
      <url>/476791e1.html</url>
      
        <content type="html"><![CDATA[<p>讲两篇分析性文章，都是在探索任务本征维度，可以指导模型压缩、知识表示工作。方法非常简单，但是结论都能引发很多思考。</p><span id="more"></span><blockquote><p>MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES 2018ICLR</p></blockquote><blockquote><p>INTRINSIC DIMENSIONALITY EXPLAINS THE EFFEC- TIVENESS OF LANGUAGEMODEL FINE-TUNING 2021 ACL</p></blockquote><h2 id="intrinsic-dimension">INTRINSIC DIMENSION</h2><p>对于一个2参数的优化任务，常常会陷入局部最优，其中的山谷形状。但对于包含多个参数的模型，山谷的概率非常低（总不能所有参数的二阶导都恰好是正的吧），反倒是马蹄形的概率比较高(一些维度是不稳定平衡)。</p><p>这启示我们，随着参数的增大，也许模型+optimizer的训练模式的难度反而会降低。也就是说，大模型的参数也许是冗余的，为了帮助降低训练难度。</p><p>那么，一个任务究竟有多难？这就是本文探索的所谓intrinsicdimension。</p><p>举个例子，如果一个任务是优化一个1000参数的模型，要求是前100参数的和是100，101-200参数的和是200……，901-1000参数的和是1000</p><p>我们可以很快发现，如果每100个参数只有1个我们可以变，我们也可以凑出一个答案。这就代表着这个任务的本征维度&lt;= 10</p><h2 id="如何获取任务的本征维度">如何获取任务的本征维度？</h2><p>稍微想想，就觉得这其实是很难的,可能还涉及到另一个Minimum DescriptionLength (MDL)理论。</p><p>我们对任务的表示常常是和模型在一起的，我们就找一个耦合的算法，这样只能求出任务本征维度的一个上限。</p><p>将一个D参数的模型表示成下列的形式 <span class="math display">\[\theta^{(D)} = \theta_0^{(D)} + P \theta^{(d)}\]</span> 原来的模型<spanclass="math inline">\(\theta\)</span>含有D个参数，我们想要尝试d个参数能不能解决这个任务。用一个变换矩阵<spanclass="math inline">\(P\in \mathbb{R}^{d\times D}\)</span>来将d变成D</p><p>注意<span class="math inline">\(\theta_0^{(D)},P\)</span>都是锁定的，只有<spanclass="math inline">\(\theta^{(d)}\)</span>是可训练的。</p><p>然后用一个所谓<spanclass="math inline">\(d_{int90}\)</span>作为评判标准，只要到了全参数训练的90%的score，就算是找到了。如果找不到，就扩大d直到找到</p><h2 id="任务的intrinsic-dimension与影响因素">任务的IntrinsicDimension与影响因素</h2><p>作者在各种数据集合任务上都进行了实验，结论部分都差不多，下面主要讲一下MNIST上的一些结果：</p><p><img src="../files/images/Intrinsic_Dimension/mnist.png"></p><p>发现dimension其实是远小于模型参数的（0.5%）。另外，本征维度是和模型结构挂钩的，对比之下，conv结构在MNIST任务上就比全连接要好很多,750&gt; 290。</p><h3 id="conv真的更好吗">conv真的更好吗？</h3><p>关于上面的FC和LeNet的差距，作者做了一个附实验。把数据集中的每一张图片都做了同样的一个线性变换。显然，FC对线性变换不敏感，本征维度还是290，但LeNet直接衰减到了1400</p><p>这告诉我们：conv对图片局部表征能力强是战胜fc的关键</p><h3 id="模型大小影响本征维度吗">模型大小影响本征维度吗？</h3><p><img src="../files/images/Intrinsic_Dimension/size.png"></p><p>作者用各种大小的FC都算了一遍，发现模型结构定下来以后，单纯的扩大规模对本征维度影响很小。这也许从侧面说明了本征维度可以很好的衡量一个结构对任务的表示能力高低。</p><h2id="预训练和预训练模型对intrinsic-dimension的影响">预训练和预训练模型对IntrinsicDimension的影响</h2><p>这里开始是上面的后文了，主要在分析这两年的预训练模型新框架下IntrinsicDimension的一些结论是否还有效、有没有新的结论</p><p>在方法上基本没有更新，就是换了个更符合transformer结构的改进，因为同一个transformer层内的元素很强的相关性，作者就做了一个共享的layer参数 <span class="math inline">\(\lambda\)</span> <spanclass="math display">\[\theta_i^D = \theta_{i,0}^D + \lambda_i P (\theta_i^{d-m})_i\]</span>这个公式的意思是把原来模型的m个层分开处理，然后先把d个参数中找出m个作为scale变量<spanclass="math inline">\(\lambda_i\)</span>，接着每一层的参数看做每一层的矩阵变换剩下的d-m个参数再进行一个scale。这样可以更好的利用transformer结构本身的特点，找到更精确地IntrinsicDimension的上界</p><h3id="预训练任务帮助下降下游任务本征维度">预训练任务帮助下降下游任务本征维度？</h3><p><img src="../files/images/Intrinsic_Dimension/BERT_size.png"></p><p>作者在这里发现了不同的规律，BERT和RoBERTa基本只是扩大了size，为什么本征维度下降这么多呢？</p><p>作者认为是预训练任务本身可以帮助下游任务减小本征维度，或者说预训练阶段的“知识”获取可以更好的帮助下游任务</p><p>为此，作者从头预训练了RoBERTa，观察dimension变化：</p><p><img src="../files/images/Intrinsic_Dimension/pretrain.png"></p><p>可以发现，随着MLM预训练的进行，下游任务的本征维度真的在减小，模型在这个过程中是不能获取到下游数据集任何信息的</p><p>RoBERTa是不是个例呢?作者又找到了其他的很多预训练模型做实验:</p><p><img src="../files/images/Intrinsic_Dimension/pretrain_size.png"></p><p>可以发现，不管是什么结构，随着预训练模型参数的增多，下游任务的本征维度都是在下降的。这又是对上面猜想的有力证据</p><h3 id="本征维度与泛化能力">本征维度与泛化能力</h3><p>作者认为本征维度越小，泛化能力就越强</p><p><img src="../files/images/Intrinsic_Dimension/generalization.png"></p><p>作者通过实验证明了这个观点</p><p>总体而言，预训练任务和前一篇的随机初始化不同，模型本身就处于一个对下游任务本征维度低的位置，同时越大的预训练模型，这种表征能力就越强</p><h2 id="我的思考">我的思考</h2><ul><li>有没有人试过不用预训练，就单纯按上面论文的方法看看transformer的本征维度结果？</li><li>本征维度远小于模型参数，这是不是说明现在的训练瓶颈其实不是模型大小，瓶颈是小模型怎么训练出参数高效的形式(上面提到的高维模型训练难度低)</li><li>预训练模型所谓的“知识获取”，体现在IntrinsicDimension中是降低，那么知识蒸馏方向是不是可以借鉴这个来衡量蒸馏的结果呢？</li><li>Intrinsic Dimension可以支持delta tuning理论，但有没有试着把IntrinsicDimension和数据集质量联系起来？我觉得这个可以其实衡量一个数据集的好坏</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> delta tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-13总结</title>
      <link href="/30d0dcee.html"/>
      <url>/30d0dcee.html</url>
      
        <content type="html"><![CDATA[<p>这两天没有前几天那么冷了，并且我换了厚被子，感觉比之前几天舒服多了，心情也跟着好了起来。</p><span id="more"></span><p>仔细想了一下，今年秋天还没有给学校拍过照，前两年都拍过最少一次。正好这周末是摄影社的第一次摄影教学课，我打算去学习一下，也许今年秋天可以拍到这几年最美的银杏树，拍到最美丽的学堂路。</p><p>另一方面，这几天在按时进行健身和街舞队训。健身的重量比前几周加了一些，我感觉到了我的力量肌力在增长，满足感来了。也许到了这学期末，我会比之前有一个大的增长！再就是，我跳舞明显感觉到核心比之前更稳定了，稳定持久的核心和持续的律动是街舞的灵魂，果然practicemakes perfect。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</title>
      <link href="/7d79ba7.html"/>
      <url>/7d79ba7.html</url>
      
        <content type="html"><![CDATA[<p>这一篇工作和前两篇是另一种思路，对模型结构不做任何改变，只是在fine-tune时改变一点点参数:bias</p><span id="more"></span><p><img src="../files/images/BitFit/author.png"></p><p>ACL 2022的短文，方法其实就没啥，但揭示出来的道理不少。</p><h2 id="方法">方法</h2><p><img src="../files/images/BitFit/qkv.png"></p><p><img src="../files/images/BitFit/inter.png"></p><p>作者把BERT模型中所有的bias进行训练</p><ul><li>attention中的qkv矩阵里的bias</li><li>feed-forward层中的bias</li></ul><p>同时剩下的参数全部锁定，在下游任务上做fine-tune</p><h2 id="实验">实验</h2><p><img src="../files/images/BitFit/result.png"></p><p>可以看出，在下游任务上BitFit和正常Fine-tune比并不差</p><p>有意思的是，作者又做了几个实验</p><h3 id="所有bias的重要性对比">所有bias的重要性对比</h3><p>作者观察了不同的bias类型的训练后改变的差值，发现：</p><ul><li>key bias基本不变</li><li>query bias和intermediate bias改变很多</li></ul><blockquote><p>感觉这一点还有很多可挖掘的地方？但作者并没有进一步分析原因</p></blockquote><h3 id="随机">随机</h3><p>作者试着随机选择模型的一些参数，和bias数量一致，结果发现</p><ul><li>随机选其实效果也还行</li><li>如果选择连着的行或者列的元素，表现会更好。但是选bias是最特殊的</li></ul><h3 id="少数据">少数据</h3><p><img src="../files/images/BitFit/low_data.png"></p><p>类似的，作者也发现BitFit在少数据中表现更好</p><h2 id="我的思考">我的思考</h2><p>这个文章就更简单，但相比于方法，我对文章中提到的一个观点更感兴趣：</p><blockquote><p>fine-tune的过程不是让模型适应另外的数据分布，而是让模型更好的应用出本身的表征能力</p></blockquote><p>另外，作者没有提到的小细节我觉得可以引发很多思考：</p><ul><li>随机选模型的0.01%的参数进行fine-tune，也能达到全参数fine-tune的大概99%的效果</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> delta tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers</title>
      <link href="/4f026f2.html"/>
      <url>/4f026f2.html</url>
      
        <content type="html"><![CDATA[<p>今天分享一篇上次adptertuning的后文，进一步提升了效果、减小了参数量，发表在NeurlPS2021上。了解这篇工作，需要先了解 <a href="/6b5172a2.html" title="论文阅读[精读]-Parameter-Efficient Transfer Learning for NLP">Adpter Tuning笔记</a></p><span id="more"></span><p><img src="../files/images/compacter/author.png"></p><p>本来以为作者是之前adpter的作者团队，结果发现竟然一个重合的都没有……</p><p><img src="../files/images/compacter/compare.png"></p><p>作者分析了一下已有的一些deltatuning方法和fine-tune。发现大多不如fine-tune好，作者想对adpter做一些改进，提升一下效果，减少一些参数。</p><h2 id="方法">方法</h2><h3 id="kronecker-product">Kronecker Product</h3><p>首先普及一下矩阵的Kronecker Product <span class="math display">\[A \otimes B =\left(\begin{array}{lll}a_{11}B &amp; ... &amp; a_{1f}B \\  &amp; ... &amp; \\a_{m1}B &amp; ... &amp; a_{mf}B \\\end{array}\right)\]</span> 相当于把A的每一个元素都和B乘一下，然后<spanclass="math inline">\(A^{m\times f},B^{p\times q}\)</span>就会变成<spanclass="math inline">\(mp \timesfq\)</span>的矩阵。注意，相当于用更少的参数表达了更大的矩阵</p><p>接下来，就是通过上面的方法来进行矩阵乘的优化</p><h3id="parameterized-hypercomplex-multiplication-layers-phm">parameterizedhypercomplex multiplication layers (PHM)</h3><p><img src="../files/images/compacter/multiply.png"></p><p>首先，作者想要达到全连接层的效果 <span class="math display">\[y^d = W^{d\times k}x^k + b\]</span> 然后主要就是优化<span class="math inline">\(W\)</span></p><p>如果找到一个d,k的公约数n的话，<spanclass="math inline">\(W\)</span>形状的矩阵可以视为 <spanclass="math display">\[W_j = \sum_{i=1}^n A_i \otimes B_i^j,\quad A\in \mathbb{R}^{n\timesn},\quad B\in \mathbb{R}^{\frac{d}{n}\times \frac{k}{n}}\]</span>之所以要变成n个加在一起，好喜爱那个是为了增加自由度，让这个矩阵的表达能力和一个真正的大矩阵w的表达能力一样？</p><h3 id="应用于adpterlphm">应用于adpter(LPHM)</h3><p>为了用于实际的adptertuning，其实就是就是把全连接层换了。同时作者还做了几个重要的修改:</p><ul><li>所有的adpter层中，对于子矩阵A是共享参数的，这是为了更好的捕捉所谓的通用知识</li><li>然后每个adpter有单独的B，为了简化B的大小，可以把B变成秩更小的形式</li></ul><p><span class="math display">\[B^{m\times n} = s \times t^{T},\]</span></p><p>s和t都是r个列向量，这样B的参数总共需要<spanclass="math inline">\(r\times(m+n)\)</span>。这里的简化是基于上次adpter的实验中<em>单独的adpter层的影响很小</em> 的结论，总而言之,最终的公式化简成了<span class="math display">\[A^l(x)=\text{LPHM}^{U^l} (\text{GeLU}(\text{LPHM}^{D^l} (x)))+x,\]</span>作者对比了一下复杂度，发现和正常的adpter相比，可以把参数减少到大约<spanclass="math inline">\(\frac{1}{n}\)</span></p><h2 id="实验">实验</h2><p>几个行的解释：</p><ul><li>compacter++:正常的adpter在每个transformerblock加两次，这里探索只在最后的feedforward层后面加一次，就是比正常的参数更好</li><li>PHM-Adapter：正常PHM矩阵乘的adpter，没有做A的共享参数，没有对B进行矩阵分解</li><li>Intrinsic-SAID:这个方式是想把模型的参数进行重参数化，然后需要用的额外参数非常少<spanclass="math inline">\(0.009\%\)</span>。这个方法我后面可能也会做分享<a href="/476791e1.html" title="论文阅读笔记-Intrinsic-Dimension相关">Intrinsic Dimension笔记</a></li><li>BitFit：和adpter另外的路子，只训练一些额外的bias，锁定正常参数</li></ul><p><img src="../files/images/compacter/result.png"></p><p>可以发现，作者对比了2021常见的几个方法，compacter方法最好：</p><ul><li>发现共享A信息的compacter效果更好，这也说明了在不同adpter层间确实需要有一些共通的知识在</li><li>作者在实现中实际上用的B的分解秩r=1，这个好像并没有真的影响模型的效果</li></ul><p>有一个好玩的点：</p><blockquote><p>Intrinsic-SAID的效果甚至挺好的，参数还非常少，代价是什么呢？代价是训练时间长，以及对模型结构非常Aware</p></blockquote><p>后面，作者直接显式地做了关于效率的附实验，有一些点就是:</p><ul><li>Intrinsic-ASID虽然参数少，但训练过程并不高效</li><li>BitFit方法虽然效果稍差，但其实训练过程是很高效的</li></ul><p><img src="../files/images/compacter/efficient.png"></p><p>作者还额外提了一下，在少数据情况下，compacter比传统fine-tune的效果更好</p><p><img src="../files/images/compacter/low_data.png"></p><h2 id="我的思考">我的思考</h2><p>这个文章其实主要的idea还是沿用了adpter，但是思考通过优化矩阵乘的效率来减少参数很精妙，尤其是利用上了PHM共享A参数学习通用知识这个点，更是很妙。</p><p>我其实有几点好奇，文中没有说：</p><ul><li>B的分解秩r的大小对模型结果的影响。因为小的r显然会使得B的表征能力下降，但如果模型效果不下滑，是不是说明模型其实在下游中用到的知识真的是很少的？</li><li>模型高层和低层对应的B的影响的实验，参考正常adpter，因为这个实验的backbone模型是T5，之前是BERT。adpter提到的特性可以应用于其他的pre-trainmodel吗？</li><li>单纯是否共享A参数的对模型效果影响的对比实验。如果这种层间通用知识真的有用，能不能在预训练阶段把模型结构就有所改进呢？</li><li>compacter在少数据表现好的特性，是deltatuning共有的吗？因为我觉得其实deltatuning的上限就是fine-tune，所以超越fine-tune是不是代表，在这个下游任务上，现有数据集其实还没碰触到数据的边际效应递减</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> delta tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Parameter-Efficient Transfer Learning for NLP</title>
      <link href="/6b5172a2.html"/>
      <url>/6b5172a2.html</url>
      
        <content type="html"><![CDATA[<p>今天分享一篇delta tuning方向的经典论文Adptertuning，是一篇比较早的工作，2019年的ICML。</p><span id="more"></span><p><img src="../files/images/adpter_tuning/author.png"></p><p>作者来自Google Research和Jagiellonian University。</p><h2 id="introduction">Introduction</h2><p>作者在标题里用的说法是parameter-efficient，这个词语碰瓷的是fine-tune方法。当时BERT刚出不久，基本统治了NLP所有任务。然后pretrain+ fine-tune的manner是主流的思路，但这个作者发现：</p><ul><li>fine-tune需要调整所有的参数，对于N个任务，最后要存储<spanclass="math inline">\(\timesN\)</span>的参数，对于云服务器很不友好。</li><li>fine-tune需要调整所有参数，对于算力的需求也比较大。</li></ul><p>作者想要寻找有没有比fine-tune更好的方法，做到：</p><ul><li>在下游任务有良好的表现</li><li>对于多个任务不用同时需求所有数据集(这个是对比一般的transfer方法，对于多个任务的一般embedding需要同时需求所有任务的训练集)</li><li>对于每个任务，不需要很多的参数</li></ul><p><img src="../files/images/adpter_tuning/compare.png"></p><p>作者想到了adpter tuning的方法，只用多训练大约3%的参数，就能在GLUEbenchmark达到正常BERT的99%的水平，可以说是非常parameter-efficient了</p><h2 id="方法">方法</h2><p>adpter的作用和优势很多，方法却非常的简单。在这里，作者一般性的考虑了transformerblock</p><p><img src="../files/images/adpter_tuning/transformer.png"></p><p>对于一般的transformerblock一般是前面是一个self-attention/cross-attention，加一个feed-forward，然后是一个残差链接，接着一个layerNorm，再接一个feed-forward，然后是一个残差链接，接layerNorm</p><p>作者在这个过程中间插入了一些小的adpter层，在训练中只有绿色的部分是可训练的，别的部分的参数被锁定(BERT的预训练参数)</p><p>实现中和设计中有几个很重要的细节：</p><ul><li>插入的adpter层在feed-forward后面，在残差链接前面，因此不影响transformerblock残差链接在深度上的的效果</li><li>adpter层本身是含有skip-connection的，因此全0初始化的adpter层对transformerblock来说相当于不变。这一点很重要，因为训练的初期模型相当于和原模型保持一致，对训练的稳定性非常重要。</li><li>作者在训练中让transformer bolck的layerNorm层是不锁参的(用的pair-wisemuliply norm)，这样的好处是</li></ul><p>对于adpter层来说，为了减小参数量，用了所谓的feed-forward-down和feed-forward-up方法，使得中间变量的维度变得很小，SiLu激活函数连接。</p><p>作者提到，还有另外一些adpter层的设计方法，和这种设计方法的表现十分接近，本文就强调了这种设计，其他的设计还类似于:</p><ul><li>adding a batch/layer normalization to the adapter</li><li>increasing the number of layers per adapter</li><li>different activation functions, such as <spanclass="math inline">\(\tanh\)</span></li><li>inserting adapters only inside the attention layer</li><li>adding adapters in parallel to the main layers, and possibly with amulti- plicative interaction.</li></ul><p>总体而言，这边文章的关键不在adpter具体的设计(设计背后的理论我在几天后也许有的论文分享中会有更详细的探讨)，而在于这种方法本身，parameter-efficient训练，或者现在叫deltatuning方法的灵感。</p><h2 id="主实验">主实验</h2><p>这篇论文的实验设计其实还挺好的，作者在包括GLUEbenchmark在内的多个任务中，用BERT作为锁参的”大模型“来对比正常的BERTfine-tune， Variable fine-tune和非BERT SOTA的结果</p><p><img src="../files/images/adpter_tuning/GLUE.png"></p><p><img src="../files/images/adpter_tuning/non_GLUE.png"></p><p>作者行文用了很多的数据来表示：adpter方法和fine-tune基本没有任何区别，效果只下降了一点点点点</p><h2 id="对比实验">对比实验</h2><p>在这一部分，作者对adpter训练方法的特性做了很多的探索，可以引发人非常多的思考，同时这一部分的实验设计更是非常巧妙：</p><h3 id="移除adpter">移除adpter</h3><p>正常adpter是在每一层都有的，作者试着单独一处某一层的、或者移除一些层的adpter看效果</p><p><img src="../files/images/adpter_tuning/remove.png"></p><p>这个热力图的横纵坐标的跨度对应的层的adpter被移除了，对角线代表只移除一个adpter，右上角代表所有的都移除。这个图其实很有意思：</p><ul><li>对角线的表现基本没有下滑，这代表单独一层的adpter其实没有起什么作用，也就是说adpter层的参数和全0没啥区别。另一点上，对角线右下角(上层)的表现下降更多一些，说明上层的layer对模型的表现更重要</li></ul><blockquote><p>这一点有些佐证了”大模型前面层表征通用知识，后面层表征细粒度知识“的论点，因为后面的adpter对下游任务的帮助更大</p></blockquote><ul><li>当移除的数量增大时，表现下滑的很快，这说明adpter层其实是共同起作用的，并且起的作用各不相同。这其实正可以说明adpter层是非常parameter-efficient的了</li></ul><h3 id="鲁棒性">鲁棒性</h3><p>作者在这里探索了adpter层参数的表示是不是鲁棒的，也就是把正常训好的adpter参数叠加一个高斯噪声。</p><p><img src="../files/images/adpter_tuning/robutness.png"></p><p>当高斯噪声不大时(<spanclass="math inline">\(\sigma\)</span>小)，模型的表现下滑不大</p><p>另一方面，作者探索了adpter层参数对表现的影响：其实用比较小的adpter就能达到差不多的表现。这个论点也许可以用intrinsicdimension的角度衡量，后面我也许会写论文阅读笔记。</p><h2 id="我的思考">我的思考</h2><ul><li><p>总体而言，作者在pre-train刚出半年多，就想到、对比了fine-tunemanner，可见科研思路的敏锐。</p></li><li><p>同时，作者的adpter主打小参数，因此设计的一些鲁棒性方面的附加实验也非常好</p></li><li><p>既然通过给basebone模型添加一些参数，可以实现媲美fine-tune的效果；那么只训练basebone模型的一点参数，或者把运算方式进行一些改变，能不能获得媲美fine-tune的效果呢？</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> delta tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-11总结</title>
      <link href="/a74fcdc7.html"/>
      <url>/a74fcdc7.html</url>
      
        <content type="html"><![CDATA[<p>不知不觉又过去了快一周。随着这学期几门课程进度的深入和作业、实验的布置，以及科研的推进，逐渐感觉到压力上来了。</p><span id="more"></span><p>十月一假期只放了三天，我倒是在外面吃了好几顿饭。好像现在的嘴越来越刁了，或者说美食欣赏水平逐渐上来了，吃一些一般的食堂、外卖总觉得不好吃，喝速溶咖啡也觉得没什么味道。只能说，由奢入俭难，笑死。</p><p>上一周的街舞和健身也在按时参与。说起来，好像我还真的涨了一些力量，一些动作做起来比一个月前好像轻松了不少。听说健身的前期是所谓的”快速增长期“，不知道趁着这股风气我能不能增重个十几二十斤。不过健身还是要坚持，希望等到学期中、学期末的时候我还能再坚持去健身。</p><p>另外，我发现最近一段时间的论文读的少了一些，是不是有些懈怠了。正好我接下来可能要读一些deltatuning方向的论文，可以多做一些笔记。虽然才大四上的前半学期，好像毕设的压力已经来到了我这边。昨天和学长聊听说贵系的优秀毕设传统是要投、并且中一个不能来烂的刊。细数一下，ICLR已经截止了，ACLrollingreview还有两个月，以及明天的EMNLP，好像我也就剩下最多三次机会了，得提速了。</p><p>最后，s12小组赛正在进行中，我打算在小组赛两轮打完以后再一起写个总结，不一天一天评价了。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-5总结</title>
      <link href="/abb04eee.html"/>
      <url>/abb04eee.html</url>
      
        <content type="html"><![CDATA[<p>北京没有春天和秋天的渐变，只会在冷和热中切换。这两天降温很多，瞬间从30度变成了3度。前几天还穿着短袖出门，现在已经要穿帽衫+羊绒外套了。</p><span id="more"></span><p>另外，今天早上去健身了——7:30起,先在清芬吃个早饭，然后8:30卡点到综体练到9:30，再去上意大利语。感觉自己的生活方式变得很健康，如果不是下午困得睡了2个小时就更好了……</p><p>晚起毁上午，早起毁一天。但也许习惯养成之后会好起来。我觉得以后周一周三第二大节反正也要上意大利语，不如就都去健身吧。这样晚上的时间可以空出来，系统地做一些事情。是的，我已经下定决心了，把计划加到iCloud日历里了！</p><p>另外，最近科研有一些新想法，感觉有些好了起来。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-3总结</title>
      <link href="/c8607bd4.html"/>
      <url>/c8607bd4.html</url>
      
        <content type="html"><![CDATA[<p>今天是十一假期的最后一天。</p><span id="more"></span><p>十一假期和去年一样，只有三天;补课也和去年一样，补一天。算下来我们好像又没有放假，比起社会面的七天假期，只能说华子还是有华子的办法。当然，人总是活在比较中的——比起去年北大放了11天假，想起今年北大也放3天，突然就觉得好像舒服了很多~</p><p>十一这几天基本就是玩了，去了一趟自然博物馆，健了两次身，尝了尝四道口的鹤一烤肉自助，买了几件跳舞的衣服，看了点意大利电影，玩了几个steam十一促销的游戏。这几天北京基本一直在下雨和阴天间切换，我也想和很多同学一样去看看石景山游乐园。不过我总是一到阴天就觉得心情也变差了不少，还是在宿舍躺着最舒服。</p><p>明天又要开始上课了，今天欠下的4节课终究还是要补回去。明天要做红细胞的实验了，后天又能上意大利语，我每周对上课的期待全都集中在几个文素课上了。可能这就是大四同学的日常。</p><p>嗷对了，这两天申了奖学金，估计今年又能有几个小奖。</p><p>最后，据了解，本周六10.8号上午将有dk,t1,edg,jdg,rng,gen.g的比赛，就在今天！</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9-29总结</title>
      <link href="/87d3e2cf.html"/>
      <url>/87d3e2cf.html</url>
      
        <content type="html"><![CDATA[<p>今天去参加了popping与locking的队训，读了2篇论文。</p><span id="more"></span><p>popping新队长口才很好，不愧是曲艺队的，和我们讲了一些pop的技巧，比较欢乐。队训的节奏比较慢，不怎么累，没有教routine，不知道后面会不会教。总体而言，我觉得这种节奏对我这样的老年人还是挺温柔的，毕竟后面还有一场仗要打。</p><p>locking队训还是熟悉的感觉，用了非常多的时间来进行体能训练，来做快和慢的rocking。只能说，这两个队训加在一起把我彻底榨干了。总体而言，locking的氛围更活跃一点，最后也教了两个8拍的routine。</p><blockquote><p>街舞里的routine就是齐舞，大家一起跳提前设计好的一些动作(每个人的动作不一定相同)。</p></blockquote><p>另外，明天S12开赛，11点rng打drx</p><p>再另外，我明天去国家自然博物馆，看看有什么好玩的。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-LEARNING TO LEARN WITH GENERATIVE MODELS OF NEURAL NETWORK CHECKPOINTS</title>
      <link href="/e7441803.html"/>
      <url>/e7441803.html</url>
      
        <content type="html"><![CDATA[<p>今天分享一篇最近挂arXiv的很有意思的工作，是讲用diffusion模型来做optimizer优化的。</p><span id="more"></span><p>作者团队是伯克利，一作其实是做CV的，他的<code>GAN-Supervised Dense Visual Alignment</code>在今年CVPR上获得了OralBest Paper</p><p><img src="../files/images/G_pt/authors.png"></p><h2 id="introduction">Introduction</h2><p>首先要说一说什么是optimizer</p><blockquote><p>optimizer就是负责优化模型的。它持续的接受当前模型的参数和本次的梯度，然后负责进行模型参数的更新。可能会利用模型历史参数的信息来做一些类似momentum的方法。</p></blockquote><p>作者在这里提到一些已有的设计好的优化器manualoptimizer比如SGD，ADAM，它们面临的问题优化速度很慢，不能从过去的经历中学到东西。这个”过去“是指过去的run。比如说你用同一个初始参数来初始化模型，用同一个数据集训练。跑100次，第一次和第100次都是需要同样多的epoch才能收敛。</p><p>于是作者开始递归了……能不能训练一个优化器来优化模型，这就是<code>learning to learn</code>的意思。希望可以从一些已有的checkpoint作为数据集（大小23M），其中含有模型参数、模型初始参数、每个状态的loss、Error、Return等信息。是的，不含有梯度信息，只是单纯的loss，因此RL的score也在这个范围内。同时作者训练一个G.pt(generativecheckpoint)模型来输入最开始的参数<spanclass="math inline">\(\theta_0\)</span>和希望到达的loss，让模型快速地输出目标参数。在这里，作者非常巧妙地用到了diffusion训练框架，后文会提到。</p><p><img src="../files/images/G_pt/dataset.png"></p><p>总体而言，这个方法有如下优点：</p><ul><li>可以对不同的初始化参数通过1次更新就得到很好的效果</li><li>它可以非常方便的对各种大小的loss都能更新出来</li><li>对于out-of-distribution的初始化方法，效果也不错(diffusion能力？)</li><li>对于不可导的loss比如RL的score，也能出结果。</li></ul><h2 id="method">Method</h2><p>总体算法如下图所示：</p><p><img src="../files/images/G_pt/algo.png"></p><h3 id="首先是如何制造这个数据集">首先是如何制造这个数据集</h3><ul><li><p>数据集里的每一个checkpoint组其实是一次running里的多个checkpoint，就是把模型正常的训练一下，然后最终每个running都采样一些</p></li><li><p>由于数据很大生成很慢，可以思考进行数据增强</p></li></ul><blockquote><p>比如permutation，连续两个矩阵运算，两边的参数分别左乘右乘互逆的交换阵，就是一个等价的模型。</p><p>比如大小变换也是，前面所有参数乘2，后面的除以2，还是等价模型。</p></blockquote><p>总之，作者最终获得了一个从MNIST, CIFAR-10数据集训练MLPs,CNNs结构的10万次running里的23M个checkpoint</p><h3 id="g.pt是什么样子">G.pt是什么样子</h3><p><img src="../files/images/G_pt/arch.png"></p><h4 id="training-and-sample">training and sample</h4><p>在训练时，可以知道一个模型的初始参数<spanclass="math inline">\(\theta_0\)</span>与初始loss <spanclass="math inline">\(L_0\)</span>。同时还有后面某个step的参数<spanclass="math inline">\(\theta^*\)</span>和loss <spanclass="math inline">\(L^*\)</span></p><p>接下来，先把<spanclass="math inline">\(\theta^*\)</span>按diffusion时间j进行noise变成<spanclass="math inline">\(\theta_j^*\)</span>,然后使用diffusion的训练方式：<span class="math display">\[\mathcal{L}(G) = \mathbb{E} \left[|| \theta^* - G(\theta_j^*, \theta_0,L_0, L^*, j) ||^2_2 \right]\]</span></p><blockquote><p>有个小细节，在这里，作者的模型G.pt是预测参数<spanclass="math inline">\(\theta_j\)</span>，而不是noise或者什么，这个方式叫做predictsignal</p></blockquote><blockquote><p>作者在后文提到了另一个细节Training on intermediate checkpointsimproves one step training。也就是说在选取训练batch的时候，初始<spanclass="math inline">\(\theta_0\)</span>可以不是真的初始参数，然后目标参数<spanclass="math inline">\(\theta^*\)</span>也可以是训练中间的一个参数，这样会使得G.pt总体表现提升50%</p></blockquote><p>训练结束以后，模型直接输入一个新的初始参数<spanclass="math inline">\(\theta_0\)</span>和<spanclass="math inline">\(L_0\)</span>，然后构造一个比较低的promptLoss，让模型跑一次，就行了(这个一次，是指一次diffusion采样，可能会很多T=1000)。作者sample公式使用DDPM传统公式</p><h4 id="tokenize">tokenize</h4><p>G.pt需要输入模型参数，参数要如何表示，作者考虑了NLP里tokenize的方法，把模型的每个layer变成M维token(超过M就用多个chunk一下)。</p><p>注意，所有的时间j，loss L， return R，errorE作为单独的token也要输入进去</p><p>接下来，作者有个小细节，tokenize完以后要用一个MLP全连接过一次，每个token的MLP是不共享参数的</p><h4 id="gpt-2">gpt 2</h4><p>后面，就直接把模型扔进一个gpt 2结构做一次decode。作者提到去掉了casualmask，这是因为模型应该需要看到”未来“，知晓所有信息</p><p>然后把前面<span class="math inline">\(\theta_j^*\)</span>位置的nexttoken prediciton的结果拿出来，再过一个不共享参数的MLP作为模型的输出</p><h4 id="global-residual-connection">Global residual connection</h4><p>作者提到，模型本身的输出还有一个全局的残差连接。然后模型其实是预测<spanclass="math inline">\(\theta^* -\theta_j^*\)</span>，同时让模型的参数按照全0初始化，也就是总体而言是一一映射初始化。</p><p><img src="../files/images/G_pt/hyperparameter.png"></p><p>超参如上图，可以看出作者的G.pt模型其实不小，训练时maxlength应该有几千，还是挺烧卡的。</p><h2 id="experiment">Experiment</h2><p>作者首先摆出来了一次sample的结果，G.pt给的promptloss在上面的超参的图里。</p><p>可以看出传统优化器一次sample基本啥效果都没有，但G.pt效果已经不错了。后面一行是传统manualoptimizer训练曲线，红线是G.pt一次的结果，看起来G.pt一次顶传统optimizer炼1个小时。</p><p><img src="../files/images/G_pt/result.png"></p><p>多次的持续sample，每次prompt loss低一些效果怎么样呢？</p><ul><li>对于初始化方法接近的模型，第一次效果基本就可以了，再来一次效果差距不大</li><li>对于out-of-distribution的数据，多次sample的效果更好</li></ul><p>另一方面，作者也考察了输入不同prompt模型输出的准确度，可以看出对于不同的promptloss，模型输出的目标参数对应的loss，确实比较接近prompt。</p><p><img src="../files/images/G_pt/different_loss.png"></p><p>接下来作者做了个很有意思的实验，首先作者探索扩大模型规模对结果的影响(10M-800M)，发现基本是线性的。同时对比了扩大数据规模的影响，发现基本触顶了，这说明的G.pt结构的能力还可以被大模型进一步的发挥。这个实验设计很有意思，参考了</p><p><img src="../files/images/G_pt/scale.png" style="zoom:33%;" ></p><h2 id="我的思考">我的思考</h2><ul><li>这个工作一看就是很好的工作，diffusion泛化能力的又一体现，让人看了就觉得自己可以follow一下。研究的问题也很有实际意义</li><li>全局残差连接是不是成功的关键？连接起了diffusion框架里predictionnoise/signal的问题，但作者没多说</li><li>我个人感觉这个最少是2023 ICLR oral，没准是bestpaper。现在放arXiv应该是赶ICLR投稿</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9-27总结</title>
      <link href="/d7ec9992.html"/>
      <url>/d7ec9992.html</url>
      
        <content type="html"><![CDATA[<p>今天又是开心的一天。第一次去了hip-hop的队训，学了一些基本的律动，进行了很累很燃脂的体能训练，最后也交了一点点drill的动作。虽然感觉自己被汗淹没，但是很开心。可能跳舞就是可以给人一种原始的快乐，就像原始人围着火堆起舞，庆祝一天打猎的丰收。</p><p>比起7、8月份的"累"和”平淡“，这个月”开心“的频率好高呀。可能当我离开了独处，有一次和同学、朋友们呆在一块之后，我才会感觉到更放松吧。我在思考要不要把心情的显示和更新频率的颜色结合起来，也许找一个简单的情感分类模型把每天的心情得分变成0-1，然后从蓝到红决定一下，再用深度表示更新字数，看起来会更有趣一些。</p><p>另外，据了解，经过大约1周的时间，我已经传染了6个人。这件事要追溯到上上周天我生病，发了一点低烧。然后上周一顶着病去装机，那一次我似乎传染了tle和磊哥。同样，在接下来的几天，我传染了朱哥和苏哥。而后，在这个星期，于哥和tle的室友也都感冒了。</p><p>这就是流感，传染人很快，但似乎每个人都没什么大事。在《神奇的免疫》学到老人由于红骨髓和纯真白细胞少，受到流感的影响会很严重。也许如果是老年大学，这件事就不会这么简单了。</p><p>最后，S12还有2天开始，RNG对阵DRX</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9-25/26总结</title>
      <link href="/dfdbbbd6.html"/>
      <url>/dfdbbbd6.html</url>
      
        <content type="html"><![CDATA[<p>25号晚上，我参加了街舞社的迎新活动。虽然已经是第三次去了，但还是感觉很开心很激动，觉得自己又能跳街舞了。之前的locking队长小崔已经毕业了，之前的popping队鞋神也已经毕业了，我却又一次成为了新社员。新入社的同学们每一年都一样，都洋溢着对街舞的热情和热爱：自己好像又年轻了起来，不像是大四的油条，也不像是把清华每一寸都丈量清楚的老狗，而像个新生一样加入新社团，认识新朋友。我喜欢这种感觉。</p><span id="more"></span><p>这学期我打算参加hip-hop的训练，当然也可以去体验一下locking和popping的氛围。总之，我希望这次可以坚持地更久，也可以真的融入到社团中来。没有期中期末和大作业二鬼拍门，我终于有时间深吸口气，等等自己的爱好了。</p><p>另外，最近科研多了几件新活，开始进入到了研究生、学期中的节奏。其实我还挺喜欢这种感觉的，有事情做，才有收获感。好像又变回了纷乱又紧张，迷茫又憧憬的大一：分流、课业、实验室、社团、班团工作忙成一锅粥，没有时间去思考未来。不过，现在我的人不一样了，虽然不一定有同样旺盛的经历，但我有了更多的经验，就像是免疫中学的”更多的memory白细胞“。我不知道我能不能handle好这些事，但我想尽力去做。</p><p>也许未来的一段时间，我老头环、大镖客、峡谷、S赛的时间会缩小部分，我不确定。但我觉得抽一些时间去健身可能会更有意义，肌肉泵感的快乐也许比峡谷心跳更刺激，谁知道呢。</p><p>以上</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9-24总结</title>
      <link href="/e604830f.html"/>
      <url>/e604830f.html</url>
      
        <content type="html"><![CDATA[<p>最近一天，我参加了两个社团；最近三天，我去了两次健身房，第四次办了季卡；最近一周，我吃了三顿火锅。真正的感受到了”开学“的氛围。</p><span id="more"></span><p>还是熟悉的健身房，还是熟悉的筋肉人。每次都是筋疲力竭，和可以想象到的隔天浑身酸痛，但多巴胺的分泌会给我带来快乐和满足感。今天在墙上的健美照片上看到了之前健身课的同学。刹那间我感触良多，有点像新闻上”法庭上嫌犯被法官认出是初中同学“。原来坚持真的可以改变很多，也许我和健身筋肉的区别就是在于我办了四次卡，却一共只去了不到30次健身房。</p><p>不能再这样下去了！从今天开始，从这学期开始，每周两次健身房，雷打不动。希望下个学期的现在，我可以突破120，乃至突破130，不说变成筋肉人，最起码变成正常人。</p><p>除此以外，我又重新加回了街舞社DK5,说起来，这已经是我第三次入社了，之前练过了locking和popping但都没坚持下来。这学期和苏哥一起入社，我在考虑是练习hiphop还是popping，可能要过两天才能作出决定。街舞和健身一样，同样需要坚持，但我现在大四了，希望要坚持下来。</p><p>不能再这样下去了！从今天开始，从这学期开始，每周一次街舞队训，雷打不动。希望下个学期的现在，我可以突破自己。不说变成舞王，最起码能拿出一段步伐。</p><p>同时，我也加了摄影社，摄影社中会教我们怎么去构图，同时好像也有一些线下的去校外的拍照活动。希望我的相机可以不再吃灰，相机后的脑子也不再猪脑过载，可以拍出一些好看的、有意境的照片。</p><p>最后，据了解，我科研面临倒闭危险。此外，S12快要开始了。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-WantWords: An Open-source Online Reverse Dictionary System</title>
      <link href="/2673b2a2.html"/>
      <url>/2673b2a2.html</url>
      
        <content type="html"><![CDATA[<p>这篇文章发表于2020年的EMNLP demo，作者团队还是义原计算小组。</p><p><img src="../files/images/Wantwords/authors.png"></p><p>既然是是demo，那它一定是有一些借鉴的方法。其方法主要是这一篇提到的：</p><blockquote><p>Multi-Channel Reverse Dictionary Model (AAAI 2020)</p></blockquote><p>他们解决的都是所谓的“反向词典”任务。</p><h2 id="反向词典任务">反向词典任务</h2><p>反向词典任务就是和词典的工作相反：输入一个描述，返回对应的意思,如下图所示：</p><p><img src="../files/images/Wantwords/description.png"></p><p>这个任务其实非常有实际意义：</p><ul><li>解决所谓tip-of-the-tongue的问题，当你想要表达一个意思，突然想不起来这是什么词（尤其是成语）</li><li>帮助语言初学者联想到自己不知道的词语，这个时候往往用户还需要对应的词语释义</li><li>有些word selection anomia病人，只能通过这个方法来组织词汇</li></ul><p>事实上，反向词典任务经过拓展还能实现更多功能：</p><ul><li>比如通过在词库添加名人名言、唐诗宋词，可以帮你在作文中进行名言推荐</li><li><del>添加党史、领导人语录，帮你在思想汇报、新闻稿中提高深度</del></li></ul><p>已有的商用反向词典比如<code>OneLook</code>,<code>ReverseDictionary</code>都不是开源的，并且效果也一般。本文提出的方法效果很好，同时支持中英之间跨语言的搜索</p><h2 id="multi-channel-reverse-dictionary-model">Multi-Channel ReverseDictionary Model</h2><p>已有的方法词典方法一般是两种路径：</p><ul><li>第一种是把查询query和词库里词语的词的释义进行比对，选最接近的。缺点是用户的query常常和释义的形式很不一样，极有可能出现偏差</li><li>第二种是把query编码到隐空间，和词库里的词向量进行比对，选择接近的。缺点是很多罕见词的词向量其实很不准确，但真实的用户query常常就是想要”罕见词“。</li></ul><p>这篇方法主要走第二个路径，但通过加入一下专家知识帮助减少误判。下面简单讲讲方法</p><p><img src="../files/images/Wantwords/arch.png"></p><p>首先是用一个Bi-LSTM把输入编码到hiddenstate。接下来有四个channel辅助判断，以减少罕见词的误差：</p><ul><li>POS预测器: 用分类器把hiddenstate给每一种词性一个分类得分。同时词库里的词对应一些POS。词语的得分额外加上”对应POS的得分“</li><li>category预测器：和上面很接近，相当于把词库里的词分成好多组，然后一样给得分。</li><li>morpheme预测器：morpheme也有点类似，每个词都有对应的morpheme。给hiddenstate每个token都进行morpheme分类算得分，然后词库里的词也有morpheme，把对应得分加上</li><li>sememe预测器：和morpheme完全一致</li></ul><p>上面四个channel的得分以及本身词向量和hiddenstate(线性到一个token)相似度的得分wordScore，加一个权重，最后来一个总的得分。这个方法效果很好，尤其对于罕见词。</p><h2 id="wantwords">WantWords</h2><p>本文的demo主要也是用Multi-Channel Reverse DictionaryModel(MRDM)的方法，但做了一些改进</p><blockquote><p>把Bi-LSTM换成了BERT</p></blockquote><p>同时处理了一些边界条件：</p><ul><li>同语言查询输入一个词：把query词的词向量直接算cosine similarity</li><li>跨语言查询：通过百度翻译API翻译到目标语言，然后查询</li><li>跨语言时输入一个词：把词对应的释义通过百度翻译API翻译到目标语言，然后查询</li></ul><p>同时WantWords也给出了查询词对应的释义，帮助语言学习者掌握联想到的词语</p><h2 id="我的思考">我的思考</h2><ul><li>不知道用大模型+prompt做这个的效果怎么样</li><li>感觉现有的词典软件都可以添加这个功能</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 义原计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9-21总结</title>
      <link href="/b43caca8.html"/>
      <url>/b43caca8.html</url>
      
        <content type="html"><![CDATA[<p>最近几天又生病了，是的，暨上学期双病、在上学期单病之后，这学期继续打卡感冒发烧。</p><p>生病是一个很神奇的事情，没有生病时就想不起生病时的难受，生病时也很难想象没有生病时的舒爽。当我躺在床上，头疼流涕，肌肉酸麻时，我有又第n次的想要把健身提上日程。为祖国健康工作五十年，需要能蚌五十年的身体，为了更好地当人民的先锋队，我得健身！</p><p>健身需要毅力，但我没有毅力。所以我想到一个绝妙的办法：我买一大桶增肌粉，这样如果不经常去，就会喝不完，希望这样可以用经济的刺激持续提醒我健身，倒逼习惯养成，形成办卡健身喝粉良性循环。话说达宝这学期也有在高强度健身，不能再这样下去了！！</p><p>除了生病以外，截止到现在，我基本上已经上了一周课了，又新增了几门课程：在补退选中悬悬选上的《计算机网络管理》，手动选课上的《意大利语》和《数据库系统概论》。和前几年不同，这次我认真地听了老师讲的内容，感觉还挺有意思的：意大利语的<code>Napoli</code>、<code>latte</code>等单词和背后的故事，绦虫等寄生虫和免疫系统斗智斗勇……说来也有趣，当你想要拿到很高分数时，就会发现老师上课枯燥无味、昏昏欲睡，语速完全比不上录屏里开二倍速效率高；但当你抛开了这些需求，真的去跟着老师的思路走时，又会开始感受到“知识习得”本身带来的快感和收获感。</p><p>也许大学更应该像后者一样真正沉浸在学习知识中度过四年，而不是像算斩杀一样一门门一点点算自己的扣分点吧。这种GAP是怎么来的呢？我不知道是我的问题还是清华课程设置的问题，但总之应该有至少一方错了吧。总之，既然有了条件，那这学期何不感受一年pro版课程体验呢。</p><p>最后，最近帮TLE装机一台，特此记录配置。据了解，在过程中我是喝药的监工：</p><ul><li>CPU:中国特供版Intel i5-12490无核显</li><li>主板：华硕B660M-pro WIFI版 = 版u加一起2179</li><li>内存：金士顿16g*2 3600HZ = 879</li><li>固态：西部数据SN570 1TB = 599</li><li>电池：安钛克金牌🏅650瓦 = 400</li><li>显卡：索泰电竞之心 3060TI = 2799</li></ul><p>另外，据了解昨天已发布4090和4080，不知道这两天是不是显卡价格会大幅度下滑。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Towards Building a Multilingual Sememe Knowledge Base: Predicting Sememes for BabelNet Synsets</title>
      <link href="/b3424711.html"/>
      <url>/b3424711.html</url>
      
        <content type="html"><![CDATA[<p>好久没写论文阅读笔记了，正好今天重读老论文，分享一下。这篇论文是关于义原预测的，发表于2020年AAAI。感觉这个方向的曝光度并不是很高，正好在这里和大家分享一下义原sememe的相关知识。</p><span id="more"></span><p><img src="../files/images/babelnet_predict/author.png"></p><p>论文作者基本都是我们义原计算小组的成员。</p><h2 id="字意思和义原">字，意思和义原</h2><p>首先为大家分享一下义原(sememe)的概念：</p><blockquote><p>在中文或者英文中，都是把字或者词作为最小的单位，但语义的最小单位可以被进一步拆分成。比如说，一个词语可以有多个意思，而每一个“意思”，都可以被视为是一个由“义原”组成的树状结构。</p></blockquote><p>义原的概念最早是1926年语言学家提出的。在上个世纪，董强先生花了大约20年具体的给中文系统整理出了大约2000个义原，并规定了它们的相互关系。同时给常见中文字都标注了对应的sense和义原树。义原树中的节点代表某个义原，而边代表了义原的相互关系。</p><p>从这里也可以看出，标注义原，尤其是结构化义原，其实是一个非常难、非常费时费力的任务。</p><p><img src="../files/images/babelnet_predict/sememe.png"></p><p>如果我们可以获取到语义义原级别的表示，可以辅助我们进行NLP任务比如词义消歧。有几个优势：</p><ul><li>首先义原和语言无关，因此不同的语言中的对应词的义原可能是非常接近的</li><li>义原作为一种专家知识，非常适用于小数据的情景。(比如一句话里的词义消歧)</li></ul><h2 id="论文本体">论文本体</h2><p>这篇论文是想要把义原知识库和知识图谱联系起来。具体来说呢，想要处理多语言义原标注的问题。这是由于已有的多语言义原的预测只能一次一个语言，速度很慢。</p><p>首先，我们找到了BabelNet数据库:是一个多语言的、同义词数据库。里面的基本单位是同义词synset，包含各个语言的版本，以及一个英文定义，可能还有链接到的WiKipedia的内容。数据来源基本上是WordNet和Wikipedia的内容的组合，语言涵盖了常见的224种语言。</p><p><img src="../files/images/babelnet_predict/synset.png"></p><p>对于义原标注来说，如果标注了一个synset，可能就能把不同语言中synset内的词语都标注完成，因此可以一次进行多语言的义原标注。提出了给synset进行义原标注的任务SPBS:predicting appropriate sememes for unannotated BabelNet synsets。</p><h2 id="babelsememe-dataset">BabelSememe Dataset</h2><p>我们提前标注了大约1.5万个synset的义原，只进行了无结构的标注。</p><p><img src="../files/images/babelnet_predict/babelsememe.png"></p><p>选取了BabelNet同时含有中英文的一个20000的子集。然后候选集合是所有中文的义原的并集。让100为参与者通过定义、Wikipedia页面等辅助信息进行进一步的挑选，筛掉了所有Krippendorff’salpha系数低的结果，保证每个synset最少三个人标注。最终平均alpha系数是0.702</p><h2 id="spbs">SPBS</h2><p>这个任务的基本范式是：对于每个synset <span class="math inline">\(b\in B\)</span>，找到每个义原<span class="math inline">\(s \inS\)</span>的属于该synset的得分<spanclass="math inline">\(P(s|b)\)</span>，然后通过超参<spanclass="math inline">\(\delta\)</span>控制结果： <spanclass="math display">\[\hat{S}_b = \{ s \in S | P(s|b) &gt; \delta \}\]</span> 这里我们用到了两种思路:</p><h3 id="基于-semantic-representations-spbs-sr">基于 SemanticRepresentations： SPBS-SR</h3><p>这个基本上之前word级无结构义原预测的方法。它的原理是:语义接近的synset对应的义原应该也相近<span class="math display">\[P(s| b) = \sum_{b&#39; \in B&#39;} \cos(\textbf{b},\textbf{b&#39;}) ·I_{S_{b&#39;}}(s) · c^{r_{b&#39;}}\]</span> 上面的加粗符号是b对应的vector。然后 <spanclass="math inline">\(I\)</span>代表是否含有特定义原(有是1，没有就是0)。c是一个超参。<spanclass="math inline">\(r_{b&#39;}\)</span>的意思是“b'的cos相似度在所有b里面排第几”，相当于更关注vector表示接近的synset。</p><p>这个方法需要有对应的vector表示。BabelNet如果synset是从Wikipedia来的，那么有对应的Wikipedia页面，可以用NASARI算法获取vector表示，可以用这个方法。</p><h3 id="基于relational-representations-spbs-rr">基于RelationalRepresentations: SPBS-RR</h3><p>这个方法适用于所有synset。它的思路是两个有相同"关系"的synset，对应的义原也应该有相同"关系"：</p><p><img src="../files/images/babelnet_predict/relation.png"></p><p>BabelNet中的元素可以视为三元组(h,r,t)。如果把“拥有某义原”视为特殊的关系的话，可以通过下面的公式进行义原表示s、synset表示b、关系表示r的联合训练：<span class="math display">\[\mathcal{L}_1 = \sum_{(h,r,t)\in G} \max[{0,\left( \tau +  d(h+r,t) -d(h+r,t&#39;)\right)}]\]</span></p><p><span class="math display">\[d(x,y) = ||x - y||^2\]</span></p><p>这个公式中t'代表另外的一个节点(不包含原来h,t的关系r)。再就是，由于synset的意思视为所有sememe意思的组合，因此有:<span class="math display">\[\mathcal{L}_2 = \sum_{b \in B} || b + r_s - \sum_{s \in S_b} s ||^2\]</span> 通过<span class="math inline">\(L = \lambda_1 \mathcal{L}_1 +\lambda_2\mathcal{L}_2\)</span>最终可以获取到所有表示，然后义原预测得分:<span class="math display">\[P(s | b) = || b + r_s - s||^2\]</span></p><h3 id="ensemble-model">ensemble model</h3><p>上面两种方法的得分可以组合： <span class="math display">\[P(s | b) = \lambda_c \frac{1}{rank_s^c}+ \lambda_r \frac{1}{rank_s^r}\]</span> rank是所有义原s的得分分别按降序排序的序号</p><p>最终在实验中，可以看到效果还不错：</p><p><img src="../files/images/babelnet_predict/score.png"></p><h2 id="思考">思考</h2><ul><li>文章的主要贡献是第一次联系起来了Hownet和BabelNet，可以进行synset的义原预测</li><li>贡献一个精细标注的数据集其实对领域的前进和曝光有很大的作用</li><li>能不能进行细细粒度的标注，比如结构？以及在义原预测的时候，能不能进一步利用definition等信息？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 义原计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置LaTeX环境:Windows,Mac,WSL</title>
      <link href="/b4c23497.html"/>
      <url>/b4c23497.html</url>
      
        <content type="html"><![CDATA[<p>昨天需要改稿，因此需要编译大段LaTeX代码，因此重新配置了LaTeX环境，在Windows、Mac、WSL上都配了一遍，特此记录。</p><span id="more"></span><h2 id="overleaf">Overleaf</h2><p>首先，如果要写LaTeX，首推去远程<ahref="https://www.overleaf.com">overleaf</a>上在线编辑，支持多人合作、各种字体以及自带很多模板，比本地可以说是方便很多了。</p><p><img src="../files/images/latex配置/overleaf.png"></p><p>不过，我由于需要编译的文本太长，超出overleaf普通用户的编译时长限制(需要大会员才能继续编译)，因此只能选择本地编译。</p><p>值得一提的是，Overleaf是个开源框架，我们系自己搭了个编译服务器，地址在<ahref="stu.cs.tsinghua.edu.cn">这里</a>，用酒井ID登录，应该是没有一大堆的限制，不多我昨天尝试是servererror了，不知道现在是否还有人在维护……</p><h2 id="在windows-mac-wsl上配置tex-live">在Windows, Mac, WSL上配置TeXLive</h2><p>LaTeX是一套排版系统，而<a href="https://tug.org/texlive/">TeXLive</a>可以视为是一个多平台的LaTeX编译器，帮你把LaTeX源码编译成pdf文档。</p><p><img src="../files/images/latex配置/texlive.png"></p><ul><li>Windows安装很简单，去官网直接选择<code>install on Windows</code>，有GUI的，安装完好像需要重启电脑。windows特色？？？</li><li>Mac安装也简单：点下面那个<code>install on MacOS</code>,也有GUI</li><li>Linux最简单的方式是,不过由于有4.5GB，可能要考虑给apt换源。</li></ul><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install texlive-full</span><br></pre></td></tr></table></figure></div><blockquote><p>如果觉得下载还是慢，可以先下载<ahref="https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/">ISO文件</a>，然后本地安装。</p></blockquote><p>最终如果在终端输入，有如下反应就是成功了:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="BASH"><figure class="iseeu highlight /bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tex -v</span><br></pre></td></tr></table></figure></div><p><img src="../files/images/latex配置/out.png"></p><h2 id="vscode配置latex编译">Vscode配置LaTeX编译</h2><p>vscode可以轻松配置链接TeX Live,需要安装这两个插件扩展:<code>LaTeX, LaTeX Workshop</code></p><p><img src="../files/images/latex配置/vscode.png"></p><p>需要注意的是，在拓展设置中：</p><p><img src="../files/images/latex配置/setting.png"></p><p>默认是使用latexmk编译器，如果你的项目使用例如XeTeX语法，则需要切换选用的编译器。(可以直接点下面的子按钮)</p>]]></content>
      
      
      <categories>
          
          <category> 开发记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>9-16总结</title>
      <link href="/ff1495c5.html"/>
      <url>/ff1495c5.html</url>
      
        <content type="html"><![CDATA[<p>据了解，我已经半个月没写日记了。</p><p>其实最近半个月还是发生了很多事的。首先我终于从隔离点出来了。隔离点说实话还不错，单人单间，每天送饭到门口，虽然是学校的15元套餐，但花样倒是每天都变，酸梅汤豆浆之类的饮料也都有。每天晚上还有额外的泡面、火腿肠提供作为夜宵。</p><span id="more"></span><p>当然，这并不能掩盖它的不自由。首先不能出房间、不能点外卖有点阴间，每天看到外面好利来里面的顾客进进出出，我们却不能点外卖，还是有点心酸的。再有就是不能开窗户，总感觉空气的流通性很差。囚犯就得有囚犯的自觉，总体而言，也算是一段难忘的经历……吧。</p><p>从隔离点回来以后，学校基本上也是封校的状态，估计1016之前都别想解封了，虽然网传9.19以后政策会放松，但看到中国传媒大学最近又出了高校聚集性疫情，好像所剩不多的希望又被进一步地压榨了。动态清零呀动态清零，什么时候才能真的清零呢？</p><p>从这周开始，新的学期就开始上课了。每天学堂路上又多出来了一大堆皮肤黝黑，车子却干干净净的新生。食堂也众所周知的顿顿爆满了起来。似乎学校里的瑞幸咖啡到中午12点就连冰块都卖完了……不得不感叹，每年都有好多人新来到这个园子里，又有好多人离开。</p><p>这学期说实话我选了不少课，专业相关的却很少：《普通心理学》、《神奇的免疫》、《意大利语》、《交响音乐赏析》、《经济学原理》……大四了，在终于摆脱追逐GPA的噩梦以后，我也开始选好玩的东西了。加油吧，希望两周以后我也会继续坚持去上课。</p><p>今年倒也有趣，开学的第一天就是中秋节，真正的上课是从星期二开始的。但这和我们也关系不大，我们在开学第一周就是保研的机试和各种面试。今天和高中同学交流，发现好多同学都在复习考研，也有不少同学在准备出国申请。某种意义上，大四有点像高三：什么时候都有人上岸，就像高三大家会拿到各种怪降分。好消息是，最终等待我们的不是一场大的考试，所以在这个过程中我们可以追逐自己喜欢的内容，对，意大利语……我必不退，身后就是莫斯科了！</p><p>是的。毕业要求大四前最多不修完《培养方案》中的三门课程，我现在有三门课程没修，其中包括限选英语。暨影视欣赏掉课、拉丁语退课、词汇的力量逃跑之后，我不得不考虑，这是不是我本科仅有的机会了。重振英语课荣光，吾辈义不容辞。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8-28总结</title>
      <link href="/ce668245.html"/>
      <url>/ce668245.html</url>
      
        <content type="html"><![CDATA[<p>上次写随笔还是在三周以前，转眼间就过去了快一个月。看了看置顶的更新情况，看看七月，再看看八月，不能说是指数衰减，可以说是log衰减了。</p><p>真要说起来，也没什么好写的，我的暑假：吃、睡、玩、科研，倒也平淡的很。</p><span id="more"></span><p>当然，现在不一样了，小学期答辩完了，暑假也就要结束了。其实我已经返校3天了，今天是第4天。不幸的是，今天我被拉去隔离了，要一直呆到在京满七天。当时是看到涿州出了病例，就急急忙忙跑毒返了校，过了几天。现在毒圈划归了风险区，我也就被拉走了。不过也不亏吧，隔离酒店外的几天，我吃了俄餐、吃了巴奴、吃了外卖、玩了老头环，想做的事情倒也做的差不多了。</p><p>在宿舍的日子短暂而令人回味。学校推行了紫荆码灰码制度，返京不到三天的都是灰码，在学校不能吃食堂，不能进教学楼，只能在宿舍摆烂。倒是挺和我这吸血鬼的生活方式的。</p><p>说起摆烂，这学期我先买了一个音箱，又买了一套乳胶床垫、乳胶枕头。没想到世事无常，快递没到，我先到了隔离点；乳胶床垫没收到，在隔离酒店倒先睡上了乳胶床垫……塞翁失马，焉知非福。隔离酒店不能出屋，饭也是送到门口自己取。让我想起了Mr.Beast一期视频——在屋里每呆一天，送一万美元，看看最多能呆多久。可惜，现在没人给我送钱，不过我只用呆三天。也算是一段难得的经历了，也许？</p><p>新的学期要开始了，二字班真的来了，而且开始军训了，而且昨天还走了20公里拉练。如果我没记错的话，我们当时是走了30公里？拉练一年短似一年，军训的天气倒是一年比一年好。不像我们当时在太阳底下站军姿，今天甚至下了点小雨，凉风里透出些许秋天的气息——最炎热的夏天终于过去了，不过等在前面的，可能是更寒冷的冬天。</p><p>今天先说这么多吧，下学期有下学期的课，秋季有秋季的发布会，我的日子大约还是这么过：像个吸血鬼，也像一尊石像。可能开学以后会更频繁地更新一点，毕竟总结总得有点能写的生活吧。</p><p>以上</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8-3总结</title>
      <link href="/f10a4c12.html"/>
      <url>/f10a4c12.html</url>
      
        <content type="html"><![CDATA[<p>今天不知道为啥很困，中午睡了3个小时的午觉。</p><p>今天吃了味道不错的东北乱炖，买了双鞋。</p><span id="more"></span><p>今日观赛</p><ul><li>WBG打JDG:今天lpl上了12.14版本，小龙的血量和buff效果都提高了。wbg选出了潘森打野，搭中单岩雀、下路机器人。阵容挺好，可惜京东的中野打太好了。被2:0</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>8-2总结</title>
      <link href="/577d47a6.html"/>
      <url>/577d47a6.html</url>
      
        <content type="html"><![CDATA[<p>近期科研进展不大，做了更多实验，看看能不能有效果。</p><span id="more"></span><p>最近观赛：</p><ul><li>T1打GEN.G:T1被2:0，ruler还五杀了，估计这回gumayusi要难受了。说实话，大飞老师被chovy压这么多刀，是不是该好好反思了</li><li>BLG打V5：V5最近因为karsa的合同问题士气低迷，xlb感觉还是没那味。反倒是blg回归了赛季初的经典人员配置，感觉变好了不少，就是大bin老师现在还有点送。总而言之，这样的v5还是能打赢这样的blg，可能blg真要开始备战下个赛季了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-30总结</title>
      <link href="/c0289731.html"/>
      <url>/c0289731.html</url>
      
        <content type="html"><![CDATA[<p>今天回了趟姥姥家，早上起的很早，晚上就很困，所以十点就睡了。</p><p>换了几种方法，效果还是不好，打算溯源一下。</p><span id="more"></span><p>今日观赛：</p><ul><li>JDG打RNG：只看了前两把，感觉两边都有点病</li><li>GEN.G打T1：由于和上面时间冲突，没看，这两天补上</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-29总结</title>
      <link href="/991ad27c.html"/>
      <url>/991ad27c.html</url>
      
        <content type="html"><![CDATA[<p>今日科研，模型效果不好，要想办法改改，又有好多代码要写了w</p><p>今日观赛：</p><ul><li>TT打TES：挺精彩的，团战处理都挺好，tt拉的挺满的，可惜没打过。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-27总结</title>
      <link href="/c925a921.html"/>
      <url>/c925a921.html</url>
      
        <content type="html"><![CDATA[<p>最近一段几天从老家回来了，干劲明显不如前几周高了。暑假就是这样，每过一天，就摆似一天。</p><span id="more"></span><p>虽然回了家，但还是不怎么出门。暑假买了两张拼图，每天拼上几块，不知不觉的竟也拼完了。干活大约也是这样，每天做一些缝缝补补的开发工作和科研工作，就要到8月份了。这真的是我过的最快的一个暑假了，而且还没啥放假的感觉。</p><p>往前一周，虽然没写随笔，日子大约也是这么过。往后一个月，虽然还没到来，不过盼头应该也不多。今天看了个视频“二舅治好了我的精神内耗”，开始怀疑自己是不是每天都在和“精神内耗“作斗争：干活，总有颗摆烂之心在跳动；摆烂，又有内疚之情在蔓延。总而言之，不怎么快乐。</p><p>想想之前几年的暑假，没有这么多的工作，也没有这么多的烦恼，作业刚开始几天赶完。后面一个多月开始心安理得的摆烂：晚起，吃喝，刷视频。老师天天强调所谓“暑假的弯道超车”，自己心里确真的不以为意——这种心态真好啊。快乐又是如何消失的呢？</p><p>我想不明白，可能我需要去问问”二舅“吧。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-20总结</title>
      <link href="/c8297af.html"/>
      <url>/c8297af.html</url>
      
        <content type="html"><![CDATA[<p>今日观赛：</p><ul><li>BLG打RNG：虽然是纯纯的打不过，但呼吸哥还是给了些机会，估计是有点想证明自己。</li></ul><p>另外，今日阅读了IDDPM论文的官方开源代码，收获良多，主要学到以下几点新知识：</p><span id="more"></span><p>抽象类:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod <span class="comment">#支持抽象类和抽象方法</span></span><br><span class="line"></span><br><span class="line">声明<span class="keyword">class</span>时继承ABC</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">xxx</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line"><span class="meta">  @abstractmethod </span><span class="comment">#用修饰符说明是抽象方法</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">yyy</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">xxx_sun</span>(<span class="title class_ inherited__">xxx</span>):</span><br><span class="line">  <span class="comment">#子类继承抽象类的话</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">yyy</span>(<span class="params">self</span>): <span class="comment">#必须要实现每一个抽象方法</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ok&quot;</span>)</span><br></pre></td></tr></table></figure></div><p>枚举器：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> enum</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelMeanType</span>(enum.Enum):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    继承一个枚举类基类的话</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    PREVIOUS_X = enum.auto()  <span class="comment">#在里面声明一些名词</span></span><br><span class="line">    START_X = enum.auto()  <span class="comment">#enum.auto() 自动推断用什么作为枚举元(string,int之类的)</span></span><br><span class="line">    EPSILON = enum.auto()  <span class="comment">#</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">is_pr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self == ModelMeanType.PREVIOUS_X <span class="comment">#可以声明一些判别函数</span></span><br><span class="line">      </span><br><span class="line">target = &#123;</span><br><span class="line">   ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(</span><br><span class="line">   x_start=x_start, x_t=x_t, t=t</span><br><span class="line">   )[<span class="number">0</span>],</span><br><span class="line">   ModelMeanType.START_X: x_start,</span><br><span class="line">   ModelMeanType.EPSILON: noise,</span><br><span class="line">&#125;[self.model_mean_type]</span><br><span class="line"><span class="comment">#后文可以直接用类似于一个Switch的方法，给不同的type声明不同的结果，然后按枚举器实际结果判别</span></span><br></pre></td></tr></table></figure></div><p>字典式参数parser：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_argparser</span>():</span><br><span class="line">    defaults = <span class="built_in">dict</span>( <span class="comment">#主函数中声明字典</span></span><br><span class="line">        data_dir=<span class="string">&quot;&quot;</span>,</span><br><span class="line">        schedule_sampler=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">        lr=<span class="number">1e-4</span>,</span><br><span class="line">    )</span><br><span class="line">    defaults.update(model_and_diffusion_defaults()) <span class="comment">#字典dict可以很方便的添加新元素</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    add_dict_to_argparser(parser, defaults)</span><br><span class="line">    <span class="keyword">return</span> parser</span><br><span class="line">  </span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">add_dict_to_argparser</span>(<span class="params">parser, default_dict</span>):</span><br><span class="line">  <span class="comment">#add函数中把字典里所有元素进行遍历，一个一个加入argparser中</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> default_dict.items():</span><br><span class="line">        v_type = <span class="built_in">type</span>(v)</span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            v_type = <span class="built_in">str</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(v, <span class="built_in">bool</span>):</span><br><span class="line">            v_type = str2bool</span><br><span class="line">        parser.add_argument(<span class="string">f&quot;--<span class="subst">&#123;k&#125;</span>&quot;</span>, default=v, <span class="built_in">type</span>=v_type)</span><br></pre></td></tr></table></figure></div><p>函数传参单*与双*，他们都可以匹配参数个数、名字不确定的情况。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">*args</span>): <span class="comment">#单*相当于把后面的内容解析成tuple</span></span><br><span class="line">   <span class="comment">#函数内部看做一个一个传进来的参数</span></span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> args:</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        </span><br><span class="line">func(*[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])<span class="comment">#如果在调用时传参前面加*，代表解一层递归，再传参</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======&quot;</span>)</span><br><span class="line">func([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">======</span><br><span class="line">([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure></div><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">**kwargs</span>): <span class="comment">#双星**代表把后面的看做一个dict</span></span><br><span class="line">    <span class="built_in">print</span>(kwargs)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        <span class="built_in">print</span>(k, v)</span><br><span class="line"></span><br><span class="line">func(**&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;Spade_&#x27;</span>, <span class="string">&#x27;number&#x27;</span>:<span class="string">&#x27;888888&#x27;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======&quot;</span>)</span><br><span class="line">func(name=<span class="string">&#x27;Spade_&#x27;</span>, number=<span class="string">&#x27;888888&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Spade_&#x27;</span>, <span class="string">&#x27;number&#x27;</span>: <span class="string">&#x27;888888&#x27;</span>&#125;</span><br><span class="line">name Spade_</span><br><span class="line">number <span class="number">888888</span></span><br><span class="line">======</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Spade_&#x27;</span>, <span class="string">&#x27;number&#x27;</span>: <span class="string">&#x27;888888&#x27;</span>&#125;</span><br><span class="line">name Spade_</span><br><span class="line">number <span class="number">888888</span></span><br></pre></td></tr></table></figure></div><p>如果函数声明中有个单独的*，就是一个分隔符，而不是参数，代表后面的参数在调用时都要以<spanclass="math inline">\(x=100\)</span>这种形式写：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_keyvalue</span>(<span class="params">a,b,*,x,y=<span class="number">2</span>,z=<span class="number">3</span></span>):</span><br><span class="line">    <span class="built_in">print</span>(a,b,x,y,z)... </span><br><span class="line">test_keyvalue(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Traceback (most recent call last):  File &quot;&quot;, line 1, in TypeError: test_keyvalue() takes 2 positional arguments but 5 were given</span></span><br><span class="line">test_keyvalue(<span class="number">1</span>,<span class="number">2</span>,x=<span class="number">3</span>,y=<span class="number">4</span>,z=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 1 2 3 4 5</span></span><br><span class="line">test_keyvalue(<span class="number">1</span>,<span class="number">2</span>,x=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 1 2 3 2 3</span></span><br><span class="line">test_keyvalue(<span class="number">1</span>,<span class="number">2</span>,x=<span class="number">3</span>,z=<span class="number">8</span>,y=<span class="number">9</span>)</span><br><span class="line"><span class="comment"># 1 2 3 9 8</span></span><br><span class="line">test_keyvalue(x=<span class="number">3</span>,z=<span class="number">8</span>,y=<span class="number">9</span>,a=<span class="number">3</span>,b=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 3 5 3 9 8</span></span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 探索 </tag>
            
            <tag> 日记 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-18总结</title>
      <link href="/b1e2de2b.html"/>
      <url>/b1e2de2b.html</url>
      
        <content type="html"><![CDATA[<p>今天晚上出门了一趟，看了看夜景</p><ul><li>天色很黑。感觉北京的天空都被各种高楼的装饰灯打亮了，没有了晚上的感觉</li><li>路边有很多排挡，还有摆在外面的桌子，和小摊。记得小时候都是这样，后面北京都把小摊撤了，生活气息淡了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Improved Denoising Diffusion Probabilistic Models</title>
      <link href="/8467c46.html"/>
      <url>/8467c46.html</url>
      
        <content type="html"><![CDATA[<p>这篇论文探索了DDPM对于NLL指标效果不好的原因，并且从实际训练的角度给出了很多可行的改进。</p><span id="more"></span><p><img src="../files/images/IDDPM/author.png"></p><p>这篇论文是21年的NIPS，作者来自OpenAI，其实就是后面GLIDE的作者。我理解大概是OpenAI看到了DDPM的论文，然后用”财大气粗“的方式来了一波复现和改进。这篇论文其实更偏向于分析性文章。另外，这篇论文的方法的代码和复现性很好。&gt; 发现一个讲IDDPM代码的是<ahref="https://www.bilibili.com/video/BV1sG411s7vV">视频</a></p><p>本文大概探索了这个几个问题：</p><ul><li>DDPM的训练object</li><li><span class="math inline">\(\beta_t\)</span>的选取</li><li>训练的采样方法</li><li>DDPM模型的可扩展性</li></ul><h2 id="denoising-diffusion-probabilistic-models">Denoising DiffusionProbabilistic Models</h2><p>这里重新梳理了一遍DDPM，我就说几个新颖的地方</p><h3 id="definition">Definition</h3><p>作者写出了<span class="math inline">\(L_{VLB}\)</span>的形式</p><p><img src="../files/images/IDDPM/vlb.png"></p><p>可以看出中间的项都是KL散度，可以通过前后两个高斯分布的均值和方差直接计算出来:</p><ul><li>正常的DDPM模型<spanclass="math inline">\(p_\theta\)</span>模型的均值由模型得出，而方差是确定性的</li><li><span class="math inline">\(q(x_{t-1} | x_t,x_0)\)</span>的均值方差均可以被代数表示</li></ul><p><img src="../files/images/IDDPM/q.png"></p><h3 id="training-in-practice">Training in Practice</h3><p>这一部分，作者说到了三种模型目标：</p><ul><li>生成 <spanclass="math inline">\(\mu_\theta(x_t,t)\)</span>，预测<spanclass="math inline">\(x_{t-1}\)</span>的均值</li><li>预测<spanclass="math inline">\(x_0\)</span>,采样时根据公式10线性拟合出<spanclass="math inline">\(x_{t-1}\)</span>进行去噪</li><li>预测加噪时的噪声<spanclass="math inline">\(\epsilon\)</span>,去噪根据下式和公式10拟合出<spanclass="math inline">\(x_{t-1}\)</span></li></ul><p><span class="math display">\[x_0 = \frac{1}{\sqrt{\alpha_t}}\left( x_t - \frac{\beta_t}{\sqrt{1 -\overline{\alpha}_t}} \epsilon \right)\]</span></p><p>DDPM在训练过程中使用最后一种，表现最好</p><p>作者把<spanclass="math inline">\(L_{VLB}\)</span>进行了一波reweight，得出下式作为训练loss进行训练<span class="math display">\[L_{\text{simple}} = \mathbb{E}_{t,x_0,\epsilon} || \epsilon -\epsilon_\theta(x_t,t)||^2\]</span></p><p>同时，作者说到在predict阶段，使用下面两种方差效果很接近</p><ul><li><span class="math inline">\(\sigma_t^2 = \beta_t\)</span></li><li><span class="math inline">\(\sigma_t^2 = \tilde\beta_t\)</span></li></ul><h2 id="improving-the-log-likelihood">Improving the Log-likelihood</h2><p>这里作者提到Log-likelihood这个指标在图像生成也很重要，但DDPM在这个指标的效果不好(虽然FID不错)。</p><h3 id="learning-sigma_thetax_tt">learning <spanclass="math inline">\(\Sigma_\theta(x_t,t)\)</span></h3><p>作者思考为什么DDPM的两个方差效果接近</p><p><img src="../files/images/IDDPM/beta.png"></p><ul><li><p>图一可以发现两种方差只在<spanclass="math inline">\(t\)</span>很小的时候有差距，别的时候基本一致</p></li><li><p>图二可以发现<spanclass="math inline">\(t\)</span>小的时候对NLL的影响是最大的</p></li></ul><p>综合上面观点，作者觉得NLL效果差的重要原因就是对方差的估计有偏差(上面两种<spanclass="math inline">\(\sigma\)</span>都是估计的形式)。因此作者想要找到更好的方差，作者希望通过网络拟合它，看做两种<spanclass="math inline">\(\sigma\)</span>的拟合 <spanclass="math display">\[\Sigma_\theta(x_t,t) = \exp (v \log \beta_t + (1-v)\log \tilde\beta_t)\]</span>其中v是可学习的参数。作者在这里没有对v的范围做限制，但实际训练还是控制在0-1内。说明DDPM作者选取的两个估计还是很靠谱的</p><p>想要学习这个v，就只能把loss直接表示成<spanclass="math inline">\(L_{VLB}\)</span>,才能反向传播到v，因此作者选择了最终的loss函数<span class="math display">\[L_{\text{hybird}} =  L_{\text{simple}} + \lambda·L_{VLB}\]</span> 其中<span class="math inline">\(\lambda=0.001\)</span></p><h3 id="improving-the-noise-schedule">Improving the Noise Schedule</h3><p>这一部分，作者探索了<spanclass="math inline">\(\beta\)</span>的选取</p><p><img src="../files/images/IDDPM/cosine.png"></p><p>DDPM使用linear来选取，作者采用新的cosine的公式选取 <spanclass="math display">\[\overline\alpha_t = \frac{f(t)}{f(0)},\quad f(t) = \cos \left(\frac{t/T+ s}{1+s}·\frac{\pi}{2} \right)^2 \\\beta_t = \min(0.999,1 -\frac{\overline\alpha_t}{\overline\alpha_{t-1}})\]</span>这中选取方式比linear更稳定。同时作者指出，类似cosine的这种形状的，最后效果都差不多</p><h3 id="reducing-gradient-noise">Reducing Gradient Noise</h3><p>第三个改进是让训练更稳定</p><p><img src="../files/images/IDDPM/stable.png"></p><p>作者发现直接优化<spanclass="math inline">\(L_{VLB}\)</span>的训练非常不稳定，比起优化<spanclass="math inline">\(L_{simple}\)</span>。作者发现这是由于<spanclass="math inline">\(L_{VLB}\)</span>参数对梯度更分散。</p><p>进一步分析，发现是在训练时平均的选取t导致训练额外的噪声，实际上的选取应该是：<span class="math display">\[L_{VLB} = \mathbb{E}_{t \sim p(t)} \left[ \frac{L_t}{p_t}  \right],\text{where}\quadp_t \propto \sqrt{\mathbb{E}[L_t^2]}, \quad \sum p_t = 1\]</span> 对于每个t，在训练过程中都可以维护<spanclass="math inline">\(L_t\)</span>，只要训练开始一段时间以后。</p><ul><li>先平均选取t，直到每个t都有10个<spanclass="math inline">\(L_t\)</span>的历史数据为止</li><li>接下来，按照前10个历史数据的平均作为目前<spanclass="math inline">\(L_t\)</span>情况的估计，进而带入公式来采样下一个t的选取</li></ul><p>最终的训练变得很稳定，如上图的绿线</p><h2 id="improving-sampling-speed">Improving Sampling Speed</h2><p>这一部分提到了可以加速采样</p><p><img src="../files/images/IDDPM/speedup.png"></p><p>这里主要对比的是DDIM，方法也差不多：采样一个<spanclass="math inline">\(1,2,...T\)</span>的长度为K子序列进行sample</p><p>可以发现，DDIM的没有IDDPM好：</p><ul><li>DDIM没有做IDDPM里这些改进。</li><li>作者提到没法做改进，这是因为DDIM的公式里的方差项没了</li></ul><p>## Scaling Model Size</p><p><img src="../files/images/IDDPM/scale.png"></p><p>这一部分是分析DDPM模型在加大size以后会不会变得更好，因此作者直接加参数</p><ul><li>FID得分和模型的参数(计算量)基本呈线性关系。满足powerlaw，因此扩大规模是个好选择</li><li>NLL得分不满足线性关系，扩大规模效果不好，可能原因有：<ul><li>很快过拟合</li><li>loss不稳定</li><li>作者没有直接优化降低NLL用的<spanclass="math inline">\(L_{VLB}\)</span>，而是一个<spanclass="math inline">\(L_{\text{hybrid}}\)</span></li></ul></li></ul><h2 id="我的思考">我的思考</h2><ul><li>总体而言，这篇文章更像是分析性文章，复习笔记。主要方法还是DDPM。</li><li>OpenAI研究员解决问题的眼光真的很高明呀，尤其是关于改变t选取和分析scale能力这一部分。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-17总结</title>
      <link href="/47aaaec2.html"/>
      <url>/47aaaec2.html</url>
      
        <content type="html"><![CDATA[<p>今天回老家，早上到的，车上有点失眠，难受。中午吃了好吃的萝卜清炖羊肉。预计呆一周，后面可能还要出去大吃大喝。</p><p>另外，明天要开始写代码了，代码神保佑。</p><span id="more"></span><p>最近观赛：</p><ul><li>TES打EDG：在火车站看的前两把，最后一把登车没看成。tes让1追2，感觉最近Tian状态真好呀，只要稳住心态应该还是很强的</li><li>V5打WBG：属于是兄弟lol阋墙战，估计大家都没想到WBG能赢，还是在红色方打了三把，终结了V5的连胜梦。SHY哥今天好猛啊，简直跟换人了一样。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-AUTOREGRESSIVE DIFFUSION MODELS</title>
      <link href="/4a98dda1.html"/>
      <url>/4a98dda1.html</url>
      
        <content type="html"><![CDATA[<p>这篇讲了一种借鉴Discrete Diffusion model来建模一个order-agnosticautoregressive模型的方法，在text8数据集取得不错的效果。</p><span id="more"></span><p><img src="../files/images/ARDM/author.png"></p><p>这篇论文发表在ICLR22上，作者来自google brain。</p><h2 id="background">background</h2><p>首先梳理几个概念</p><h3 id="arm">ARM</h3><p>自回归数据集里的每句话: <span class="math display">\[\log p(x) = \sum_{t=1}^D \log p(x_t | x_{&lt;t})\]</span>用一个上三角矩阵把每句话mask住做D次predict就能算出模型对本次数据的loss</p><h3 id="order-agnostic-arms">Order Agnostic ARMs</h3><p>XLNet采用这个结构，就是说模型不是从左到右预测的，而是按照任何一个顺序都可以:<span class="math display">\[\begin{aligned}\log p(x)  &amp; = \log \mathbb{E}_{\sigma \sim \mathcal{U}(S_D)}\sum_{t=1}^D p(x_{\sigma(t)} | x_{\sigma(&lt;t)})\\&amp; \geq \mathbb{E}_{\sigma \sim \mathcal{U}(S_D)} \sum_{t=1}^D \logp(x_{\sigma(t)} | x_{\sigma(&lt;t)})\end{aligned}\]</span> 这个公式里，<spanclass="math inline">\(\sigma\)</span>是对<spanclass="math inline">\(1,2,...D\)</span>的一个重排。大概就是给定一个重排，按照这个重排的前一部分信息，生成剩下的每一个位置<spanclass="math inline">\(\sigma(t)\)</span>的量。</p><p>式子中的不等式来自杰森不等式</p><blockquote><p>从左到右可以看做一种特定的重排。上面式子相当于对所有重排的总的loss的期望，训练时任选一个重排就行</p></blockquote><h3 id="discrete-diffusion">Discrete Diffusion</h3><p>这里引用的是21年的论文,大概就是把MLM视为某种diffusionmodel，是文本领域diffusion model的一种解法</p><h2 id="autoregressive-diffusion-models">AUTOREGRESSIVE DIFFUSIONMODELS</h2><h3 id="method">method</h3><p>这一部分讲本文的方法</p><p>上来就是loss的公式计算，作者用了不同的推导方式： <spanclass="math display">\[\begin{aligned}\log p(x)  &amp; \geq \mathbb{E}_{\sigma \sim \mathcal{U}(S_D)}\sum_{t=1}^D \log p(x_{\sigma(t)} | x_{\sigma(&lt;t)}) \\&amp; = \mathbb{E}_{\sigma \sim \mathcal{U}(S_D)} D \times \mathbb{E}_{t\sim \mathcal{U}(1,2,..D)} \log p(x_{\sigma(t)} | x_{\sigma(&lt;t)}) \\&amp; = D \times \mathbb{E}_{t \sim \mathcal{U}(1,2,..D)}\mathbb{E}_{\sigma \sim \mathcal{U}(S_D)} \frac{1}{D-t+1} \sum_{k\in\sigma(&gt; t)} \log p(x_{k} | x_{\sigma(&lt;t)})\end{aligned}\]</span> 这个公式有几个特点：</p><ul><li>他进一步考虑了XLNet中没有考虑的时间t</li><li>他把”重排中剩下位置“当成了”很多个原子的生成过程的并行“的形式</li></ul><p>具体而言他定义了：</p><p><img src="../files/images/ARDM/equ.png"></p><p>上面的式子就比较好理解了：<spanclass="math inline">\(\mathcal{L}_t\)</span>的计算只和<spanclass="math inline">\(t,\sigma\)</span>有关，可以视为把句子mask住一些位置，然后预测剩下的每一个位置的loss之和(BERTloss之和)，再加上一个正则量</p><p>具体训练中，作者直接使用transformer结构，然后就直接用下面的算法进行训练和预测：</p><p><img src="../files/images/ARDM/algo.png"></p><p>其中：</p><ul><li>m是一个布尔向量，代表重排<spanclass="math inline">\(\sigma\)</span>的前t-1个位置是1</li><li>m是一个布尔向量，代表重排<spanclass="math inline">\(\sigma\)</span>的第t个位置是1</li><li><spanclass="math inline">\(\bigodot\)</span>是指和布尔向量乘，布尔0位置置为0</li><li><spanclass="math inline">\(\mathcal{C}\)</span>是一个cat分布，代表以<spanclass="math inline">\(f(m\bigodotn)\)</span>的概率分别取K(词表大小)个值中的一个</li></ul><p>对于NLP而言，可以直接设定一个词表K外的词K+1代表MASK，wordembedding为a,然后就能在代码上比较统一: <span class="math display">\[\text{mask}(x,m) = m \bigodot x + (1-m)\bigodot a\]</span></p><h3 id="parallelized-ardms">PARALLELIZED ARDMS</h3><p>这一部分，作者分析了上面公式中的并行性的特点带来更快的解码速度。因为如果有t个位置的结果以后，解码剩下的位置都是并行的，</p><p>如果假设<span class="math inline">\(L_{t,t+k} =\sum_{i=1}^k\mathcal{L}_{t+i} \approx k\mathcal{L}_t\)</span>,视为解码所需要的代价</p><p>那么用一个动态规划算法，就能算出使得代价最小的并行化解码方法(固定解码步骤<spanclass="math inline">\(c\)</span>)。下图就是把一个20步的解码只用5步来完成</p><p><img src="../files/images/ARDM/dp.png"></p><h2 id="experiment">Experiment</h2><p>这一部分作者，用bpc的NLLloss来对比了作者的方法OA-ARDM,和之前的离散diffusion方法<spanclass="math inline">\(D3PM-*\)</span>，所有模型用的结构是一样的，只有训练方法和采样方法不一样</p><p><img src="../files/images/ARDM/result.png"></p><ul><li>可以看到主试验的结果基本完胜 Austin 21的方法。</li><li>值得一题的是，效果没有从左到右的auto-regressive好(一个普通transformerbaseline结果是1.35)</li><li>另一方面就是新的结果由于比较好的并行性，只用20步的解码就可以达到之前大概250步的结果</li><li>实验用的数据集是text8，解码的例子如下：</li></ul><p><img src="../files/images/ARDM/example.png"></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>运行30天情况</title>
      <link href="/980d63fc.html"/>
      <url>/980d63fc.html</url>
      
        <content type="html"><![CDATA[<p>从6.16日到现在，正好过去了1个月的时间。在过去一个月中，一共更新了45篇，有效输出15篇，共7万字。</p><p>论文阅读笔记方面，这一套流程实际感受下来就是</p><span id="more"></span><ul><li>好像有一股力量在监督你把今天的论文读了。</li></ul><p>不得不承认，找到好的论文是最重要的，但找到以后提起精神读下去也很重要。从这一个月的感受来看，反馈比较正面，后天应该可以一直坚持下去。</p><p>另一方面，现在是暑假阶段，因此课程方面的内容没有更新，可能还得等开学以后再观察课程笔记的反馈情况。</p><p>最后，赛事观察方面，感觉还是停留在“吐槽”的阶段，不过这样也挺好，毕竟本身也没有bp、意识的基础和水平。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-ON THE UTILITY OF GRADIENT COMPRESSION IN DISTRIBUTED TRAINING SYSTEMS</title>
      <link href="/a9f8b29.html"/>
      <url>/a9f8b29.html</url>
      
        <content type="html"><![CDATA[<p>MLSys22中编译优化方向的论文都看完了，今天看一个别的主题，分布式训练。这篇文章是一篇分析性文章，讲了分布式训练中梯度压缩算法在实际场景中的效果不好，分析原因，并且提出了一个和真实场景效果接近的performancemodel。</p><span id="more"></span><p><img src="../files/images/compression_performance/author.png"></p><p>作者来自华盛顿大学和CMU。</p><h2 id="introduction">INTRODUCTION</h2><p>这个introduction很长，足足三页，分析性文章是这样的。</p><p>首先作者讲了Synchronous data parallel stochastic gradient descent(SGD)方法，简称sync-SGD，是分布式训练中的一个方法，分为两个部分：</p><ul><li>gradient computation：梯度的计算，不同node的时间是串联的。</li><li>gradientaggregation：这一部分需要收集不同的node算出的梯度，需要node间的通信。目前的瓶颈在这，通信带宽。</li></ul><p>针对上面的瓶颈有几种解决办法：</p><ul><li><p>lossy gradientcompression：把loss传输前先压缩一下。需要和深度学习框架交互，会带来额外的一些超参数</p></li><li><p>system-leveloptimizations：是说把梯度组织成tree之类的优化，和深度学习库非常耦合，时间复杂度和node数成<spanclass="math inline">\(\mathcal{O}(n),\mathcal{O}(\logn)\)</span>的关系</p></li><li><p><em>overlap</em> the gradient computation and communicationphases：这是说把上面sync-SGD的两个阶段并行的计算。第一个node计算完就可以开始第一个node的梯度汇总。这个方法效果很好，并且对用户完全透明，不需要用户加超参。如下图：</p></li></ul><p><img src="../files/images/compression_performance/merge.png"></p><p>本篇工作重点在于评估上面的不同方法的效果：在96个GPU的集群上，对上面的方法进行超过200种setting的测试。</p><p>作者发现了下面几个结论：</p><ul><li><em>limited opportunity for gradientcompression</em>：梯度压缩效果不好。<ul><li>一方面，这压缩、解压缩时间太长。正常的通信要200ms，压缩解压缩要50ms，能力有一个比较差的上界。</li><li>另一方面，不能真正利用硬件的并行性。因为梯度计算，梯度压缩都要GPU，有竞争关系。大batchsize效果好，是因为某种意义上隐藏了这个竞争消耗的时间。</li></ul></li><li>all-reducecompression方法在扩大规模时表现更好。all-reduce定义见后文</li></ul><p>同时，为了研究者可以在理论上分析算法的好坏，作者设计了一个performancemodel，和实际效果很接近。</p><ul><li>一个结论是：比起提高压缩率，压缩总体时间更重要。因为带宽也没有很低，即使对于大模型，4x压缩率就够了。</li></ul><p><img src="../files/images/compression_performance/performance_model.png"></p><h2 id="background-and-related-work">BACKGROUND AND RELATED WORK</h2><h3 id="gradient-compression">Gradient Compression</h3><p>这一部分说了很多压缩算法，作者指出用来评测几个算法是：</p><ul><li>quantization based SIGNSGD</li><li>low-rank decomposition based POWERSGD</li><li>sparsification based MSTOP-K</li></ul><h3 id="system-advances">System Advances</h3><p>这里作者梳理了几个概念</p><h4 id="all-reduce">All-reduce</h4><p>题外话，先说一下分布式训练的几个概念。分布式训练是把模型或者batch分布到不同节点上，因此每batch跑完后需要通讯</p><ul><li><p>broadcast：是说将根服务器的是所有数据发给所有服务器</p></li><li><p>scatter：是说一次发太大，带宽瓶颈。把自己的数据分成几个批，每批分别发给一个服务器</p></li><li><p>gather：每个服务器都获得了其他服务器上的一个数据块，直接拼接到一起。</p></li><li><p>allgather：所有服务器都做gather，都获得了全部服务器的数据。</p></li><li><p>reduce：和gather相对。是说一个服务器收到数据时，不是去拼接原来的，而是做一个操作(比如相加)</p></li><li><p>AllReduce：每个服务器都做reduce。</p></li></ul><p>分布式训练大概就是每个node持有一个mini-batch，算出自己部分的梯度后broadcast给其他节点，其他节点收到以后做reduce。最后通过一个allreduce让所有节点都有整个batch的总的梯度，并且不需要额外的存储(显存、内存)</p><p>作者提到，现有的这些allreduce算法都是在latency和bandwidth间做权衡。如下图：</p><p><img src="../files/images/compression_performance/compare.png"></p><p>值得一体的是，ringreduce方法的bandwidth花费时间和节点数p基本没关系</p><p>后面，作者提到了all-reduce算法的几种优化，比如前面提到的overlapping和bucketgradient。后者是缩减all-reduce的函数调用时间，用一些cache优化。</p><h2 id="evaluating-gradient-compression">EVALUATING GRADIENTCOMPRESSION</h2><p>后面四页都是在说做了什么实验，由于比较多，就不一个一个讲了，就说说结论。</p><p>所有的实验都是在一个64块V100的集群上进行的</p><p><img src="../files/images/compression_performance/allreduce.png"></p><p>上面的图说明all-reduce效果显著好于其他方法</p><p><img src="../files/images/compression_performance/scale.png"></p><p>上面这个图是说对比之下，随着node的上升，压缩算法的优化逐渐变得稳定(不随node的增加而增加)。其原因就是在上面提到的</p><h2id="identifying-regimes-of-high-gradient-compression-utility">IDENTIFYINGREGIMES OF HIGH GRADIENT COMPRESSION UTILITY</h2><h3 id="performance-model-for-distributed-data-parallel">PerformanceModel for Distributed Data Parallel</h3><p>这一部分就是在讲作者的performance model，公式如下： <spanclass="math display">\[T_{obs} \approx \max \left[\lambda T_{comp}, (k-1) T_{comm}(b,p,BW) +T_{comm}(\hat{b},p,BW)\right]\]</span></p><ul><li>模型可以分为k个bucket，每个前k-1个的大小是b，最后余出来的大小是$</li><li><span class="math inline">\(T_{obs}\)</span> 实际需要时间</li><li><spanclass="math inline">\(T_{comp}\)</span>单个node的backward计算时间</li><li><span class="math inline">\(BW\)</span>带宽</li><li><span class="math inline">\(T_{comm}\)</span>，通讯时间</li><li><spanclass="math inline">\(\lambda\)</span>,由于通讯和计算之间的延迟，带来的固有的下界，观测值大约是1.04-1.1</li></ul><p>举例来说，比如ring-reduce的话，<spanclass="math inline">\(T_{comm}\)</span>可以具体化为 <spanclass="math display">\[T_{comm}(b,p,BW) = 2\alpha \times(p-1) + 2\times b\times\frac{p-1}{p\times BW}\]</span> 剩下的算法可以根据上面提到的复杂度进行计算</p><p>接下来，作者分析了上面的模型和实际观测时间的准确性，发现基本一致</p><p>作者提到了现有performance模型的缺陷：</p><ul><li>没有考虑model太大导致必须分布到不同node的情况</li><li>没考虑异步。假设同步只发生在每个iteration之后</li></ul><h3 id="insights-from-the-performance-model">Insights from thePerformance Model</h3><p>从上面的 performance model，我们可以看出</p><ul><li>压缩率不一定要很高，基本上4x就已经很足够了</li><li>压缩算法只有在带宽很低(&lt;8Gbps)效果好</li></ul><h2 id="思考">思考</h2><ul><li>我在想，这篇文章之所以能成功，是不是因为只有他们拿出了64张v100，别人没这么多，所以分析不了”node大的情况下压缩算法的表现“……</li><li>作者提到的这个trade-off的现象，很有道理：探索在目前带宽逐渐增大的情况下压缩算法的未来演进方向确实很有意义</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 分布式训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-TORCH.FX: PRACTICAL PROGRAM CAPTURE AND TRANSFORMATION FOR DEEP LEARNING IN PYTHON </title>
      <link href="/824812d3.html"/>
      <url>/824812d3.html</url>
      
        <content type="html"><![CDATA[<p>我理解这篇论文就是torch.fx的论文，作者是站在设计torch.fx的角度思考“我们为什么要这么做”，把他们的一系列实现整理成了论文发了出来。</p><span id="more"></span><h2 id="摘要">摘要</h2><p>摘要部分就是说很多python框架虽然使用eagerexecution提升了易用性，但是在真实的落地场景中，用户其实希望对模型做性能优化，可视化，分析和硬件调优。为了满足这个gap，作者设计了Torch.fx，一个纯python的程序捕捉和变换的库。</p><blockquote><p>Eagerexecution:其实就是define-by-run，可以让用户直接用high-level编程语言去定义计算图，可自动进行求导，进而训练和预测。非常方便开发</p></blockquote><h2 id="introduction">Introduction</h2><p>先区分了以下两种模式：</p><ul><li>define-and-run：早期的架构，定义很多API。用户来操作这些api去构建一些图的IR，进而框架去进行图优化、算子融合等编译优化。问题在于，用户需要同时懂hostlanguage和一种运行时的domain-Specific的语言，后者尝尝很麻烦。比如python的pdbdebug系统。</li><li>define-by-run（eager mode）：PyTorch或者TensorFloweager等支持的。直接去翻译高层语言，去支持自动求导等过程，方便开发。但这时，就只能用一些just-in-time的编译方法，而不是ahead-of-time里有的很多算子融合等技术，因此效果不如上面的好。</li></ul><p>事实上，只是使用已知的计算图的一些IR结构就能方便的做一些图优化的手段。想要使用上他们，就需要一个eagermode框架也可以去捕捉程序结构，进而获取上面的信息。</p><p>其实，TorchScript通过捕捉python程序所有的AST是可以做这件事的。但问题在于，它捕捉所有的程序，cost太大，其实我们只需要捕捉计算图(nn.module)。因为优化手段基本上都局限在DAG图的范围内，因此这个捕捉手段是可以化简的。</p><p>因此作者实现了torch.fx,聚焦于dag图的表示，设计了一系列用户接口来让图满足这种格式，同时对各种图优化进行了支持</p><ul><li>分析了图优化之于AI的重要性</li><li>实现了纯python的满足上述条件的库</li><li>一个只有6条指令的IR，便于表示dag图</li><li>可以进行图变化，进而生成优化后的代码返回给host language</li><li>分析了torch.fx的使用实例</li></ul><h2 id="background">BACKGROUND</h2><p>这一部分，提出了eager-mode的程序需要的几个要素</p><h3 id="capturing-program-structure">Capturing Program Structure</h3><p>这一部分对比了已有的框架们获取程序表示的几种方式：</p><ul><li>trace每一个操作的情况，需要用户给出样例输入。PyTorch’sjit.trace采用。</li><li>trace每个的抽象的值而不是样例输入，不用用户给出样例输入。Tensor-Flow’s tf.function采用<ul><li>上两种的问题都是，只能支持python功能的一个子集，并且错误的追踪不精确</li><li>另一个问题是只能用一些定义的API，而不是直接的python语言做开发。</li></ul></li><li>允许让用户直接用embedded programming language开发，如TorchScript。<ul><li>问题是这种框架实现特别复杂，而且，还是只能支持python功能的一个子集(比上面更大的子集)</li></ul></li><li>一些框架提供python到别的语言的接口，然后用别的语言的方式做优化，如Swiftfor TensorFlow。<ul><li>问题是需要退出python生态。但这个生态，包含里面的各种库，只有python有好的实现。</li></ul></li></ul><h3 id="specializing-programs">Specializing Programs</h3><p>这一部分讲特化，其实是关于shape的定义。 <span class="math display">\[a + b\]</span>这个表达式在python里可以后推断各种type的输入，但对于计算图它需要比较确定。</p><ul><li>PyTorch’s jit.trace：只支持固定形状(样例输入)的输入，别的就炸了</li><li>LazyTensor：可以just-in-time进行tracing。更灵活，但由于每次输入需要重新trace，代价太大</li><li>JAX’s jitcombinator：每次重新capture不是必要的，可以根据输入决定是否需要重新tracing。但可能会导致无法预测的各种运行时的问题。</li></ul><h3 id="intermediate-representation-design">Intermediate RepresentationDesign</h3><p>这一部分讲IR的设计，总体而言，IR复杂，会使得优化效果更好，但也更难实现。</p><ul><li><p>Language：有些框架是跨编程语言的，比如PyTorch’s JIT andMXNet用c++作为他们的datastructure。运行时表现更好，更好序列化，但是需要用户在开发python时有额外的学习成本</p></li><li><p>Control flow：很多network不需要controlflow，只要一系列if-statements or loops操作，称作basic blockprogram。</p><ul><li><em>basic block</em>program可以直接表示成一个DAG图，MLP，CNN，transformer都符合这个情况</li><li>RNN不符合，因为什么时候停止需要动态推断。因此每一次state的内部都是一个basicblock，总体有一个控制流</li></ul></li><li><p>State：这个state其实就是指模型参数。以为pytorch支持aliasing和mutation的语法，因此需要检查对state的操作是合法的。</p><ul><li>TorchScriptIR支持别名分析、指代分析，对IR做变换。代价很大，因为每一个表达式都要计算别名、指代消解。而且很多函数可能有改全局变量这种阴间操作。这种方法会降低优化的能力，但用户友好。</li><li>JAX使用一些别的框架来做这件事，比如把模型用FLAX封装一遍。任何一种变换都比较复杂，因为需要跨框架的交互。</li></ul></li></ul><h2 id="design-principles">DESIGN PRINCIPLES</h2><p>根据上面的分析，提出了torch.fx的几个设计理念：</p><ul><li>更关注于已有的、经典的模型，不去执着于long-tailed、复杂的实现</li><li>更多使用开发者熟悉的python的数据结构、已支持的算子</li><li>让程序的捕获具有灵活性，方便用户去实现的自己的long-tailed需求</li></ul><h2 id="torch.fx-overview">TORCH.FX OVERVIEW</h2><p>前面作者的思考和分析讲完了，这一部分讲作者到底怎么设计。</p><p>不在捕捉的时候特化计算图，而是在优化的时候做。</p><p><img src="../files/images/torchfx/transformation.png"></p><p>一个变换的例子如上图，直接用python语言书写图变换的方法</p><h3 id="capture">capture</h3><ul><li>GraphModule是 torch. nn. Module 的子类，其 forward方法运行捕获的Graph。我们可以打印此图的Nodes以查看捕获的 IR。</li><li>placeholder节点表示输入，单个output节点表示Graph的结果。</li><li>call_function 节点直接引用了它将调用的 Python 函数。</li><li>call_method节点直接调用其第一个参数的方法。</li><li>Graph被重组为 Python 代码（traced.code）以供调用。</li></ul><p>捕捉的例子如下：</p><p><img src="../files/images/torchfx/capture.png"></p><h3 id="ir">IR</h3><p>设计了只有6个语句的IR，非常简单</p><ul><li><p>torch. fx 的中间表示（IR）由一个Python数据结构Graph来做的。</p></li><li><p>这个Graph实际上是一个包含一系列Node的线性表。</p><ul><li>节点有一个字符串操作码opcode，描述节点代表什么类型的操作（操作码的语义可以在附录A.1 中找到）。</li><li>节点有一个关联的目标，它是调用节点（call_Module、call_function 和call_method）的调用目标。</li><li>节点有 args 和 kwargs，在trace期间它们一起表示 Python调用约定中的目标参数（每个opcode对应的 args 和 kwargs 的语义可以在附录A.2 中找到）。</li><li>节点之间的数据依赖关系表示为 args 和 kwargs中对其他节点的引用。</li></ul></li><li><p>torch. fx 将程序的状态存储在 GraphModule 类中。</p><ul><li>GraphModule 是转换程序的容器，暴露转换后生成的代码，并提供 nn.Module 类似的参数管理APIs。</li><li>GraphModule 可以在任何可以使用普通的 nn. Module的地方使用，以提供转换后的代码和PyTorch生态系统的其余部分之间的互操作性。</li></ul></li></ul><h3 id="source-to-source-transformation">Source-to-SourceTransformation</h3><p>transformation的最后一步是重新从IR翻译回python语言，而不是到其他生态系统。同时也可以继续进行transformation。</p><h2 id="case-studies-and-evaluation">CASE STUDIES AND EVALUATION</h2><p>大概就是说，torch.fx是做IR抽象还原的，所以IR很重要，比较了一下IR可读性、简便性。</p><p><img src="../files/images/torchfx/read.png"></p><p>左右一对比，确实可读性、简便性胜出。</p><p>还做了个实验比较效果，比如算子融合之类的，发现确实都可以做。</p><p><img src="../files/images/torchfx/fusion.png"></p><h2 id="思考">思考</h2><ul><li>感觉这个就是让工程师可以很方便的“用python优化python”，可以自己定义自己的pass，还能非常方便的再变换回python语言</li><li>我其实有点好奇这个东西的作用在哪里，后来看了一下是方便开发者可以直接把搞好的模型出来做python-python的转换：<ul><li>比如把所有的op都换一下</li><li>比如不改原始代码，直接提取pretrainmodel的中间层的输出来做下游任务</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 深度学习编译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-13总结</title>
      <link href="/b3e58ad1.html"/>
      <url>/b3e58ad1.html</url>
      
        <content type="html"><![CDATA[<p>今日读两篇论文，遗憾的是，没有编译方面的。</p><p>明天就要进度同步了，show me your work! 不能这样下去了，现在就赶w</p><p>据了解，今日时隔一个月召开组会，又感觉到了上进的气息。</p><span id="more"></span><p>今日观赛：</p><ul><li>BLG打LGD：BLG第一把红色方被锤烂第二把火速把ICON太回饮水机后连下两城。但BIN满血刀妹想单杀纳尔被塔里反杀只能说，失智攻击。估计这周饭堂稳了。BLG换了个二队ad感觉还没有doggo打的好呀……那个人还会回来吗（</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Diffusion models beat gans on image synthesis</title>
      <link href="/99193096.html"/>
      <url>/99193096.html</url>
      
        <content type="html"><![CDATA[<p>这篇工作是DiffusionModel关注度高起来的第二篇重要文献。在此之前，DDPM证明Diffusionmodel可以生成diversity，但score上，比起“专门造假”的GAN还是略显不足，但OpenAI这片新作，证明了Diffusionmodel有实力生成比GAN优秀的结果。</p><span id="more"></span><p><img src="../files/images/BEAT_GAN/author.png"></p><p>作者在Introduction部分先分析了之前GAN比DiffusionModel好的两个原因：</p><ul><li>GAN使用的模型结构被探索的更完善，这点可能会随着DiffusionModel的关注度提升而解决。作者在本篇工作探索了UNet结构的几种改进对DDPM的影响</li><li>GAN更好的满足忠诚度。因此作者想要trade-offdpm的diversity和忠诚度，这也是本文的中心思想。</li></ul><p>在Background部分作者描述了一下DDPM的思路，这里就略过了。</p><h2 id="improvements">Improvements</h2><p>作者这里的说的是使用iDDPM的方法，学习采样时的方差: <spanclass="math display">\[\Sigma_\theta(x_t,t) = \exp(v \log \beta_t + (1-v)\log \hat{\beta}_t)\]</span></p><p><span class="math display">\[\hat{\beta_t} := \frac{1 - \overline{\alpha}_{t-1}}{1 -\overline{\alpha}_{t}} \beta_t \\\]</span></p><p>v是学习的参数<span class="math inline">\(M*N*T\)</span></p><p>当然，我们之前讲的论文已经证明这也是负优化了</p><a href="/babb0879.html" title="论文阅读[精读]-ANALYTIC-DPM: AN ANALYTIC ESTIMATE OF THE OPTIMAL REVERSE VARIANCE IN DIFFUSION PROBABILISTIC MODELS">Analytic-DPM笔记</a><p>另一方面，DDIM可以加速DDPM采样。（这个也提到可以被Analytic-DPM平替）</p><p>同时，作者也讲了一些metric的改进，这些就是纯图像领域FID的内容，我也不太了解，就跳了。</p><h2 id="architecture-improvements">Architecture Improvements</h2><p>这一部分，作者讲了讨论了几种架构型对DDPM的形象，寻找最适合DDPM的模型。</p><p>首先作者提到UNet结构可以在实质上增强Diffusionmodel的能力，探索了：</p><p><img src="../files/images/BEAT_GAN/Unet.png"></p><h2 id="classifier-guidance">Classifier Guidance</h2><p>这一部分其实就是之前提到的，作者在这里指明了对应的DDPM和DDIM的算法</p><p><img src="../files/images/BEAT_GAN/algo.png"></p><p>上面两种算法的进阶形式的推导主要是先用贝叶斯分离(<spanclass="math inline">\(x与y\)</span>) <span class="math display">\[p_{\theta,\phi}(x_t | x_{t+1},y) = Z p_\theta(x_t | x_{t+1}) p_\phi(y |x_t)\]</span> 左侧进行泰勒展开，取一阶形式进行化简，剔除，最终得到：</p><p><img src="../files/images/BEAT_GAN/proof.png"></p><p>最终，作者探索了这几个改进对DDPM的影响</p><p><img src="../files/images/BEAT_GAN/effect.png"></p><p>可以看到，合适的scale系数s和guidance可以极大程度提升DDPM的效果。</p><p>同时，最佳的ADM模型战胜了之前GAN的SOTA: BIGGAN.</p><p>这篇文章最主要的贡献就是提出了所谓classifierguidance的方法，以及证明了DPM是有超过GAN的潜力。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</title>
      <link href="/b41d6ff9.html"/>
      <url>/b41d6ff9.html</url>
      
        <content type="html"><![CDATA[<p>这篇工作其实就是有名的DALL.E2的模型结构，只是规模是3.5B(DALL.E是12B)。本篇工作是第一个用diffusionmodel来做text2image任务的。</p><span id="more"></span><p><img src="../files/images/GLIDE/author.png"></p><p>作者是财大气粗的OpenAI，这几个联合一作都是重量级：</p><ul><li>Alex Nichol：DALL.E 2(unCLIP)作者</li><li>Prafulla Dhariwal：GPT3作者，ADM作者</li><li>Aditya Ramesh：DALL.E作者</li></ul><p>这篇工作中关于Diffusionmodel的原理就不写了，但里面提到了两种有趣的guidance方法我想写一下：</p><p>所有的所谓guidance都是说想要生成包含条件y的结果x，即<spanclass="math inline">\(P_\theta(x|y)\)</span></p><h2 id="guided-diffusion">Guided Diffusion</h2><p>这种原理始见于</p><blockquote><p>Diffusion models beat gans on image synthesis.</p></blockquote><a href="/99193096.html" title="论文阅读[粗读]-Diffusion models beat gans on image synthesis">ADM笔记</a><p>这里用的原理公式是： <span class="math display">\[\hat{\mu}_\theta(x_t | y) = \mu_\theta(x_t | y) + s \times\Sigma_{\theta} (x_ | y) \times \bigtriangledown_{x_t}\log p_\phi(y|x_t)\]</span> 意思是说，我们需要训练一个分类器<spanclass="math inline">\(p_\phi(y|x_t)\)</span>可以对任何一个样本分类出对应的分类y。有了这个分类器的导数之后，下一次的去早就可以看成均值拟合器<spanclass="math inline">\(\mu_\theta(x_t |y)\)</span>(这个可以看是一个不含y的模型)结果加上一个偏向于y分类器出y的方差。</p><p>s是系数，当s变大时，diversity会下降，但是生成的结果更偏向于y。</p><h2 id="classifier-free-guidance">Classifier-free guidance</h2><p>这个方法更有意思了，始见于</p><blockquote><p>Classifier-free diffusion guidance</p></blockquote><p>它建立的逻辑是：不想训练classifier(因为需要给每一个分类都训练分类器，泛化能力不强).观察分类器做的事情，由贝叶斯公式进行化简： <span class="math display">\[p(y | x_t) \propto \frac{p(x_t| y)}{p(x_t)}\]</span></p><p><span class="math display">\[\bigtriangledown_{x_t}\log p(y | x_t) \propto(\bigtriangledown_{x_t}\log p(x_t | y) -\bigtriangledown_{x_t}\logp(x_t))\]</span></p><p>我们可以把生成器直接做成condition的形式：<spanclass="math inline">\(p_\theta(x_t |y)\)</span>,y可以是比如说文本信号，如果y输入空就是无条件的生成。因此上面正常guidance公式就可以修改成：<span class="math display">\[\hat{\epsilon}_\theta(x_t | y) = \epsilon_\theta(x_t | \phi) + s \times(\epsilon_\theta(x_t | y) - \epsilon_\theta(x_t | \phi))\]</span> 这里的log和<spanclass="math inline">\(\bigtriangledown\)</span>消失了是因为在DDPM如果看做socrebased model的话将会有<span class="math inline">\(\epsilon_\theta =\bigtriangledown_{x_t} \log p(x_t)\)</span></p><p>s是一个scale量，某种意义上抵消贝叶斯公式的偏差</p><p>上面的公式很有意思：</p><ul><li><p>只需要一个模型就能完成guidance，不需要额外训练classifier，更加稳定</p></li><li><p>这个方法的表征能力更强，可以获取很多无法被classifier感知到的“类别”(文本等)</p></li></ul><h2 id="clip-guidance">CLIP Guidance</h2><p>这个是上面classifier guidance 的一种特例，就是把<spanclass="math inline">\(\cos&lt;clip(\hat{y}),clip(x)&gt;\)</span>当做分类器的结果，因此clip的特点，也可以表征抽象的文本“类别”</p><h2 id="glide">GLIDE</h2><p>下面说说GLIDE模型。由于这部分对NLP不重要，就不重点写。大概有几个特征：</p><ul><li>主体Diffusion model采用 ADM结构</li><li>训练集和DALL.E保持一致</li><li>pretrain时直接用&lt;x,y&gt;对进行<spanclass="math inline">\(p_\theta(x_t | y)\)</span>生成</li><li>在pretrain结束以后进行classifier-freeguidance，用上面的公式来一个指导生成<ul><li>训练时20%部分文本被mask，把任务变成无条件生成，保证模型本身的生成能力不被破坏</li></ul></li><li>在RGB外多加了一个channel代表被mask的部分，可以进行Inpainting的需求</li></ul><p>结论是：</p><ul><li>对比classifier-free guidance和CLIPguidance，前者的效果更好(人工评测)</li><li>用了Diffusion model的GUIDE比不用的DALL.E 好很多</li></ul><p><img src="../files/images/GLIDE/inpainting.png"></p><p>inpainting任务就是上图这个，相当于"P图"，绿色部分就是额外的maskchannel中置为1的地方</p><p><img src="../files/images/GLIDE/compare.png"></p><p>上图是文本控制的图片生成，对比下来好像GLIDE确实是最好的</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-12总结</title>
      <link href="/15928165.html"/>
      <url>/15928165.html</url>
      
        <content type="html"><![CDATA[<p>今天去北医三院看牙，来回单程只需要40min的高铁，倒是很快。应该是最后一次根管治疗，再观察一段时间就可以做牙冠了。</p><span id="more"></span><p>另外，今天第一次去猫咖，见到了各种猫：英短蓝猫、英短蓝白、各种颜色的美短、布偶猫、豹纹猫、埃及法老毛、德文卷毛猫、矮脚猫、橘猫……一天撸了一年的猫。感觉猫咖的形式很好，后面可能还能去。</p><p>另外，今天去吃巴奴，之前认识的服务员小姐姐现在升级成了红衣，物是人非了。</p><p>今日观赛：</p><ul><li>TES打OMG：omg蓝色方赢了一把，able萨米拉吃恶意收购打队友叠了S，闪现进场转4个。后两把属于是Tian表演局。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-THE CORA TENSOR COMPILER: COMPILATION FOR RAGGED TENSORS WITH MINIMAL PADDING</title>
      <link href="/b0f88437.html"/>
      <url>/b0f88437.html</url>
      
        <content type="html"><![CDATA[<p>陈天奇在MLSys22发的另一篇文章，讲的是如何提高输入不对齐时的表现。</p><span id="more"></span><p><img src="../files/images/CORA/author.png"></p><p>已有的方法面对不对齐的数据，常常是padding到一个长度，这样会造成额外的计算。本文解决这个问题，得到了Pytorch上16倍的速度提升，TensorFlow上1.4倍的性能提升</p><h2 id="introduction">Introduction</h2><p>DL模型的输入常常不对齐，如下图所示：</p><p><img src="../files/images/CORA/padding.png"></p><p>最近有一些编译器开始支持不对齐的数据，但是范围、效果很受限。而经典的cuDNN，oneDNN库都采用了MASK，PADDING的方法。这种方法会带来额外的计算损失(红色部分)，而且随着batchsize的增大这个损失会更加严重。</p><p>本文提出了一套compiler-based方法解决不对齐数据的问题。稀疏和密集的tensor的研究很多，但大多不能应用到ragtensor,同时本文提出CORA (Compiler for Ragged Tensors)：</p><ul><li>Irregularity in generatedcode：可变的循环带来可变的代码，使得GPU性能下降。<ul><li>CORA使用minimal padding方法，来平衡高效代码生成和thread remappingstrategies</li></ul></li><li>Insufficient compilermechanisms：由于可变循环内的依赖关系复杂，Representingtransformations效果很差。并且稀疏tensor的优化方法可能不能应用到ragtensor，因为后者可能更加稀疏。<ul><li>Cora用uninterpreted functions表示循环边界，来调度operation</li></ul></li><li>Ill-fitting computation abstractions：接口和抽象不匹配的问题。densecompilers不能表达rag op，sparse compilers没有办法生成高效的code。<ul><li>CORA给padding，threadremapping等提供一套抽象，来解决不匹配的问题。</li></ul></li></ul><p>和已有方法对比如下：</p><p><img src="../files/images/CORA/comp.png"></p><p>本文主要贡献：</p><ul><li>提出CORA，第一个高效解决ragged tensor的tensor编译器</li><li>为ragged tensor提供了新的API，抽象和scheduleing方法</li><li>进行了详细评测。</li></ul><h2 id="cora-overview">CORA OVERVIEW</h2><p>基于以下的两个灵感：</p><ul><li>ragged operation中的不对齐性在计算前就可以获取到<ul><li>可以在真正计算前预计算不对齐性带来的辅助信息</li></ul></li><li>ragged tensor比起稀疏tensor，可以用<spanclass="math inline">\(\mathcal{O}(1)\)</span>时间访问（HASH方法虽然可以<spanclass="math inline">\(\mathcal{O}(1)\)</span>稀疏tensor，但不利于GPU执行）<ul><li>可以辅助生成高效的代码</li></ul></li></ul><p><img src="../files/images/CORA/pipeline.png"></p><p>CORA总体执行如上图所示</p><h2 id="术语">术语</h2><ul><li><em>variable loops</em> or <em>vloops</em>：如果内部循环的边界是和外层变量有关的</li><li>vloop nest：包含多个vloop</li><li><em>variabledimensions</em>：当存储tensor不用padding时，齐形状取决于外部tensor。这个形状称为variabledimensions，或者vdims</li><li>cdims：固定形状</li><li>ragged tensor：有最少一个vdim的tensor</li></ul><h2 id="coras-ragged-api">CORA’S RAGGED API</h2><p><img src="../files/images/CORA/api.png"></p><p>CORA的API要求对于raggedtensor描述出vdim是怎么计算的，由外层循环的什么特征来定义。满足此条件后，CORA就可以自动的符合后面计算API的实现。</p><h3 id="scheduling-primitives">Scheduling Primitives</h3><ul><li>LoopScheduling：CORA不允许对vloop交换顺序，因为没有发现有什么实际需求。</li><li>OperationSplitting：CORA可以把OP分解成多个OP，后面把拆除的部分并行计算。</li><li>HorizontalFusion：CORA支持hfusion，可以把最外层循环内的几个operator并行计算</li><li>Loop and Storage Padding：对于minimalpadding(二对齐、四对齐)，可以对齐的存储，并且不浪费很多额外的空间</li><li>Tensor Dimension Scheduling：CORA允许用户交换tensor的维度</li><li>Load Balancing：CORA允许用户去自己满足<em>thread remappingpolicy</em>，来解决这个问题。</li></ul><h2 id="coras-ragged-api-lowering">CORA’S RAGGED API LOWERING</h2><p>这一部分讨论把CORA API lower到SSA-based IR的技术</p><h3 id="loop-and-tensor-dimension-fusion">Loop and Tensor DimensionFusion</h3><p><img src="../files/images/CORA/loop.png"></p><p>对于左上角的二重循环，如果想要变成一层循环，就需要计算出边界以及什么时候换。CORA预计算这些信息，再执行计算部分的代码。</p><h3 id="bounds-inference">Bounds Inference</h3><ul><li>Variable Loop Fusion：</li><li>Named Dimensions</li></ul><h3 id="storage-access-lowering">Storage Access Lowering</h3><p><img src="../files/images/CORA/shape.png"></p><p>考虑上面的4维tensor，24维是vdim。</p><p>CORA提供右图左面的组织形式，可以达到快速的<spanclass="math inline">\(\mathcal{O}(1)\)</span>访问时间，通过预计算。CORA需要存储额外的形状信息。</p><h2 id="implementation">IMPLEMENTATION</h2><p>CORA通过扩展TVM的代码进行实现</p><ul><li>Ragged API：目前的代码只支持vdim取决于最外层的dim</li><li>Lowering：这部分代码基于一篇已有工作修改。</li></ul><h2 id="evaluation">Evaluation</h2><p>大概就是CORA更好</p><p><img src="../files/images/CORA/CORA.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 深度学习编译 </tag>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-10总结</title>
      <link href="/820d904c.html"/>
      <url>/820d904c.html</url>
      
        <content type="html"><![CDATA[<p>今日研读一篇编译论文，两篇AI论文，今日吃到妈妈做的蛋挞。</p><p>后天回北医三院看牙，canyon保佑。</p><span id="more"></span><p>今日观赛：</p><ul><li>EDG打V5：edg纯纯的打不过，感觉今天V5有冠军相呀，不过春季赛也是这么想的，关键时刻还得看ppgod！<ul><li>波比+岩雀+烈娜塔也太狠了吧：一个封路，一个恶意收购，接波比锤走c位，对面直接暴毙呀</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-APOLLO: AUTOMATIC PARTITION-BASED OPERATOR FUSION THROUGH LAYER BY LAYER OPTIMIZATION</title>
      <link href="/cacd892d.html"/>
      <url>/cacd892d.html</url>
      
        <content type="html"><![CDATA[<p>今天来读一篇MLSys的文章，作者提出了JIT的APOLLO框架，可以同时考虑memory- /compute-bound 的算子优化，比XLA，TensorFlow原生要快不少。</p><span id="more"></span><blockquote><p>JIT(just-in-time)：和AOT相对，是指边运行边编译，不能支持某些编译优化方法。</p></blockquote><p><img src="../files/images/APOLLO/comp.png"></p><p><img src="../files/images/APOLLO/author.png"></p><p>作者来自郑州大学、华为、香港大学</p><h2 id="introduction">Introduction</h2><p>这部分说了目前fusion技术面临了三个问题：</p><ul><li>现有的图编译器只考虑了memory-bound op的优化，对于compute-boundop，只是分发到不同的kernel，不做fusion。因此fusion的搜索空间是不完全的</li><li>对于循环loop fusion来说：不同层次的编译器之间存才</li><li>stitching融合技术，把没有依赖的不同算子放到同一个kernel来同时计算，提高硬件的并行性。这种技术对于自定义算子的支持非常差</li></ul><p>本文提出APOLLO框架，把sub-graph进一步看成很多micro-graph，在这个过程中同时考虑memory-/compute-bound的算子fusion。划分的规则取消了上游图编译器的依赖，规则符合下游编译器的需要。可以符合一个多面体模型，同时也可以扩大规模。</p><blockquote><p>APOLLO := Automatic Partition-based Operator fusion framework throughLayer by Layer Optimization</p></blockquote><ul><li>首先，用一个<em>polyhedral loopfusion</em>算法让micro-graph内部的算子fusion，并且让有依赖管的数据在更快的localmemory里</li><li>接下里用一个<em>memorystitching</em>算法让micro-graph之间的算子fusion，弥补前面polyhedral算法的劣势</li><li>最后，由一个toplayer把前两步的结果组合在一起，更好的利用硬件的并行性。</li></ul><p>APOLLO可以把每一步中间生成的IR传给下一步。本文的贡献总结为：</p><ul><li>扩大了fusion 的搜索空间，考虑更多种op</li><li>通过operator-level优化器反向反馈结果，可以进行更大规模的fusion</li><li>同时考虑数据的局部性、并行性，效果更好</li><li>展现了JIT compilation的水平</li></ul><h2 id="architecture-of-apollo">ARCHITECTURE OF APOLLO</h2><p><img src="../files/images/APOLLO/APOLLO.png"></p><p>用一个例子来说明apollo结构，上图是一个计算图：</p><ul><li><p>计算图可被分成上下两个子图(相互是可以并行执行的)</p></li><li><p>Op5，op3是计算瓶颈的节点</p></li><li><p>Op1,op6是原子的节点</p></li><li><p>Op2,op4,op7是可再分的节点</p></li></ul><p>对于计算瓶颈节点op5,op3,已有编译器尝尝直接丢给vendorlibrary，并且把，每个子图sub-graph看成孤立的不同部分</p><p>Tensorcompilers面临上下游需求不同的问题，比如上游编译器希望op7编译成一个kernel，但如果op71是reduceop，下游编译器很难满足这个需求</p><p>另一个问题是当batchsize小的时候不能利用好硬件的并行性，现有方法经常把同一个子图中的比如op1，op2放到同一个kernel，这和实际的trainingscenarios不符合。</p><p><img src="../files/images/APOLLO/arch.png"></p><p>本文提出的APOLLO框架如下：</p><ul><li><p>首先把图分成不同子图sub-graph，子图之间没有数据依赖(<spanclass="math inline">\(\mathcal{F}_1,\mathcal{F}_2\)</span>)</p></li><li><p>接下来，用一系列基于规则的方法把每个子图分出不同micro-graph(<spanclass="math inline">\(\mathcal{G}_1,\mathcal{G}_2...\)</span>)</p></li><li><p>对于每个micro-graph，进行fusion生成IR，用的方法是polyhedral loopfusion heuristic</p></li><li><p>把上一层的结果传给下一层，layer 2对结果进行grouping，同时根据reducing op进行fusion</p></li><li><p>最终layer 3接受上一层输入，进行子图之间的并行性的优化</p></li></ul><h3 id="polyhedral-model">polyhedral model</h3><p>好多篇里都说了这个polyhedral，到底是咋回事呢？这里拓展一下。</p><p>大概就是一个多重循环，里面的所有可能的取值(i,j,k)可以用一个矩阵来表示：<span class="math display">\[A_{3,3} \left(\begin{array}{}i \\ j \\ k\end{array}\right) \geq\left(\begin{array}{}a \\ b \\ c\end{array}\right)\]</span>之类的，总之这种方程在高位空间就是一个多面体，可以认为循环都是在多面体内执行。有什么作用呢？</p><ul><li>循环间的数据依赖大概看成是多面体内内部有一些”线“，如果是水平的、垂直的，那么就不会有冲突，只有“斜线“会带来冲突，但如果我们对这个多面体进行仿射变换，就可以把斜线都变成直线了。</li><li>我们希望循环内可以更多利用cache，同样利用仿射，可以把空间距离近的取值让时间距离也拉进。</li></ul><p>大概就是这个原理，因为有了数学的表示，所有优化也就比较简单。</p><p><img src="../files/images/APOLLO/model.png"></p><h2 id="partition-phase">PARTITION PHASE</h2><p>这一部分谈怎么变成micro-graph</p><p>作者首先使用几条规则把图优化成符合多面体模型的图</p><p><img src="../files/images/APOLLO/duomian.png"></p><p>接下来，要找出计算图中除了用户定义的op和control flow<em>op</em>s以外的op，变成一些子图。</p><h3 id="opening-compound-operators">Opening Compound Operators</h3><p>再接下来，作者把compound op拆解，比如说logsoftmax： <spanclass="math display">\[S(t_i) = t_i -\ln \sum_{j=1}^N e^{t_j}\]</span> <img src="../files/images/APOLLO/softmax.png"></p><p>其对应蓝色的两个op：减法，和一个循环计算的op，这两个op有数据依赖。</p><p>如果把op3拆开分析，就能把op31和前面的op1，op2融合到一起，，增大fusion的搜索空间，这就是拆解compoundop的好处。</p><h3 id="aggregating-primitive-operators">Aggregating PrimitiveOperators</h3><p>这一部分要分解出很多micro-graph</p><p>首先把primitive op看做micro-graph，然后用aggregationrules去逐步扩大micro-graph。规则大概是下面这样定义的几条：</p><p><img src="../files/images/APOLLO/aggre.png"></p><p>规则不用覆盖所有op，因为有些op本身就不需要fuse。</p><h2 id="fusion-phase">FUSION PHASE</h2><p>fusion的阶段符合自底向上的顺序</p><h3 id="layer-i-polyhedral-loop-fusion">Layer I: Polyhedral LoopFusion</h3><p>这一部分讲了micro-graph内部是如何做fusion的。方法很多，很复杂，这里就不细讲了，感觉只有看代码才能深入了解，大概逻辑就是像下图这样：</p><p><img src="../files/images/APOLLO/fusion.png"></p><h3 id="layer-ii-memory-stitching">Layer II: Memory Stitching</h3><p>这一部分是说如何把fusion后的micro-graph再fusion，再组装回sub-graph</p><p><img src="../files/images/APOLLO/memory.png"></p><p>大概思路就是如果一个micro-graph是一个reduce型的micro-graph，那么它也许可以和后面你的micro-graph的中间变量排布在一起，用到更快的localmemory。</p><h3 id="layer-iii-parallelism-stitching">Layer III: ParallelismStitching</h3><p>这一部分考虑子图之间的并行性，作者观察到并行性大多存在于：</p><ul><li>branches of a multi-head/-tail <em>op</em></li><li>两个子图没有数据的依赖关系。这种可以合并成上面那种</li></ul><p>作者大概考虑了怎么优化 multi-head/-tail<em>op</em>，作者用一个cost模型逐步的选取并行效果最好的节点加入。</p><p><img src="../files/images/APOLLO/para.png"></p><h2 id="putting-it-all-together">PUTTING IT ALL TOGETHER</h2><ul><li>Auto-Tuning:APOLLO解决了scalability的问题，并且由于peicewise编译速度不慢，因此支持auto-tuning</li><li>PiecewiseCompilation：每一层的各个节点之间的编译是并行的，只需要在层交换的时候做一次不同</li><li>Code Generation：生成A100 cuda代码。</li></ul><h2 id="result">Result</h2><p>大概就是说APOLLO处的代码效果更好</p><p><img src="../files/images/APOLLO/speed.png"></p><blockquote><p>mindspore：是华为的开源AI框架，APOLLO可以试做是在优化mindspore的编译</p></blockquote><h2 id="结论">结论</h2><ul><li>polyhedral优化的扩展性是一个NP问题，因此很难大范围的应用。本文通过把图拆解成子图，每一部分分别做，一定程度上解决了这个问题。</li><li>目前的规则很简单，不一定适合其他的、未来的算子</li><li>本文使用的costmodel很简单，只是单纯的在并行和同步之间做权衡</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 深度学习编译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-ANALYTIC-DPM: AN ANALYTIC ESTIMATE OF THE OPTIMAL REVERSE VARIANCE IN DIFFUSION PROBABILISTIC MODELS</title>
      <link href="/babb0879.html"/>
      <url>/babb0879.html</url>
      
        <content type="html"><![CDATA[<p>这篇论文是朱军老师组做的工作，非常的数学。</p><h2 id="introduction">Introduction</h2><ul><li>DPM生成很慢，这是因为逆过程对方差的估计花了额外的时间</li><li>本文发现最优的期望和方差竟有解析形式（iDDPM岂不是优化了个寂寞…）</li><li>本文用蒙特卡洛模拟最优方差，预计算</li><li>在获得更高质量图片的同时得到了80倍的速度提升(对，效果甚至更好)</li><li>本文在选取快速路径时用动态规划</li><li>本文是plug-and-play的，任何DPM模型拿过来换个<spanclass="math inline">\(\sigma\)</span>就行</li></ul><h2 id="background">Background</h2><p>重新说了一下DDIM里的推广形式：</p><p><img src="../files/images/Analytic-DPM/ddim.png"></p><p>其中<span class="math inline">\(\overline{\alpha}:=\prod_{i=1}^N\alpha_i, \overline{\beta} := 1-\overline{\alpha}\)</span></p><ul><li>当<span class="math inline">\(\lambda_n^2 = \tilde{\beta_n} :=\frac{\overline{\beta}_{n}}{\overline{\beta}_{n-1}}\beta_n\)</span>时，模型就会推导出DDPM</li><li>当<span class="math inline">\(\lambda_n^2 \equiv0\)</span>时，就是DDIM</li></ul><p>上面这种前项过程的逆过程是一个马尔科夫过程： <spanclass="math display">\[p(x_{0:N}) = p(x_N) \prod_{n=1}^N p(x_{n-1}|x_n), \quad p(x_N) \sim\mathcal{N}(0,I) \\p(x_{n-1}|x_n) = \mathcal{N}(x_{n-1}| \mu_n(x_n),\sigma_n^2 I)\]</span> 其中<span class="math inline">\(\mu_n(x_n)\)</span>用一个scorebased model来学习<span class="math inline">\(s_n(x_n)\)</span>， <spanclass="math display">\[\mu_n(x_n) =\tilde{\mu}_n\left(  x_n,\frac{1}{\sqrt{\overline{\alpha_n}}} (x_n+\overline{\beta_n} s_n(x_n) ) \right)\]</span> 如果认为<span class="math inline">\(s_n(x_n) =-\frac{1}{\sqrt{\overline{\beta_n}}}\epsilon_n(x_n)\)</span>，这和DDPM的推导结果是一致的</p><p>对于方差的估计：</p><ul><li>DDPM使用<span class="math inline">\(\sigma_n^2 = \beta_n或\sigma_n^2= \tilde{\beta}_n\)</span></li><li>DDIM使用<span class="math inline">\(\sigma_n^2 =\lambda_n^2\)</span></li></ul><p>作者说，这实际上是一个负优化</p><h2 id="analytic-estimate-of-the-optimal-reverse-variance">ANALYTICESTIMATE OF THE OPTIMAL REVERSE VARIANCE</h2><p>接下来引入文章的本体，作者证明了上述定义的DPM的逆过程的均值、方差的最优形式：</p><p><img src="../files/images/Analytic-DPM/best.png"></p><p>这个结论的证明非常复杂，感兴趣的同学可以看附录A</p><ul><li><p>只要按照上面的式子进行去噪，就能得到最好的结果，到达模型的极限能力</p></li><li><p>对应的SDE的连续时间形式，也有类似的结论，参考附录(目前用不上)</p></li><li><p>可以看出，上面的均值部分和已有方法的优化的<spanclass="math inline">\(\mathcal{L}_{LVB}\)</span>具有一样的形式，因此现在的DDPM等模型的训练就是在逼近最优的均值，训练没有问题。</p></li></ul><p>接下来，作者就要把之前的手动设计的方差替换为解析的形式，对于后面的密度项，作者使用蒙特卡洛进行估计<span class="math display">\[\Gamma_n = \frac{1}{M}\sum_{m=1}^{M} \frac{ || s_n(x_{n,m})||^2 } {d} ,\quad s_{n,m}\mathop{\sim}^{i.i.d} q_n{x_n}\]</span></p><p><span class="math display">\[\hat{\sigma}^2_n = \lambda_n^2 + \left(\sqrt{\frac{\overline{\beta_n}}{\alpha_n}} -\sqrt{\overline{\beta}_{n-1} - \lambda_n^2 } \right)^2 (1 -\overline{\beta}_{n}\Gamma_n)\]</span></p><ul><li>这个方差和训练无关。在模型训练以后，可以预计算出来每一步的方差，再统一的进行infer</li><li>随着蒙特卡洛采样数M的增大，这个估计会越来越准确，逐渐逼近最优的方差。作者提到，取M=10,100基本上就收敛了</li></ul><p><img src="../files/images/Analytic-DPM/sigma.png"></p><p>下面这个图</p><ul><li>显示出了作者的估计值和DDIM和DDPM手动设计的方差的区别</li><li>显示了作者的方差对于<spanclass="math inline">\(L_{vb}\)</span>是最小的，这个应该是针对一次infer而言。</li></ul><h3 id="bounding-the-optimal-reverse-variance-to-reduce-bias">BOUNDINGTHE OPTIMAL REVERSE VARIANCE TO REDUCE BIAS</h3><p><img src="../files/images/Analytic-DPM/bias.png"></p><p>这一部分，作者分析了他的逼近形式和真正的最优形式之间的bias：</p><ul><li>由于模型的训练，会带来不可避免的误差(因为不可能有完全没误差的模型)</li><li>如果在sample时选取一个较短的路径，左边的coefficient部分会变大</li></ul><p>接下来，作者尝试能不能减小误差，作者又证明了真实方差的上下界</p><p><img src="../files/images/Analytic-DPM/bound.png"></p><ul><li>这个上下界都是确定的、和模型训练无关的，在选定了<spanclass="math inline">\(\beta_n\)</span>之后就唯一确定了。</li><li>同时作者证明了(12)中的上界是一个比较接近真实值的上界。</li></ul><p>因此，作者才计算完拟合的方差<spanclass="math inline">\(\hat{\sigma}_n\)</span>之后可以用上下界再做一次CLIP。</p><h2 id="analytic-estimation-of-the-optimal-trajectory">ANALYTICESTIMATION OF THE OPTIMAL TRAJECTORY</h2><p>这一部分，作者讲了在一个缩减的去噪路径中优化的方法。还是先提到了DDIM优化的故事线：</p><ul><li>其中 $1 = _1 &lt; _2 ... &lt; _K = N$,把一个长为N的路径变成了长为K的采样路径</li></ul><p><img src="../files/images/Analytic-DPM/ddim_infer.png"></p><p>在这个过程中，我们的<spanclass="math inline">\(\lambda^2_{\tau_{k-1}|\tau_k}\)</span>也可以变成拟合的新形式：</p><ul><li>其中的<spanclass="math inline">\(\Gamma_{\tau_{k}}\)</span>可以在<spanclass="math inline">\(\tau_{k-1} \sim \tau_{k}\)</span>之间通用</li></ul><p><span class="math display">\[\hat{\sigma}^2_{\tau_{k-1}| \tau_k} = \lambda_{\tau_{k-1}| \tau_k}^2 +\left( \sqrt{\frac{\overline{\beta_{\tau_k}}}{\alpha_{\tau_{k-1}|\tau_k}}} - \sqrt{\overline{\beta}_{\tau_{k-1}} - \lambda_{\tau_{k-1}|\tau_k}^2 } \right)^2 (1 -\overline{\beta}_{\tau_{k-1}}\Gamma_{\tau_{k}})\]</span></p><p>上面的推广是显然的，然而，路径的选取大有说法，</p><p><img src="../files/images/Analytic-DPM/min.png"></p><p>作者竟然推导出了一个K路径中模拟的偏差的下界：</p><ul><li>其中<span class="math inline">\(J(\tau_{k-1},\tau_k) = \log(\frac{\sigma^{*2}_{\tau_{k-1}| \tau_k}}{\lambda_{\tau_{k-1}|\tau_k}^2})\)</span>,c是一个和路径选择无关的量</li><li>如果把<span class="math inline">\(\sigma^{*} \sim\hat{\sigma}\)</span>的话，这个最小值就是可计算的，只要路径被选定</li></ul><p>甚至，这是一个动态规划问题，从T里选取K个数，要求K个数的损失之和最小，每个数的损失都是确定的。在模型训练完、K被选定后，可以执行一次算法，得出具体怎么选会得到最小的损失</p><p><img src="../files/images/Analytic-DPM/DP.png"></p><h2id="relationship-between-the-score-function-and-the-data-covariance-matrix">RELATIONSHIPBETWEEN THE SCORE FUNCTION AND THE DATA COVARIANCE MATRIX</h2><p>这一部分是说了scorefunction和协方差矩阵之间的关系。今天作者在ICML的另一篇论文推广了这个优化，这篇就不讲了。</p><h2 id="experiment">Experiment</h2><p>这里作者用了：</p><ul><li><p>自己在CIFAR-10上分别用cosine和linear两种<spanclass="math inline">\(\beta\)</span>炼出来的CIFAR-10(LS),CIFAR-10(CS)</p></li><li><p>DDPM和DDIM在ImageNet上预训练的开源模型，分别使用和不使用Analytic方差</p></li><li><p>作者在全部模型上CLIP了方差，因为这个上下界是恒存在的</p></li></ul><p><img src="../files/images/Analytic-DPM/result.png"></p><p>这个结论是很明显的，可以说是吊打：</p><ul><li>凡是加上了拟合方差的，基本上都比不加要好</li><li>对于K小的时候，会很明显</li><li>当T=K的时候，总的效果也要更好</li><li>甚至用了50步，就战胜了普通DDIM 1000步的效果</li></ul><p>当然作者也提到了几个现象：</p><ul><li>Analytic的形式不是永远比不用要好，这和解析形式的“最优”结论是不一致的</li><li>K更大时效果不一定会更好</li><li>用动态规划OT的K选取不一定总是最好的</li></ul><h2 id="我的想法">我的想法</h2><ul><li>DDPM里面有一点就是，去噪方差要手动设置。<ul><li>一个直观的想法就是，如果方差里也引入模型，效果会更好。iDDPM就是用模型来学习方差，但收敛难（参考GAN）</li><li>这篇文章结论就是：用学均值的模型，可以顺便用一个蒙特卡洛模拟来近似最优的方差！不需要额外学！</li></ul></li><li>这篇文章正文没有一个图片，就拿了ICLR outstandingpaper，值得唏嘘。正文10页，附录29页，基本全是数学证明，可见AI的本质是数学（</li><li>作者在附录中的futurework里说后面要做speech领域，用上面说的最优形式的连续时间推广形式。我们是不是想办法看看NLP怎么用</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-9总结</title>
      <link href="/8ac3dc8d.html"/>
      <url>/8ac3dc8d.html</url>
      
        <content type="html"><![CDATA[<p>突然发现好几天没写总结了，今天写一个。</p><p>最近几天其实还挺摆的，尤其是快手那边，可以说是毫无进展，连进度同步都推迟了……好消息是我做决定了，</p><ul><li>明天我上午就起床，先搞一篇编译的工作，不能再这样下去了！</li><li>明天下午也要肝，搞点Diffusion model看一下</li></ul><span id="more"></span><p>今日看李沐老师的DALL.E2论文阅读，我直接送出今年第二个一键三连……别猜了，第一个是《多 啦Agent》</p><p>昨天出了这学期的绩点，不出意外的满绩…话说我有点好像这学期满绩的有多少人，info9排名只能看并列名次来着……只能说，贵系，懂得都懂</p><p>前天被防空警报吵醒，特此记录</p><p>近几日观赛：</p><ul><li>今天BLG打WBG：blg怎么也开始排列组合了，fofo换下去上来can有用，但是crisp和大bin老师融入的问题是不是要解决一下。can只能说出道即院长，未来可期。<ul><li>话说回来，WBG是不是也就能打打blg这种队了……</li></ul></li><li>今天TES打RNG：打的好呀tes，我只能说，蓝色方。wayward这波纳尔反counter单杀剑姬确实帅，牛子哥直接原形毕露，大bin附体。</li><li>昨天T1打GEN.G：T1竟然红色方逆袭赢了。这波是，ruler艾希远程支援——没事选啥艾希呀，都t4多少版本了也不想想。大飞老师确实明显，我能接受被chovy压刀，但你总不能，冰女被阿卡丽压刀吧，我真的哭死……</li><li>昨天LNG打JDG：泪目了，多久没见到369c了。说实话，ALE这肉墩有点拉胯，救一下呀，一个赛季了（</li><li>前天DRX打DK：第一把canyon选出卑尔维斯，属于是想赢了，结果关键团没打过，捡不到虚空珊瑚，输了情有可原……但第二把DK怎么摆烂了，虽然红色方是劣势，但不能摆烂呀……<ul><li>管泽元说，队伍关键是不能每一路都出问题，DK看一下呀。</li><li>BeryL爆锤老东家，第一把直接木木MVP，不谈了。原来你真的还会选我。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-Minimum Bayes-Risk Decoding for Statistical Machine Translation</title>
      <link href="/f1fcbd7.html"/>
      <url>/f1fcbd7.html</url>
      
        <content type="html"><![CDATA[<p>这篇文章是讲机器翻译的，准确来说，是解码策略。作者提到已有很多评测machinetranslation的方法，但已有的解码大多没有同时考虑这些方法。</p><p>本文提出的MBR方法可以：</p><ul><li>根据特定的metric，解码出得分更高的候选</li><li>通过特定的loss函数，把语法树考虑进统计语言模型</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-Diffusion-LM Improves Controllable Text Generation(2)</title>
      <link href="/88eeb90b.html"/>
      <url>/88eeb90b.html</url>
      
        <content type="html"><![CDATA[<p>之前我们聊了</p><ul><li>DDPM的原理 <a href="/75bfb84b.html" title="论文阅读:Diffusion-LM Improves Controllable Text Generation(1)">DDPM笔记</a></li><li>也聊了 score based models 原理 <a href="/593c50eb.html" title="博客阅读-Generative Modeling by Estimating Gradients of the Data Distribution">博客阅读笔记</a></li></ul><p>今天我们满足了所有前置知识，可以来看这篇的本体了。如何把diffusionmodels 用到可控文本生成中。虽然是老本行，但这篇还是精读一下。</p><span id="more"></span><h2 id="摘要">摘要</h2><ul><li>可控文本生成是一个重要的任务</li><li>已有的方法对粗粒度的控制尚可(情感)，但对细粒度的控制效果不好(句法树)</li><li>本文设计了一个非自回归的、连续的diffusion model来解决这个问题</li><li>去噪过程中的多个中间量可以指导可控的文本生成过程</li><li>在6个可控生成任务中，Diffusion-LM模型大幅战胜了SOTA</li></ul><h2 id="introduction">Introduction</h2><p>自回归的语言模型的生成能力很强，但可控性很弱。如果想做可控的生成：</p><ul><li>直观方法是做有监督的训练，但这样需要每个任务炼一个模型，成本高。</li><li>plug-and-play的方法，冻住模型。但此类方法效果差，难以实现细粒度的控制</li></ul><p>传统的连续diffusionmodel不适用于文本，因为word是离散的。本文通过添加一个embedding步骤和一个rounding步骤解决这个问题。同时，本文通过一种可控的gradientupdate指导模型可以平衡fluency和controllable两种属性。</p><p>本文在6个可控任务中大幅战胜plug-and-playSOTA，同时战胜了fine-tune的方法。甚至，在不用classifier的情况下和自回归的模型有的一拼。</p><h2 id="related-work">Related work</h2><p>Diffusion Models for Text：已有方法都是在离散的空间做这件事，提到的论文我也看了，有时间给大家写一篇笔记。本文用了连续的方法来做。</p><p>Autoregressive and Non-autoregressive LMs：已有的模型大多是是从左到右训练的，但是对于infilling等任务来说，这限制了模型的发挥。这个问题，有一些解决方法。本文的diffusionmodel是classifier指导的，所以可以看到句子的全部。</p><p>Plug-and-Play ControllableGeneration：冻结模型，训一个类似分类器的东西。已有方法大多基于自回归模型：</p><ul><li>FUDGE：通过对生成质量的估计来reweight LM prediction</li><li>GeDi, DExperts： 通过fine--tune一个小的LM来reweight LM prediction<ul><li>GeDi的论文没准后面也写个笔记</li></ul></li></ul><p>PPLM方法和本文最接近，通过一个分类器做梯度上升。但由于是自回归的，只能从左到右，不能实现细粒度的控制。</p><h2 id="problem-statement-and-background">Problem Statement andBackground</h2><h3id="generative-models-and-controllable-generation-for-text">GenerativeModels and Controllable Generation for Text</h3><p>如何在控制条件c的情况下生成w，最大化<spanclass="math inline">\(P(w|c)\)</span>.由于 <span class="math display">\[\begin{aligned}P(w|c) &amp; = \frac{P(w)\times P(c|w)}{P(c)} \\&amp; \propto P(w)\times P(c|w)\end{aligned}\]</span> 如果对w的生成求导： <span class="math display">\[\bigtriangledown_w \log P(w|c)  = \bigtriangledown_w \log P(w)+\bigtriangledown_w \log P(c|w)\]</span>可以发现，右边是正常模型的输出的梯度，后面是某个分类器的输出的梯度。这个其实就是经典的在图像领域做可控生成的diffusionmodel的方法</p><h3 id="autoregressive-language-models">Autoregressive LanguageModels</h3><p><span class="math display">\[P_{lm}(w) = P_{lm}(w_1) \sum_{i=2}^N P_{lm}(w_i| w_{&lt;i})\]</span></p><p>这是一个经典的从左到右的解码过程。一般大家都是用一个transformer结构来做<spanclass="math inline">\(P_{lm}(w)\)</span></p><h3 id="diffusion-models-for-continuous-domains">Diffusion Models forContinuous Domains</h3><p>这一部分是DDPM的子集，可以参考</p><ul><li><a href="/75bfb84b.html" title="论文阅读:Diffusion-LM Improves Controllable Text Generation(1)">DDPM笔记</a></li><li><ahref="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">whatare diffusion models</a></li></ul><p>结论是： <span class="math display">\[\mathcal{L}_{simple} = \sum_{t=1}^T \mathop{\mathbb{E}} \limits_{q(x_t|x_0)} || \mu_\theta(x_t,t) - \hat{\mu}(x_t,x_0)||^2 \\\hat{\mu}(x_t,x_0) = \text{mean} [q(x_{t−1}|x_0, x_t)]\]</span> 大概就是估计模型对每一个<spanclass="math inline">\(x_{t-1}\)</span>都做估计，把估计和实际值的偏差叠加在一起。所谓的"mean"是指后面来自一个高斯分布，这个是期望</p><h2id="diffusion-lm-continuous-diffusion-language-modeling">Diffusion-LM:Continuous Diffusion Language Modeling</h2><p><img src="../files/images/Diffusion-LM/structure.png"></p><p>其实这一部分开始才是本文的方法：</p><ul><li>首先用一个EMB层把离散的word映射到连续空间</li><li>定义一个端到端的training objectives</li><li>定义一个rounding method再把生成的连续向量映射回word</li><li>同时，为了rounding的效果，提出了专门的训练策略</li></ul><h3 id="end-to-end-training">End-to-end Training</h3><p><span class="math display">\[EMB(w) = [EMB(w_1), . . . , EMB(w_n)] \in R^{nd}.\]</span></p><p>是一个词向量映射，把n个word映射成n个d维向量，这个是可学习的。</p><p>接下来把 <span class="math display">\[q_\phi (x_0|w) = \mathcal{N} (EMB(w), \sigma_0I)\]</span> 视做diffusion process的第一步(其实<spanclass="math inline">\(\phi\)</span>就是embedding层模型……)</p><p>此外，添加rounding过程 <span class="math display">\[p_\theta(w | x_0) = \sum^n_{i=1} p_\theta(w_i | x_i)\]</span> <span class="math inline">\(p_\theta(w_i |x_i)\)</span>就是一个经典的softmax变换。</p><p>最后： <span class="math display">\[\begin{aligned}\mathcal{L}_{vlb}(x_0) &amp; = \mathbb{E}{(x_0\sim p_{data})}  [\logp_\theta (x_0 )] \\&amp; = \mathop{\mathbb{E}}\limits_{x_{1:T}|x_0} \left[ \log\frac{P_\theta(x_T | x_0)}{P_\theta (x_T)} + \sum_{t=2}^T \log\frac{q(x_{t−1}|x_0, x_t)}{P_\theta(x_{t-1} | x_t)}  -\logP_\theta(x_{0} | x_1) \right]\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}\mathcal{L}_{vlb}^{e2e}(w) &amp; =\mathop{\mathbb{E}}\limits_{x_0|w}\left[\mathcal{L}_{vlb}(x_0) +\logq_\phi(x_0 | w) - \log p_\theta(w | x_0) \right] \\&amp; = \mathop{\mathbb{E}}\limits_{x_{0:T}|w} \left[\mathcal{L}_{simple}(x_0) + ||EMB(w)−\mu_\theta (x_1 ,1) ||^2 -\logp_\theta(w | x_0) \right]\end{aligned}\]</span></p><p>这是一个端到端的训练loss。只需要给出很多w，用DDPM的算法，就可以慢慢让模型学习了。作者也发现，这样模型和embedding一起学出来，最后wordembedding也可以表现出一定的聚合性</p><p><img src="../files/images/Diffusion-LM/embedding.png"></p><h3 id="reducing-rounding-errors">Reducing Rounding Errors</h3><p>rounding的过程其实是对每一个位置选取最接近的embedding。</p><p>作者提到，如果像一般的DDPM那样用模型预测$p_(x_{t−1} | x_t)$的均值，会导致输入长度很短的时候效果很差(图像领域没出问题，因为最短也是32x32)。</p><p>解决办法是用模型直接预测<span class="math inline">\(x_0\)</span>.<span class="math display">\[\mathcal{L}_{x0-simple}^{e2e}(x_0) = \sum_{t=1}^T \mathbb{E}_{x_t}||f_\theta(x_t,t) - x_0||^2\\f_\theta(x_t,t) \to x_0\]</span>这里有点怪：换模型的目标好像会影响sample方法？还是说同样是预测<spanclass="math inline">\(x_{t-1}\)</span>，但是把<spanclass="math inline">\(x_0\)</span>视为<spanclass="math inline">\(x_{t-1},x_t\)</span>的线性组合</p><p>这样模型可以很快发现<spanclass="math inline">\(x_0\)</span>的每一个位置都应该对应某个word的embedding。</p><p>同时，修改原来的sample方法： <span class="math display">\[x_{t−1} =\sqrt{\overline{\alpha}}f_\theta(x_t,t)+ \sqrt{1-\overline{\alpha}} \epsilon \\\overline{\alpha} = \sum_{s=0}^t (1 -\beta_t) ,\quad \epsilon \sim\mathcal{N}(0,I)\]</span> (能成立是因为连续的高斯采样等价于从最开始只做一次高斯采样)</p><p>上面的方法随着t的递减，逐渐从随机噪声恢复成一个无噪音的数据</p><p>如果希望模型可以显式地对齐某个embedding，可以在每一步都加一个clamp。取和<spanclass="math inline">\(f_\theta(x_t,t)\)</span>最接近的word embedding<span class="math display">\[x_{t−1} =\sqrt{\overline{\alpha}} \text{Clamp}(f_\theta(x_t,t))+\sqrt{1- \overline{\alpha}} \epsilon\]</span> 同时作者也提到，对于t很大时做clamp是一个负优化，因为这时<spanclass="math display">\[f_\theta(x_t,t)\]</span>还没有对应到某些embedding，因此选取clamp的开始时间是一个超参。</p><h2 id="decoding-and-controllable-generation-with-diffusion-lm">Decodingand Controllable Generation with Diffusion-LM</h2><h3 id="controllable-text-generation">Controllable Text Generation</h3><p>正如前面提到的： <span class="math display">\[\bigtriangledown_{x_{t-1}} \log P(x_{t-1}|x_t,c)  =\bigtriangledown_{x_{t-1}} \log P(x_{t-1} | x_t)+\bigtriangledown_{x_{t-1}} \log P(c|x_{t-1})\]</span>其中式子右边的第一项是Diffusion-LM的输出。右边是一种分类器的输出。如果对于目标任务，我们在目标数据集训练一个分类器，在sample时每一步都根据这个输出做更新，那么最终就会生成符合条件c的结果。</p><p>同时作者为了细粒度的控制”可控性“，定义一个超参<spanclass="math inline">\(\lambda\)</span>来平衡流畅度和控制程度 <spanclass="math display">\[p(x_{t−1} | x_t, c) = \lambda \log P(x_{t-1} | x_t)+\log P(c|x_{t-1})\]</span> 作为最终用来指导梯度的东西。作者提到每次得到<spanclass="math inline">\(x_t\)</span>之后：</p><ul><li><p>sample <span class="math inline">\(x_{t-1}\)</span></p></li><li><p>再做三轮Adagrad更新</p></li><li><p>把sample步骤从2000步变成200步来加速</p></li></ul><p>这个方法我理解类似于之前微分方程那篇文章提到的PC更新方式</p><h3 id="minimum-bayes-risk-decoding">Minimum Bayes Risk Decoding</h3><p>这个方法可以得到一个高质量的结果。作者用下面的最小化risk函数： <spanclass="math display">\[\hat{w} = \mathop{\text{argmin}}_{w \in S} \sum_{w&#39; \in S}\frac{1}{|S|} \mathcal{L}_{bleu}(w,w&#39;)\]</span> 这种方法可以参考 <a href="/f1fcbd7.html" title="论文阅读[粗读]-Minimum Bayes-Risk Decoding for Statistical Machine Translation">MBR解码博客</a></p><h2 id="experimental-setup">Experimental Setup</h2><p>作者在很多个任务上使用了Diffusion-LM方法：</p><ul><li>classifier-guided</li><li>classifier-free：不需要分类器，比如说句子长度。只要最开始的初始化向量变成需要长度即可(控制率显然是100%)</li></ul><p>Diffusionmodel里面的内部解码层作者使用了一个非常传统的transformer结构，80M参数，可以说是很少:</p><ul><li>步骤T = 2000</li><li>词向量维度 d = 16-128</li><li>句长n=64</li></ul><p><img src="../files/images/Diffusion-LM/example.png"></p><p>输入就像上图input所示：通过分类器来在解码中每一次去噪都提供梯度</p><h2 id="result">Result</h2><p><img src="../files/images/Diffusion-LM/result.png"></p><p>主要结果就如上图所示。</p><ul><li>基本上把SOTA得分翻倍了，但这个SOTA是指plug-and-play的方法</li><li>在Syntax Tree， SyntaxSpans任务上，甚至比fine-tune的模型效果还好</li></ul><p><img src="../files/images/Diffusion-LM/compose.png"></p><p>同时作者也做了综合的控制实验，比如说同时限定两个条件。可以看出，效果很好，方法就是单纯的把控制的梯度转换成两个classifier梯度的综合。</p><p><img src="../files/images/Diffusion-LM/infilling.png"></p><p>在infilling任务中，Diffusion-LM在没有做额外训练的情况下就表现很好</p><h2 id="我的思考">我的思考</h2><p>这篇工作很新颖，很有启发，我觉得可以做的点还有以下两个方面：</p><ul><li>这个方法好像不能实现平行的转换，只能”根据某些控制生成特定的句子“。像之前DALL.E都是可以把特定的句子翻译成对应的图像。</li><li>感觉短文本领域，只有更加平行，才能更加泛用。不过在长文本领域，可能这种没有平行的生成会更好，但作者并没有测试。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
            <tag> 可控文本生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-5总结</title>
      <link href="/4d63b6f9.html"/>
      <url>/4d63b6f9.html</url>
      
        <content type="html"><![CDATA[<p>今天和几个老朋友一起吃了个饭，聊了聊不同学校的最近都是什么情况，挺开心的。</p><p>另外，今天开了暑假的第一次论文分享，讲了关于repetition的论文，收获良多。</p><span id="more"></span><p>最后，今日观赛：</p><ul><li>AL打WBG：第一把没看，wbg赢了。但后两把AL竟然把wbg赢了？？最近怎么都是怪局，edg病毒这是把wbg也感染了吗…</li><li>预报：周五，t1打gen.g</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-4总结</title>
      <link href="/eb14bd4d.html"/>
      <url>/eb14bd4d.html</url>
      
        <content type="html"><![CDATA[<p>昨天晚上失眠了，呆到了4点多才睡着，感觉蝉鸣好刺耳。不知道是不是咖啡的原因，确实有几天没喝咖啡了……这波是抗药性衰退</p><span id="more"></span><p>今天读了一篇论文，然后又喝了几杯马黛茶。</p><p>今日观赛：</p><ul><li><p>TT打BLG：TT今天换了三个人，还包括春季赛在dk，夏季赛被Nuguri卷过来的Hoya，确实挺c。赢BLG不意外，关键是感觉crisp真的融入了……</p></li><li><p>OMG打EDG：只能说黑色对标大哥易主了。edg输了，经典卫冕冠军，厉害呀shanji哥</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-BOLT: BRIDGING THE GAP BETWEEN AUTO-TUNERS AND HARDWARE-NATIVE PERFORMANCE</title>
      <link href="/39e1c917.html"/>
      <url>/39e1c917.html</url>
      
        <content type="html"><![CDATA[<p>上次读了陈天奇关于auto-tuner中使用动态形状的论文，这篇论文是解决利用硬件信息的。本篇工作重新思考了图、算子、模型粒度下的优化方式，打算更好的在auto-tuner中利用硬件信息，达到vendorlibrary相似的效果。</p><h2 id="introduction">Introduction</h2><p><img src="../files/images/BOLT/compare_with_lib.png"></p><p>作者首先点出来一个事实，那就是现在的两种常用方法：auto-scheduler和vendorlibrary的效果差的很远，如上图差了5倍。作者寻找原因：</p><ul><li>auto-tuner没有考虑硬件信息，硬件不透明，导致不能利用很多硬件原生的加速</li><li>vendor library考虑硬件实现，虽然拓展性更差，但效果会更好</li></ul><p>作者还提到，不仅是效果，编译时间也差的很多：ResNet-50在NVIDIAGPU需要编译7天。作者提到一个可能的解决办法是通过特别的数据库来更好的利用cache信息。但这种方法并不长远，因为模型的动态性和输入的动态性让cache很难得到高效地利用。</p><p>本文想要弥补这两种方法的gap。观察到一个事实：现有的vendorlibrary有一个模板化的趋势(templated)</p><ul><li>NVIDIACUTLASS：给出一些模板，不对具体的函数负责，比较好的扩展性。</li><li>Intel OneDNN (Intel) ，AMD ROCm：类似的特性</li></ul><p>作者提出了BOLT，代码已经merge到TVM：</p><ul><li>首先，用vendor library支持的模板来搜索优化模板，利用hardware-nativeperformance，生成tensor program</li><li>接下来，通过排列模板进行计算图优化</li><li>可以用hardware-nativeperformance来同时进行图级别和算子级别的优化</li><li>也可以通过符合提出的设计原则的模型来进行模型级别的优化</li></ul><p>作者的主要贡献：</p><ul><li>结合templated vendor library和auto-tuner来弥补前面的表现差异</li><li>提出persistent kernel fusion，</li><li>提出寻找模板参数的方法，可以直接生成常规的vendor library代码</li><li>总结了三个系统友好的设计理念</li><li>详细评测了BOLT的效果，远超SOTA</li></ul><h2 id="background-and-motivation">BACKGROUND AND MOTIVATION</h2><h3 id="auto-tuners-have-a-performance-gap">Auto-tuners have aperformance gap</h3><p>一方面，Auto-tuner达不到硬件原生的表现。因为比如NVIDIA特殊的硬件结构，tensor核心，FP16加速机制是Ansor达不到的,因为对于auto-tuner来说硬件是不透明的。</p><p>另一方面，Auto-tuner编译速度慢。一种解决办法是利用cache，重复使用前面的tuninglog。对于静态的模型，效果不错。但对于动态模型、动态输入，只有运行时才能获取真正的工作流，效果不好。本文的Bolt方法可以减少编译时间。</p><h3 id="the-emerging-trend-templated-libraries">The emerging trend:Templated libraries</h3><p><img src="../files/images/BOLT/template.png"></p><p>作者提到一些模板化的vendorlibrary可以方便的实现新函数，实现新模板。这些templatedlibraries可以考虑硬件的实现，达到硬件透明的auto-tuner达不到的效果。</p><ul><li>CUTLASS：NVIDIA的templated library。给每个GEMM（矩阵乘）的CUDAmodel的每一个layer都做了c++的模板。带入正确的参数( tile size, datatype等)，就可以实现电压、安培架构、优化FP优化、混精度计算等要求。</li></ul><p>上图的例子是在计算<span class="math inline">\(C = A ·B\)</span>,C是绿色。例子表明CUTLASS可以自动利用起局部的cache(容量更小但是更快)</p><h3 id="bolt-the-best-of-both-worlds"><strong>Bolt</strong>: The best ofboth worlds</h3><p>新的模板化vendor library提供下列特性：</p><ul><li>可重复利用的达到硬件原生性能的primitives</li><li>对于不同的输入格式，可以很方便的参数化</li><li>更好的进行auto-tuning search</li><li>基础类可以很好地自定义、扩展、封装成更复杂的实现，进一步提高搜索效果</li></ul><p>这些特性带来下面的编译器设计：</p><ul><li>更轻量化，更好的考虑硬件的实现</li><li>可以直接利用上面的templated library</li></ul><p><strong>图级别</strong>：Bolt可以更好的进行operator fusion。提出了<em>persistent kernel fusion</em>的方法，原有的auto-tuner可以实现更深层次的fusion，但得出的算子不被cuDNN支持，但Bolt的可以。扩展了graph-optimization的方向。</p><p><strong>算子级别</strong>：原生的templatedlibrary太底层，并且需要集成到模型中。Bolt通过设计一个 light-weightperformanceprofiler来自动的进行模板参数的搜索。可以在缩减搜索时间的同时，让生成的tensorcode达到硬件原生的性能。</p><p><strong>模型级别</strong>： 符合设计理念的模型可以被更好的优化。</p><h2 id="bolt-design">BOLT DESIGN</h2><p><img src="../files/images/BOLT/workflow.png"></p><p>整个Bolt工作流如上图：</p><ul><li>对于一个特定的框架的模型，先通过TVM的前端生成一个relay graph</li><li>接下里，调用deep fusion对graph做优化</li><li>再接下来，同时用TVM和Bolt的templatedlibrary参数搜索对两种子图做优化</li><li>最后，把生成的tensor code编译到一起</li></ul><h3 id="enabling-deeper-operator-fusion">Enabling deeper operatorfusion</h3><p>这一部分讲了如何做复杂的op fusion：</p><ul><li>可以减少内存读取的时间损失</li><li>可以减少部署延迟，对于小的batchsize</li><li>增加的可优化的空间</li></ul><p>作者提到，Bolt第一步是先用CUDLASS自带的epiloguefusion方法优化一次，然后在此基础上继续优化。</p><p><img src="../files/images/BOLT/fusion.png"></p><h4 id="persistent-kernel-gemmconv-fusion"><em>Persistent kernel(GEMM/Conv) fusion</em></h4><p>这一部分的大体逻辑是如果做更深层的fusion，上一轮的运算结果可以存在寄存器或者sharedmemory里，更快。 <span class="math display">\[D_0 = \alpha_0 A_0 W_0 + \beta_0 C_0 \\D_1 = \alpha_1 D_0 W_1 + \beta_1 C_1\]</span> 像这种组合的式子，如果合成一个算子，如上图。就能减少运算，</p><p><strong>Key property: Threadblockresidence</strong>：管家在于算GEMM2的时候可以不用从globalmemory读取前面的输出。 作者提到一种 <em>threadblockresidence</em>，如果GEMM2和GEMM1对应的threadblock有相同的memory(sharedmemory或register files)。没有一致性的话，又要去globalmemory取数据，等于白优化。</p><blockquote><p>查了一下：好像thread是cuda里的变成概念，可以硬件并行化。然后不同thread组成了叫block的概念。我理解大概是比如矩阵运算的时候会做很多的并行，然后这一堆”线程“就放在一个快里，共享一堆内存。</p></blockquote><p><img src="../files/images/BOLT/presis.png"></p><p>后面作者分别讲了在register file级别和shared memory级别这种一致性怎么保持，这一块读了半天都没读懂，实在是专业不对口……大概就是作者修改了一些CUDLASS的代码来保持这种连续操作的存储一致性，没动本身计算的接口。</p><blockquote><p>大家可以看原论文(5-6页)来自己理解一下: <ahref="https://proceedings.mlsys.org/paper/2022/hash/38b3eff8baf56627478ec76a704e9b52-Abstract.html">原论文</a></p></blockquote><p>总结一下，这种deeperfusion可以对连续的GEMM和CONV操作进行合并，通过实现persistent kerneltemplates</p><h3 id="automating-templated-code-generation">Automating templated codegeneration</h3><h4 id="challenges-in-code-generation"><em>Challenges in codegeneration</em></h4><p>作者提到，templatedlibrary只提供一部分算子函数，不提供模型的端到端的、函数化的支持。已有的BYOC方法可以最大化的引入TVM等。但不能解决所有问题：</p><ul><li>templatedlibrary本身不能跑，需要实际的参数。Bolt构建了一个light-weighthardware-native performanceprofiler，可以针对实际工作流搜索模板的最佳参数。</li><li>BYOC把library看做黑箱，编译时用hook链接，这不利于自定义和增量开发。Bolt把函数看做白盒，可以直接生成符合lib规范的函数</li></ul><blockquote><p>注1：BYOC作者有个<ahref="https://www.bilibili.com/video/BV1r54y1W7Zk?spm_id_from=333.337.search-card.all.click&amp;vd_source=8d8e22a2686676c4db7b7429ce8a2e98">介绍视频</a>,大概就是可以让你支持多个编译器后端。在Bolt中就是引入CUDLASS库到TVM里</p></blockquote><blockquote><p>注2：hook。大概就是编译时把这个函数看做一个指针直接指，后面把指针链接到对应的库函数。因此对编译器来说这个函数是黑箱的。</p></blockquote><h4 id="light-weight-performance-profiler"><em>Light-weight performanceprofiler</em></h4><ul><li>传统的自动调优器，通过生成样本并测量其速度来推断 Costmodel，这需要大量的搜索空间和较长的调优时间。BOLT通过将耗时的样例程序生成与性能测量分离，并通过有效利用硬件细节进行加速，大大减少了搜索时间。</li><li>CUTLASS 模板中与性能相关的参数包括 threadblock、warp 和 instructionshapes、swizzling functor和stage 等。</li><li>BOLT 采用白盒方法， 根据 GPU架构以及特定于每个硬件的调优指南确定它们的可能值。</li><li>对于每一个GPU架构，BOLT都会产生数十个最佳参数组合，并通过初始化模板生成对应的示例程序。这些样例程序可通过给定的不同输入跨模型和工作负载重用。不需要用户提供额外的信息</li><li>在运行时，BOLT可以通过调用带有具体输入的预生成示例程序来分析性能。</li></ul><h4 id="templated-code-generation"><em>Templated codegeneration</em></h4><p>传统的BYOC不能支持模板化的库函数。Bolt通过搜出的参数先生成一波函数。其中，用到了以下优化：</p><ul><li>Layouttransformation：对于CONV操作，CUTLASS只支持NHWC内存布局，但所有的pytorch模型都是NCHW布局。BOLT先都转成NCHW，全优化完再都转回NHWC。</li><li>Kernel padding：传统CUTLASS支持各种大小的kernel，但是表现差挺多的。Bolt自动用alignment8，并且提前申请内存。</li></ul><h3 id="designing-system-friendly-models">Designing system-friendlymodels</h3><p>作者提出了以下三个设计原则，我觉得用“设计原则”不太合适，感觉更像是”withBolt，你可以怎么样“：</p><ul><li><p>Exploring different activation functions with epiloguefusion：由于Bolt尝试epiloguefusion，因此用什么激活函数对速度不敏感。</p></li><li><p>Deepening models with 1×1 Convs：由于Bolt有deepfusion，你加一大堆 1x1Convs可以提高准确率，而且模型基本不会变慢</p></li><li><p>Aligning tensor shapes to use GPUs moreefficiently：尽量让用到的tensor的形状是对齐的。</p></li></ul><h2 id="evaluation">Evaluation</h2><p>这一部分还是简略说</p><p><img src="../files/images/BOLT/eva.png"></p><p>结论就是：</p><ul><li>Bolt跑的更快</li><li>Bolt编译更快</li></ul><h2 id="我的思考">我的思考</h2><ul><li>同样是解决 auto-tuner和vendorlibrary效果差距的文章，这篇是另外一个角度。</li><li>感觉vendorlibrary也不是吃干饭的，上次说它扩展性不好，这次人家就有templatedlibrary搞出来了…</li><li>auto-tuner利用硬件信息我觉得是很必要的，这是上次综述文章里讲到的backend应该干的活。但我在想这一部分是不是还是得硬件开发上那边支持，毕竟写硬件的人最懂硬件怎么跑得快w</li><li>说实话，感觉编译方面的论文都有点，啰嗦，就持续地说好多遍同样的东西……</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习编译 </tag>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-3总结</title>
      <link href="/2eb383c3.html"/>
      <url>/2eb383c3.html</url>
      
        <content type="html"><![CDATA[<p>今天基本啥也没干，摆了一天。看了关注了B站up主</p><blockquote><p><ahref="https://space.bilibili.com/8065464/?spm_id_from=333.999.0.0">引力子G</a></p></blockquote><p>他转发一些中文字幕版的youtube科普视频。近日了解了一些级数和混沌、分型的知识。</p><span id="more"></span><p>近日和朋友一起玩《双人成行》，剧情好长，超值。另，现在steam夏促双人成行4折。</p><p>另外，今天看了爱死机第三季，感觉脑洞很大，很好看。我想到一个脑洞：</p><ul><li>有一个机器人叫pangu，负责在中文和英文之间添加空格。通过这一个简单的任务，它逐渐学会了不同的语言，他变得越来越智能。它开始创造自己的语言，它开始认为自己是”开天辟地“的盘古。最终，它”活了“。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-2总结</title>
      <link href="/88c48877.html"/>
      <url>/88c48877.html</url>
      
        <content type="html"><![CDATA[<p>今天读了两篇论文，写了两个笔记，但明天还有更多要读，只能说重量级。</p><p>今天被拉到了一个新群，可能新的锅又要来了。</p><span id="more"></span><p>今日观赛：</p><ul><li>FPX打JDG：无法想象，fpx竟然赢了。我的SUMMIT，新上单新气象</li><li>WBG打TES：达到12点都没结束，只能说今天管泽元输麻了。新一嘉宾当麻了，连坐9个小时。最后还是让tes赢了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-DENOISING DIFFUSION IMPLICIT MODELS</title>
      <link href="/93d2057b.html"/>
      <url>/93d2057b.html</url>
      
        <content type="html"><![CDATA[<p>今天读了一篇编译，接下来回到老本行，来一篇diffusionmodel。这篇工作很有名，有点碰瓷DDPM的意思，其中“IMPLICIT”的意思是隐含。和DDPM的“probabilistic”相比，意思是减少随机性，加速采样。作者自己在AITime上有个报告，好挺好的。</p><blockquote><p><ahref="https://www.bilibili.com/video/BV1M44y1B7bp?spm_id_from=333.999.0.0&amp;vd_source=8d8e22a2686676c4db7b7429ce8a2e98">宋佳铭DDIM报告</a></p></blockquote><p>这篇工作和别的diffusionmodel工作一样，比较数学，很多推导我就不详细写了，可以从附录里看。</p><h2 id="introduction">Introduction</h2><p>大概意思就是：</p><ul><li>GAN很好，但今年的DDPM也不差</li></ul><blockquote><p>21年有一篇OpenAI的 BEATGANS，用了DDIM，关注度很高，意思就是DDPM真的比GAN好</p></blockquote><ul><li>DDPM的问题是慢<ul><li>在CIFAR-10做32x32要20h</li><li>在CIFAR-10做256x256要1000h</li></ul></li><li>本文解决这个问题,通过一个非马尔科夫的过程</li></ul><h2 id="background">Background</h2><p>作者先讲了一般的DDPM是怎么搞的，这个叙述方式挺好的，讲讲重点</p><p>有一个公式： <span class="math display">\[q(x_{1:T} | x_0) := \prod_{t=1}^T q(x_t |x_{t-1}) \\q(x_t |x_{t-1}) := N(\sqrt{\frac{\alpha_t}{\alpha_{t-1}} } x_{t-1},(1-\frac{\alpha_t}{\alpha_{t-1}})I)\]</span> 也就是说，正常的DDPM是一个马尔科夫过程，想要从<spanclass="math inline">\(X_0\)</span>得到后面，需要一步一步加噪声。这个噪声是一个高斯分布，其均值和<spanclass="math inline">\(X_{t-1}\)</span>有关，是sample出来的。</p><p>反向的公式是： <span class="math display">\[q(x_{t−1}|x_t,x_0) = N(x_{t−1};μ_t(x_t,x_0),\hat{\beta_t} I),\]</span> 其中 <span class="math display">\[\text{均值:} \mu_t(x_t,x_0) = \frac{ \sqrt{ \overline{\alpha_{t-1}}}\beta_t }{1 - \overline{\alpha_{t}}} x_0 + \frac{\sqrt{\alpha_t} (1 -\overline{\alpha_{t-1}} )}{1 - \overline{\alpha_{t}}} x_t \\\text{方差:} \hat{\beta_t} = \frac{1 - \overline{\alpha_{t-1}}}{1 -\overline{\alpha_{t}}}\beta_t\]</span></p><p>另一方面，由于这个高斯分布的传递性，我们可以把<spanclass="math inline">\(x_t\)</span>视为单位高斯分布和<spanclass="math inline">\(x_0\)</span>的线性叠加： <spanclass="math display">\[x_t = \sqrt{\hat\alpha_t} x_0 + \sqrt{1 - \hat\alpha_t} \epsilon, \quad\epsilon \sim N(0,I)\]</span> 我们在训练时我们想要让 <span class="math inline">\(\logp_\theta(x_0) \to \log q(x_0)\)</span>，也就是说，我们想要优化： <spanclass="math display">\[L_{t-1} = E_q [ \frac{1}{2\sigma^2} || \mu_t(x_t,x_0) -\mu_\theta(x_t,t)||^2]\]</span> 其中</p><ul><li><p>左边是实际马尔科夫链对<spanclass="math inline">\(x_{t-1}\)</span>的估计，可计算</p></li><li><p>右边是我们的去噪模型<spanclass="math inline">\(\theta\)</span>对<spanclass="math inline">\(x_{t-1}\)</span>的估计，可计算</p></li></ul><p>这里我们套入上面<spanclass="math inline">\(x_t\)</span>的公式，看做<spanclass="math inline">\(x_t,\epsilon\)</span>的函数，进行一波化简，最终得到<span class="math display">\[L_{t-1} = C * \frac{1}{2\sigma^2} || \epsilon -\mu_\theta(\sqrt{\hat\alpha_t} x_0 + \sqrt{1 - \hat\alpha_t},t)||^2\]</span> 忽略常数C，同时对所有的L进行优化： <spanclass="math display">\[L_{\text{simple}} = \sum_{t=1}^T L_t\]</span> 就能学出DDPM模型了</p><h2id="variational-inference-for-non-markovian-forward-processes">VARIATIONALINFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES</h2><p><img src="../files/images/DDIM/compare.png"></p><p>上面全是DDPM的数学推导。接下来，作者讲了他的贡献。它发现：</p><ul><li>DDPM中的loss<spanclass="math inline">\(L_\lambda\)</span>只依赖于<spanclass="math inline">\(q(x_t|x_0)\)</span>，和<spanclass="math inline">\(q(x_{1:T}|x_0)\)</span>无关。这样符合边缘分布的可能性有很多，作者选取了一个非马尔科夫的过程</li></ul><p><span class="math display">\[q_\sigma(x_{1:T} |x_0) := q(x_{T} |x_0) \prod_{t=2}^T q_\sigma (x_{t−1}|x_t , x_0 ) \\q_\sigma (x_{t−1} |x_t , x_0 ) = N \left( \sqrt{\alpha_{t-1}}x_0 +\sqrt{1 - \alpha_{t-1} - \sigma^2_t} \times \frac{x_t -\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}}  , \sigma_t^2 I\right)\]</span></p><p>接下来，对于 <span class="math inline">\(x_0 \sim q(x_0) , \epsilon_t\sim N (0, I)\)</span>我们可以获取其对应的<spanclass="math inline">\(X_t\)</span>，由 <span class="math display">\[x_t = \sqrt{\hat\alpha_t} x_0 + \sqrt{1 - \hat\alpha_t} \epsilon_t\]</span> 如果我们有一个对<spanclass="math inline">\(\epsilon_t\)</span>的预测器<spanclass="math inline">\(\epsilon_\theta^{(t)}(x_t)\)</span>输入不含<spanclass="math inline">\(X_0\)</span>。我们可以用他来预测<spanclass="math inline">\(X_0\)</span>： <span class="math display">\[f^{(t)}(x_t) := (x_t − \sqrt{1 − \alpha_t} ·\epsilon^{(t)}(x_t))/\sqrt{\alpha_t}\]</span> 由此，这个去噪的过程可以看做： <span class="math display">\[p_\theta^{(t)}(x_{t-1} | x_t) = q_\sigma (x_{t−1} |x_t , f^{(t)}(x_t) )\]</span> 式子右边只和<spanclass="math inline">\(x_t,\epsilon\)</span>有关</p><p>作者接下来证明了这个算法的train过程用到的loss和DDPM是等价的(差一个常数)。<strong>也就是说，训好的DDPM模型可以认为是训好的DDIM模型</strong></p><h2 id="sample">sample</h2><p>这个模型是怎么infer的呢？</p><p><img src="../files/images/DDIM/infer.png"></p><p>进一步展开、化简刚才的<spanclass="math inline">\(p_\theta^{(t)}(x_{t-1} |x_t)\)</span>，我们可以得到</p><p><img src="../files/images/DDIM/form2.png"></p><p><img src="../files/images/DDIM/form.png"></p><p>这个式子里面只有最右边的部分是带有随机成分的。而且当 <spanclass="math display">\[\sigma_t = \sqrt{(1 − \alpha_{t−1})/(1 −  \alpha_{t})}\sqrt{1 −\alpha_{t}/\alpha_{t-1}}\]</span> 时退化为DDPM</p><p>如果我们取<span class="math inline">\(\sigma_t =0\)</span>。式子有确定性的输出，这个模型称作DDIM。</p><h3 id="accelerated-generation-processes">ACCELERATED GENERATIONPROCESSES</h3><p>说完了采样，那么加速在哪呢？</p><p>作者证明了：</p><ul><li>上面的逆过程不需要从<spanclass="math inline">\(T,T-1,...,1,0\)</span>一路下降，其实选取一个递减的子集也是可以的！</li></ul><h2 id="实验">实验</h2><p>实验部分，作者实际上选取了： <span class="math display">\[\sigma_t = \eta \sqrt{(1 − \alpha_{t−1})/(1 −  \alpha_{t})}\sqrt{1 −\alpha_{t}/\alpha_{t-1}}\]</span> 其中<spanclass="math inline">\(\eta\)</span>是超参，0代表是DDIM，1代表是DDPM。另一个变量是选取的子集S的大小(<spanclass="math inline">\(|S|=1000\)</span>代表没有简化)，跑了这个图：</p><p><img src="../files/images/DDIM/re.png"></p><p>可以看出，DDIM在步数少的时候表现最好。最下面那个<spanclass="math inline">\(\hat\sigma\)</span>代表原始的DDPM祖先采样。可以看出，衰减非常明显</p><p>作者还提到了这个方法的另一个优势：</p><ul><li>由于确定性的增加，相同的初始噪声映射到基本相同的结果。可以方便做图片的修改。</li></ul><p><img src="../files/images/DDIM/change.png"></p><h2 id="我的思考">我的思考</h2><ul><li>虽然证明和数学过程很复杂，但结论却是惊人的简单：同样的DDPM，换一个采样方法，就能加速50倍</li><li>这个感觉是对上面论文ODE的一种实现？作者在论文里也用一小节说了这事。</li></ul><a href="/6b94db09.html" title="论文阅读-SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS">论文阅读-SCORE-BASED-GENERATIVE-MODELING-THROUGH-STOCHASTIC-DIFFERENTIAL-EQUATIONS</a><ul><li>有个后文 DPM-Solver，10步就能媲美DDPM-1000步，过两天笔记整上</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-DIETCODE: AUTOMATIC OPTIMIZATION FOR DYNAMIC TENSOR PROGRAMS</title>
      <link href="/99874d79.html"/>
      <url>/99874d79.html</url>
      
        <content type="html"><![CDATA[<p>昨天阅读survey之后，今天继续阅读编译优化论文一篇。是MLSys会议上的论文。这个会上全是一些深度学习+系统的文章，好像国内知名度不是很高，甚至不再CCF推荐列表上……</p><p>说回本文，本文最后一个作者是陈天奇，TVM的作者，之前我看了他的经历，挺受鼓舞的，推荐大家都可以看一下~</p><blockquote><p><ahref="https://www.jiqizhixin.com/articles/2019-07-20">陈天奇：机器学习科研的十年</a></p></blockquote><p>由于这个方向我了解甚少，这篇论文还是精度，笔记也做得详细一点。</p><h2 id="摘要">摘要</h2><p>在计算密集的负载下得到高效率的计算是很难的问题。目前解决办法：</p><ul><li>vendor libraries：需要大量工程开发</li><li>auto-schedulers：只支持确定形状。否则scheduler速度会变的非常慢。</li></ul><p>作者观察到只支持确定形状的瓶颈在于：搜索空间是和形状有关的。作者提出新方法，可以构建一个形状无关的搜索空间，在同一空间中、用同样的costmodel来同时搜索所有形状。</p><p>实验表明，新方法的速度和效果均大幅超越SOTA。</p><h2 id="introduction">Introduction</h2><p>随着DL大规模应用，成本变得很高。<spanclass="math inline">\(5\%\)</span>的性能提升可能要多花4000$。</p><p>然而，提升所有算子的效果很难，需要详细的实验、开发。很多研究者应用oneDNN，cuDNN等vendorlibrary。后者的开发时间和更新周期都很长。（一年一更）</p><p>为了解决这个问题，多种auto-schedule负责进行High-levelIR计算定义和Low-levelIR实现之间的过程。然而，现有方法都需要在计算时知道所有位置的形状，这样才能正确地构建搜索空间，考虑所有情况。</p><p>然而，很多工作需要形状不确定。一个方法是对所有形状都遍历优化一遍：</p><ul><li>慢，一个BERT层需要42h来优化</li></ul><p>作者发现，如果把想同类型、不同形状的算子看成一个可变形状的workload，然后只规划一次，可以显著减少计算量。已有方法不能这么做，是因为他们要对不同形状开不用的搜索空间。本文提出一种新的方法DietCode：</p><p>构建一个由micro-kernel构成的shape-generic搜索空间，每个micro-kernel都是形状不固定的。这样就能让DietCode有一个统一的搜索空间。</p><p>DietCode构建的cost-model包括：</p><ul><li>复杂的shape-generic 的部分。需要从micro-kernel中提取loopfeature等信息，需要从硬件上获取参数</li><li>简单的依赖形状的部分。不需要额外信息</li></ul><p>本文的贡献可以归纳为：</p><ul><li>提出通过构建shape-generic的搜索空间解决动态形状的规划问题</li><li>提出DietCode，可以同时规划所有形状，用同样的shape-generic costmodel</li><li>在BERT模型编译中，比SOTA快5.88倍，如果考虑所有形状，快100倍。效果上比auto-schedule好<spanclass="math inline">\(69\%\)</span>,比库方法好<spanclass="math inline">\(18.6\%\)</span></li></ul><h2 id="background">Background</h2><h3 id="现有auto-scheduler工作流">现有auto-scheduler工作流</h3><p>现有的vendor library成本很高，有很多auto-scheduler来替代。</p><p><img src="../files/images/DietCode/input.png"></p><p>其输入如上图：包含一个tensor的表达式，同时对于tensor的形状有附加的说明。</p><p><img src="../files/images/DietCode/workflow.png"></p><p>现有方法如上图左:</p><ul><li>1.通过表达式和形状信息，与硬件的可能优化方式一起构建搜索空间</li><li>2.通过硬件信息构建一个cost model</li><li>3.用某种算法进行学习</li></ul><p>最后搜出来的方法大约比vendor-library方法快3.88倍</p><h3 id="动态形状tensor的程序">动态形状tensor的程序</h3><p>现有方法需要：在优化时知道所有的形状信息。因此不适用于以下需求：</p><ul><li>NAS中，需要搜索网络超参。每组超参的实现，网络的结构都不同(举例，12800种)</li><li>在NLP等序列任务中，输入的长度可能运行时才能确定，比如BERT就是长度1-128</li><li>统一模型，不同层的维度可能都不一样：比如BERT的hiddenstate在不同层可能是<spanclass="math inline">\((768,2304,3072)\)</span></li></ul><h3id="为什么需要新的auto-scheduler框架">为什么需要新的auto-scheduler框架</h3><p>现有方法加一些代码不行吗？</p><ul><li>对于vendor-library：输入性状改变可能带来13倍的性能衰减。这是因为代码里有好多hard-code。改这些东西的代价很大，码量爆炸。</li><li>已有auto-scheduler：现有的工作流，上图左，形状信息都是焊死的。不改代码的话，只能对所有形状遍历，这个速度肯定不可行。</li></ul><p>已有一些auto-scheduler的动态形状改进有一些问题：</p><ul><li>Selectivetuning：把不同形状聚成一个个cluster，然后对每个cluster优化。需要额外的知识，不能完全自动化</li><li>Nimble：在一个大形状上预计算，然后在别的动态形状应用。问题是在一个大形状上的最优不一定可以泛化到别的最优。</li><li>Bucketing：把形状变化区间分成子区间，然后用每个子区间的最大值作为值来计算一次。<ul><li>需要额外计算形状信息，并且由于化简，带来计算不准确。</li><li>由于padding，会出现不必要的计算开销。</li></ul></li></ul><p>综上，已有方法都不好。</p><h2 id="dietcode原理关键因素">DietCode原理、关键因素</h2><h3 id="shape-generic-search-space">Shape-Generic Search Space</h3><p>观察下面这个图：</p><p><img src="../files/images/DietCode/c++.png"></p><p>左边是一个含参数T的代码，右边是已有auto-scheduler的做法：</p><ul><li>可见，他们只尝试了T的因子，比如<span class="math inline">\(T=49,t\in[1,7,49]\)</span>,显然比如t=10时就不准确。</li></ul><p>本文，则考虑硬件约束下，给出了一些micro-kernel。这是符合硬件条件的一些算子，可以一个动态形状的运算可以视为一些micro-kernel的组合，比如下列运算：<span class="math display">\[Y = XW^T \\X :[16*T,768] ,\quad W:[2304,768]\]</span> 其中T是可变的参数。如果我们有一个运算器可以运算<spanclass="math inline">\(128*128\)</span>的矩阵micro-kernel：dense_128x128。</p><p>当T=64时，由于<span class="math inline">\(16·T = 8×128, 2304 =18×128\)</span>可以把他们视为<spanclass="math inline">\(8*18\)</span>的kernel的排列在一起进行运算，如图：</p><p><img src="../files/images/DietCode/kernel.png"></p><p>通过排布使用micro-kernel dense_128x128,我们对于<spanclass="math inline">\(T=1,2,...128\)</span>都可以很方便的实现上面的运算。<strong>也就是说，通过这种设计，我们只需要使用形状无关的micro-kernel，就可以实现任意形状输入的优化</strong></p><p>这种方法要解决几个问题：</p><ul><li>如果T=60，那么就需要一个padding，如上图的下面。也就是说，会带来一些额外计算。要怎样高效利用所有micro-kernel？</li><li>怎样准确评判这种由micro-kernel构成的程序的效率？</li></ul><h3 id="micro-kernel-based-cost-model">Micro-Kernel-based CostModel</h3><p>上面说的第一个问题通过padding可以一定程度上解决。对于后一个问题，需要构建一个新的costmodel。</p><p>已有的cost-model需要输入整个程序，从中抽取一些特征： <spanclass="math display">\[\text{Cost}(P ) = f (\text{FeatureExtractor}(P )) \\f : \text{cost function (e.g., XGBoost)} \\P : \text{complete program } \\\text{Cost} :\text{ compute throughput}\]</span>但是，由于micro-kernel是程序的一种切片，不是整个程序，因此不适用传统costmodel。</p><p>设计新的cost model，发现如果一个动态形状程序P被切片成很多micro-kernelM，那么cost可以拆解成两部分：</p><ul><li><spanclass="math inline">\(f_{MK}\)</span>：这些micro-kernel的损失。和形状无关（多个micro-kernel可以并行，时间是常数）</li><li><span class="math inline">\(f_{adapt}\)</span>：把P切片成M带来的损失。和形状有关，但是计算很简单(比如上图就是算padding的比例)</li></ul><p><span class="math display">\[\text{Cost}_M(P)=f_{MK}(\text{FeatureExtractor}(M))·f_{\text{adapt}}(P,M)\]</span></p><p>式子的左边可以通过已有方法的公式计算，更重要的是，由于M和程序P无关，可以事先预计算。式子的右边计算速度很快。综上，新模型的计算效率很高。</p><h3 id="joint-learning-with-dietcode">Joint Learning with DietCode</h3><p>本文提出了DietCode，基于以下要素：</p><ul><li>一个Shape-Generic Search Space</li><li>一个Micro-Kernel-based Cost Model</li><li>在运行时通过上面的cost公式对确定的输入形状派发一种micro-kernel排列</li></ul><p>和已有方法对比如下图：</p><p><img src="../files/images/DietCode/compare.png"></p><p>可以对所有categories同时学习，因为有统一的cost公式，复杂度是<spanclass="math inline">\(O(1)\)</span></p><h2 id="实现细节">实现细节</h2><p>这种方法作为TVM的auto-scheduler方法</p><h3 id="local-padding">local padding</h3><p>上面提到的由于使用padding，可能会带来性能下降，高达17倍。这是因为引入了很多的branch指令，而且每个branch都要计算数据。</p><p>作者研究了三种已有解决方法，举例子如下图，这个例子有一个数据读取，一个数据修改，一个数据写回</p><p><img src="../files/images/DietCode/padding.png"></p><ul><li>globalpadding：如b。提前算好，不需要在里面做branch，但是需要引入额外的存储空间和padding结果的运算</li><li>Looppartitioning：如c。把程序切成两片，其中上一片无论如何都不会爆。当<spanclass="math inline">\(t \llT\)</span>时效果不错，但这个条件很难保证。</li><li>local padding:在取数据和写回数据时保证正确性，但是在计算时不保证。这个有两个好处：<ul><li>只有计算阶段的branch会大幅影响表现。fetch和writeback的时间可以被已有流水线技术优化掉</li><li>如果计算阶段算了额外算了padding的部分，写回阶段会把它忽略，因此不怎么影响表现</li></ul></li></ul><p>本工作使用local padding方法</p><h3 id="micro-kernel-based-cost-model-具体实现">Micro-Kernel-based CostModel 具体实现</h3><p>cost的计算应该考虑以下部分：</p><ul><li>1、micro-kernel的表现</li><li>2、硬件占有率带来的惩罚，用的micro-kernel越多，瞬时的占有率高，吞吐量大，就是好。</li><li>3、padding带来的额外惩罚，padding的比例越低越好</li></ul><p>这三部分可以这样理解：</p><p><img src="../files/images/DietCode/理解cost.png"></p><p>最终公式如下：</p><p><img src="../files/images/DietCode/cost.png"></p><p>其中<spanclass="math inline">\(f_{PAD}\)</span>和硬件无关，之和micro-kernel设计、程序P有关</p><p>其中<span class="math inline">\(f_{OCC}\)</span>是占有率惩罚，用一个回归模型预测</p><p><img src="../files/images/DietCode/occ.png"></p><p>k，b是可学习的参数，由于每个micro-kernel被派发到不同的核，<spanclass="math inline">\(f_{OCC}\)</span>也可以看做占了多少的核。如果做一个归一化的话：</p><p><img src="../files/images/DietCode/normal.png"></p><p>可以节省一个参数b</p><h3 id="automatic-dispatching">Automatic Dispatching</h3><p>在同时计算完所有的M以后，我们根据： <span class="math display">\[vote(S) = argmax_M(CostM(P(S,M)))\]</span> 这个得分是和吞吐率成正比的，因此越高越好。用argmax</p><p>来对与特定的形状S派发一组micro-kernel的排列M，这个可以遍历。结束以后，我们把<spanclass="math inline">\(&lt;S,vote(S)&gt;\)</span>做成一颗决策树，在运行时通过决策树决定M即可。</p><p>​</p><h2 id="evaluation">EVALUATION</h2><p>这一部分就不详细讲了。简单说一下，就是测试BERT模型的表现，baseline是：</p><ul><li>vendor: cuDNN</li><li>Auto-scheduler: Ansor</li><li>dynamic code generation:Nimble。进一步优化Ansor，计算最大形状，然后应用在所有形状上</li></ul><p>评测指标，全都是越小越好：</p><ul><li>end-to-end latency on the entire model, measured as msec。</li><li>the runtime cost of the generated tensor programs on a singleoperator, measured as μsec and averaged over 100 runs。</li><li>the total auto-scheduling time used, measured as hours</li></ul><p><img src="../files/images/DietCode/end-to-end.png"></p><p>上面这个图可以看出，对于不同的输入维度，DietCode的效果基本最好。同时和vendor的差距小，但是vendor需要人工大码量、长时间的开发。</p><p><img src="../files/images/DietCode/time.png"></p><p>上面这个图是编译时间，虚线代表理论推导，实际根本不可行。可以看出，DietCode非常快。</p><p><img src="../files/images/DietCode/runtime.png"></p><p>上面这个图是指选定某种结构以后，真正的跑的速度。更快。作者测试了dense层(一个动态量)，batchmatmul层(2个动态量)，效果都更好。</p><h2 id="我的思考">我的思考</h2><ul><li>其实我之前也不知道这种“动态形状”的输入是怎么规划的。但我确实感觉到动态形状的支持应该是必要的，基本所有的NLP问题都需要这个。</li><li>看作者的意思，这个功能现在已经上TVM了，我在想这个是不是和操作系统一样：</li></ul><blockquote><p>先有实现，后有理论</p></blockquote><ul><li>后续我可能会继续调研一些TVM的原理，感觉还是挺神奇的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习编译 </tag>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>7-1总结</title>
      <link href="/b92c92ea.html"/>
      <url>/b92c92ea.html</url>
      
        <content type="html"><![CDATA[<p>​其实不是今天回家的，昨天下午就到家了。回家以后感觉比宿舍宽敞多了…除了睡觉有点热。经过昨天收拾工作区以后，我现在工作区已经叠buff了：</p><ul><li>600M光纤</li><li>天空之境金粉轴机械键盘</li><li>罗技304电竞鼠标</li><li>RTX 3060显卡</li><li>双置音响</li><li>144Hz 带鱼屏</li></ul><p>只能说，万事俱备，就差开打</p><p>​但其实我也没有开摆，最起码今天没有。中午12点睡醒，吃个午饭，泡上马黛茶。为了应付小学期，读了一篇35页的survey，由于是不同方向的，看起来特别慢，写了一个句粒度的阅读笔记，一共8000字。晚上为了调解心情，看了个10页的pre-train论文。就要睡觉了。</p><p>​ 另，今日观赛：</p><ul><li>t1打drx：说实话，zeus和oner真强呀，感觉gumayusi就混一混就行了。有点奇怪，guamayusi是春季赛的挂用完了吗？感觉怎么从MSI开始下路就一直打不过，今天直接选塞纳跟打野了……只能说，和去年的gala有的一拼，但今年gala夏季赛挂续上了呀（</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[粗读]-MVP: Multi-task Supervised Pre-training for Natural Language Generation</title>
      <link href="/1256e99b.html"/>
      <url>/1256e99b.html</url>
      
        <content type="html"><![CDATA[<p>​今天看了个又臭又长的survey，回到老本行看个好玩的论文洗洗脑。这篇论文是人大做的，一作是个硕一的学生，看看人家w。这篇文章说的贡献是：拓展了有监督预训练到NLG生成任务中，并且击败了基线BART，同时在多个数据集达到了SOTA。</p><p>​ 正好前两天刚看了GPT3，这个感觉有点“碰瓷”的意思，来读一读。</p><h2 id="introduction">Introduction</h2><p>主要就是表示，本文探索两个问题：</p><ul><li>有监督预训练如何解决NLG问题</li><li>有监督预训练能不能繁华到更多任务上</li></ul><h2 id="有监督预训练">有监督预训练？</h2><p>大家都知道无监督预训练就是收集一些人类语料<spanclass="math inline">\(Y =x_1,x_2,...x_N\)</span>，做一些自回归任务，比如GPT： <spanclass="math display">\[\log P(x_1,x_2...x_N) = \log p(x0) \sum_{i=1}^N \log(x_i | x_{i-1})\]</span> ​做这种马尔科夫链的自回归，然后期望随着PPL的降低，模型可以“学会怎么说话”，生成流畅的、符合人类说话习惯的语料。对于理解模型，就是可以“理解语句的含义”。</p><p>​所谓有监督预训练，就是说提前收集一些已有任务的数据集。数据集的特点是有标注，比如摘要任务是<spanclass="math inline">\((Y,summary)\)</span>。如果收集了大量的、来自不同预训练任务的有标注语料的话，就可以定义一种输入方式，可以把不同的语料和标注一起喂到模型里，继续进行auto-regressive的训练。</p><p>​ 具体到本模型MVP，就是比如 summerization：<spanclass="math inline">\((Y,X)\)</span>，输入是 <spanclass="math display">\[summary: \quad Y \quad[sep] \quad X\]</span> 其他任务也是有一些前缀。</p><p>具体来说，作者归纳了4大类NLG任务，收集了一共60GB的有监督语料，对比：</p><ul><li>GPT用570GB无监督语料</li><li>BART用了160GB无监督语料</li></ul><h2 id="模型结构">模型结构</h2><p>​模型结构是什么呢？作者说和BART是一样的。不仅如此，作者甚至是从BART-large初始化了权重。然后用60GB有监督语料接着训练。</p><p>​ 说实话，我觉得”用BART初始化“是这个有监督预训练成功的关键因素……</p><h2 id="评测">评测</h2><p>作者提到了两种方式：</p><ul><li>full tuning：就是MVP拿过来，做fine-tune</li><li>Parameter-Efficient TuningPerformance：把backbone模型冻结，然后加上soft-prompt进行训练</li></ul><p>作者进行了很多任务的评测，结论是：比BART好。然后和SOTA比，也有提升。</p><p>重点是作者还做了一个泛化测试：</p><ul><li>用训练时没见过的paraphrase、styletransfer任务做测试，效果也比SOTA好。</li></ul><h2 id="我的评价">我的评价</h2><ul><li>我在想一个问题：他找的60GB有监督语料里面有没有后面测试用的数据集呢？<ul><li>作者说跳过了一些训练任务，专门来评测。但据我所知，很多生成任务的数据集构建都是取材于别的生成任务，也就是说：即使没有标签重合，但很可能有数据重合</li><li>好吧无监督预训练也被喷这个事……不追究这个了</li></ul></li><li>比BART是不是显而易见的？<ul><li>用BART参数初始化，当然得被BART好了…不然不是负优化</li><li>最后战胜SOTA是很好，但这个是”有监督预训练“的功效吗？我感觉这个方法更像是”大号的fine-tune“</li></ul></li><li>不过本篇工作有一个很重要的点：<strong>大规模的搞数据集上的auto-regressive也是有用的</strong>。之前的fine-tune都是对输出算loss，现在这个相当于把loss传播到输入也算。</li><li>泛化性能为什么也有提升？感觉这个是后面探索空间最大的一个点。我个人观点：<ul><li>用”超越SOTA“作为”泛化性能强“是否有点不公平。对别的模型来说所有任务都是”泛化“。我觉得更合理的s实验是”新任务中MVP战胜SOTA的幅度和老任务一样“</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 预训练模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读[精读]-The Deep Learning Compiler: A Comprehensive Survey</title>
      <link href="/f3519427.html"/>
      <url>/f3519427.html</url>
      
        <content type="html"><![CDATA[<p>​这是我读的第一篇深度学习编译方向的论文，是一篇survey工作。由于是第一篇，那基本上就得逐字逐句慢慢看了，又因为是survey，那二三十页是跑不了了。因此我为了加深印象，干脆就同步更新一下笔记，就当做”把论文翻成中文“了。</p><h2 id="摘要">摘要</h2><ul><li>学术界提出了XLA, TVM等深度学习编译技术</li><li>DL compiler输入模型，输出适用于不同平台的、优化后的代码</li><li>本篇工作聚焦于multi-level IR，前后端的优化技术。</li><li>提出了对这些技术的作者的点评，未来的改进方向</li><li>这是第一篇深度学习编译优化的综述文章</li></ul><h2 id="introduction">Introduction</h2><p>DL硬件分类：</p><ul><li>软硬件联合设计的 general-prupose 硬件<ul><li>线性代数库BLAS支持DL运算</li><li>还有MKL-DNN，cuDNN等专门的加速库</li><li>还有TensorRT等加速库</li></ul></li><li>专为DL设计的硬件<ul><li>类似的加速库</li></ul></li><li>从生物脑科学获得灵感的神经元硬件</li></ul><p>库的更新经常赶不上硬件更新的速度，因此很难利用好DL硬件的算力</p><p>DL编译器</p><ul><li><p>优化 模型定义-&gt; 实现代码 这一过程</p></li><li><p>算子合并、层合并等方法</p></li><li><p>是层次性的。</p></li></ul><p>本篇工作的贡献：</p><ul><li>归纳了前端、多层IR、后端级别的优化方法</li><li>分裂归纳了已有的DL编译器</li><li>定量对比了不同编译器在CNN的效率提升（end-to-end和单层粒度下）</li><li>提出了DL编译器的未来方向： dynamic shape and pre-/post-processing,advanced auto-tuning, polyhedral model, subgraph partitioning,quantization, unified optimizations, differentiable programming andprivacy protection</li></ul><h2 id="background">Background</h2><p><img src="../files/images/DL_compiler_survey/framework.png"></p><h3 id="dl框架对比">DL框架对比</h3><ul><li>tensorflow：其他语言支持最广泛：C ++, Python, Java, Go, R, andHaskell。TensorFlowLite为Android设计。Keria作为tensorflow的前端。eager-mode类似pytorch的动态计算图</li><li>keria：高层神经网络库，快速构建model。兼容scikit-learn等库。由于过度封装，增加算子、获取底层信息很难。</li><li>Pytorch：动态框架，按行执行。合并了Caffe2。FastAI是pytorch的高层封装，借鉴了keria。</li><li>Caffe/Caffe2,MXNet,CNTK,PaddlePaddle略过</li><li>ONNX：别的框架模型可以转换成ONNX模型，以便于交互（使用pytorchAPI）。</li></ul><h3 id="深度学习硬件">深度学习硬件</h3><ul><li>General-purposeHardware：类似NVIDIA-GPU。同时上层有cuDNN等支持，软硬件联合开发。</li><li>Dedicated Hardware：A100，cloud service provider（google TPU）等等<ul><li>TPU包含：<ul><li>Matrix Multiplier Unit (MXU),</li><li>Unified Buffer (UB),</li><li>and Activation Unit (AU)</li></ul></li><li>TPU由host硬件 进行CISC指令集的驱动</li><li>TPU可以直接把矩阵视为单元，而不是vector或者标量集</li></ul></li><li>Neuromorphic Hardware：IBM’s TrueNorth and Intel’s Loihi.<ul><li>神经元可以同时存储和处理数据，存储区和运算区在一起，没有专门存储区。</li><li>距离大规模应用很遥远</li><li>探索rapid, life-long learning。</li></ul></li></ul><h3 id="fpga">FPGA</h3><p>cpu/gpu泛用性强，但是费电；AISC省电，但是转为某任务设计。编程FPGA是一个折中，把模型部署在FPGA上。</p><p>High-Level Synthesis (HLS) programmingmodel帮助用户编辑FPGA，不用写很多verilog。</p><p>FGPA应用的问题：</p><ul><li>AI模型用框架描述，而不是c/c++</li><li>DL特有的优化方式很难推广到FPGA</li></ul><p>hardware-specific codegenerator：输入DL模型，输出HLS/verilog/VHDL代码，再编译成bitstream烧录:</p><ul><li>The processor architecture:目标FPGA和正常处理器类似。</li><li>The streamingarchitecture：目标FPGA是个流水线。快，瓶颈是FPGA内的memory不够</li></ul><h3 id="dl编译器结构">DL编译器结构</h3><p><img src="../files/images/DL_compiler_survey/compiler_architecture.png"></p><p>正常编译器分为前端和后端。中间由IR联系起来，IR分为多个层次(multi-levelIR)。</p><p>前端基于高层IR做硬件无关的优化；后端基于低层IR，做硬件专有的优化、代码生成、编译。</p><ul><li><p>High-level IR:定义计算和控制流，硬件无关。目标是得到程序的控制流和数据依赖性，提供图级别的优化。</p></li><li><p>Low-level IR:足够细粒度。需要支持后端的第三方工具链。</p></li><li><p>前端： 从node-level，block-level, dataflow-level优化模型，生成graph-IR.</p></li><li><p>后端：输入high-level IR,输出 low-level IR。 可以直接把high-levelIR转成LLVMIR等第三方工具链支持输入做优化，也可以通过已有的硬件结构、模型结构做自己的特殊优化</p></li></ul><h2 id="dl编译器的关键因素">DL编译器的关键因素</h2><h3 id="high-level-ir">High-level IR</h3><h4 id="high-level-ir计算图的表示">High-level IR计算图的表示</h4><p>图的表示法不同，优化方向也不同：</p><ul><li>DAG图：有向无环图，点表示算子，边表示tensor。可以快速分析数据依赖。优点是方便优化，缺点是图上的节点、边语义不明确</li><li>Let-binding-basedIR：对每一个变量”let“建立节点，语义明确。它对每一个let都算结果，建立一个map。每个表示通过查表找到结果</li></ul><p>TVM等方法借鉴两种表示，博才两家之长。</p><p>tensor计算的表示法：</p><ul><li>Function-based：Glow, nGraph andXLA采用，提供一些封装的算子。tensor计算视为一种函数</li><li>lambda expression:用lambda函数表达，不需要声明新函数。TVM采用此方法，需要先计算输出的形状。</li><li>Einsteinnotation：比lambda表达式更简单。算子需要相关联，可交换，方便并行化。</li></ul><h4 id="graph-ir-的实现">Graph-IR 的实现</h4><p>数据(tensor)的表示:</p><ul><li><p>Placeholder:标记tensor的shape，有这个就有shape。</p></li><li><p>Unknown (Dynamic) shaperepresentation：允许某一维的大小未知</p></li><li><p>Data layout：逻辑地址到内存分片的映射。</p><ul><li>TVM and Glow把获取datalayer作为一个专门的算子，这样不需要专门实现这个方法，更方便优化。</li><li>XLA把这个视为后端硬件的一个约束</li><li>Relay and MLIR要求tensor在type中描述layout</li></ul></li><li><p>Boundinference：推断迭代器的上下界。TVM中iterator建立一个dag图，点代表迭代器，边代表运算。根节点的shapesof placeholders被确定，就能递归处理。</p></li></ul><p>graph-IR需要支持很多算子：代数算子、tensor算子、controlflow算子。举例子：</p><ul><li>broadcast：放宽一般算子对形状的依赖性。</li><li>control flow:if或者while</li><li>Derivative：自动求导。</li><li>Customized operators：自定义算符。<ul><li>Glow中定义算符需要实现多层的封装</li><li>TVM、TC只需实现一个implementation</li></ul></li></ul><p>不同编译器有不同的 graph-IR实现，但有一定的相似之处。需要注意，graph-IR一定要是硬件无关的。</p><h3 id="low-level-ir">Low-level IR</h3><h4 id="实现">实现</h4><p>不同的实现</p><ul><li>Halide-basedIR：设计哲学是计算和规划分离。可以试多种规划，选一个最好的。原始方法需要形状确定，TVM改进了它：<ul><li>取消LLVM的依赖</li><li>重构project module</li><li>提高复用性，方便自定义算子</li><li>保证每个变量只有一个定义位置</li></ul></li><li>Polyhedral-basedIR：视为多面体。loop的大小更加灵活，方便采用polyhedraltransformations优化方法。有多种优化器：isl,Omega,PIP,Polylib,PPL</li><li>Other unique IR: 他们用一些自定义的优化方法，然后编译成LLVM IR<ul><li>GLOW：包含declare和program两种操作。用@in,<span class="citation"data-cites="out">@out</span>,<span class="citation"data-cites="inout帮助分析内存优化的时机">@inout帮助分析内存优化的时机</span></li><li>MLIR：受到LLVM影响。用dialect提供对别的IR的抽象，包含：TensorFlowIR, XLA HLO IR, experimental polyhedral IR, LLVM IR, TensorFlow Lite。自定义dialect很简单，方便开发者适配新硬件。</li><li>HLO IR of XLA：同时是high/low-levelIR。足够细粒度，提供硬件级的优化，生成 LLVM IR</li></ul></li></ul><h4 id="代码生成">代码生成</h4><p>绝大多数都是生成LLVMIR，接下来通过LLVM进行多种优化。需要提供几种优化：</p><ul><li>循环转换</li><li>提供目标硬件的额外信息</li></ul><h3 id="前端的优化">前端的优化</h3><p>前端进行计算图的优化，和硬件实现无关。前端的优化称为pass，通过多次遍历图，每轮进行不同的操作。一旦模型被import、转换成graph，前端可以获取各个地方的shape。</p><h4 id="node-level-optimizations">Node-level optimizations</h4><ul><li>消除不必要的节点：比如sum（1,0），Nop Elimination</li><li>Zero-dim-tensorelimination：消除0维向量，或者消除某个维度为0的向量</li></ul><h4 id="block-level-optimizations">Block-level optimizations</h4><ul><li>消除单位运算+0,<span class="math inline">\(*1\)</span></li><li>用简单运算符替代复杂运算符</li><li>预计算常数</li></ul><p>DL运算符也可以优化，比如：</p><ul><li><p>optimization of computation order：<spanclass="math inline">\(A^T B^T = (BA)^T\)</span></p></li><li><p>optimization of nodecombination:比如把多个transpose消成一个</p></li><li><p>optimization of ReduceMean nodes:用AVGPool代替ReduceMean</p></li><li><p>Operatorfusion：算子合并。难点是如何合并包含多个reshape,boardcast,reduce等节点的算子。</p></li><li><p>Operator sink：消除可以消除的节点</p></li></ul><h4 id="dataflow-level-optimizations">Dataflow-level optimizations</h4><ul><li>Common sub-expression elimination(CSE)：预计算可以计算的控制流。</li><li>Dead code elimination(DCE)：如果结果不用，就是dead，不需要存中间结果。经常在上面的优化以后出现</li><li>Static memory planning，对于内存受限的机器很重要：<ul><li>in-place memory sharing：复用输入和输出的memory</li><li>Standard memorysharing：在不覆盖的情况下，复用前面用到的存储空间</li></ul></li><li>Layout transformation：<ul><li>计算出tensor最好的存储形式，然后在计算图增加layouttransformation节点。</li><li>最优方式随tensor计算公式不同，硬件不同而不同</li><li>速度提升明显</li></ul></li></ul><h3 id="后端的优化">后端的优化</h3><h4 id="hardware-specific-optimization">Hardware-specificOptimization</h4><p><img src="../files/images/DL_compiler_survey/hardware_optimization.png"></p><p>一种方式是编译到 LLVMIR，另一种方式是定义自己的优化方式，用模型的信息。举5个例子：</p><ul><li><p>Hardware intrinsicmapping：编译成硬件已经优化过的一些kernel。</p></li><li><p>Memory allocation andfetching：针对硬件上不同存储的延迟不同做优化</p></li><li><p>Memory latencyhiding：等内存延迟的时候干别的，好多硬件自己实现了，但TPU没有。TVM通过虚拟线程解决这事</p></li><li><p>Loop oriented optimizations：Halide和LLVM实现过了。</p><ul><li>Loop fusion: 把多个边界一样、没有依赖的循环放到一起</li><li>Sliding windows：直到需要数据时再计算</li><li>Tiling：把一个循环拆成多重循环，更好的局部性</li><li>Loop reordering：重新排列循环顺序。让空间局部的循环们放在一起</li><li>Loop unrolling：循环展开，更好地做指令级并行。</li></ul></li><li><p>Parallelization：更好的支持线程级并行等，提高硬件利用率。需要额外的、模型的知识，更难开发。</p></li></ul><h4 id="auto-tuning">Auto-tuning</h4><p>由于各种优化很多，搜索空间很大，需要自动搜索优化方式。TVM,TC,XLA都支持这个。包含四个要素：</p><ul><li>Parameterization：搜索参数时需要知道的一些已有参数<ul><li>Data and target：tensor的形状，GPU上各个memory的延迟、大小</li><li>Optimizationoptions：优化方法有什么，对应什么超参。TC，XLA支持把超参参数化，比如batch_size。</li></ul></li><li>Cost model：评价优化方法的模型<ul><li>Black-box model：只关心运行时间，对内部情况不关心。TC采用</li><li>ML-based cost model: 用ML模型衡量现在情况有多好。TVM，XLA采用</li><li>Pre-defined cost model:预先定义好。搜索更快，但是定义很难，和DLmodel有关。</li></ul></li><li>Searching technique：如何搜索<ul><li>Initialization and searching spacedetermination：随机/特定初始化。TVM允许用户决定搜索空间</li><li>Genetic algorithm (GA) ：类似蚁群算法，TC采用</li><li>Simulated annealing algorithm (SA)：模拟退火。TVM采用</li><li>Reinforcement learning (RL): Chameleon采用（基于TVM开发）</li></ul></li><li>Acceleration：如何加速搜索过程<ul><li><p>Parallelization：</p></li><li><p>Configurationreuse：复用上次的结果，如果某些局部的参数和上次一样。</p><h4 id="optimized-kernel-libraries">Optimized Kernel Libraries</h4></li></ul></li></ul><p>硬件预定义好了一些快速的计算库(比如cuDNN)。后端可以调用这些库</p><ul><li>调用库带来巨大的性能提升</li><li>但调用库需要符合库的调用约定，可能破坏最佳控制流</li><li>库函数对编译器是黑盒，可能影响优化，不能做operator fusion等。</li></ul><h2 id="已有编译器的分类">已有编译器的分类</h2><p>通过上面已经讲过的不同方面的优化方法，可以梳理、归纳、分类一下现在比较火的DL编译器。总体结果如下图：</p><p><img src="../files/images/DL_compiler_survey/分类.png"></p><h2 id="评测">评测</h2><p>这一部分评测了一下已有的一些编译器的性能，具体实验就略过了，对学习DL编译优化知识没什么帮助。</p><h2 id="结论和未来方向">结论和未来方向</h2><p>这一部分归纳梳理了一下DL编译未来的发展空间：</p><ul><li><p>Dynamic shape and pre/post processing：</p><ul><li>一方面类似NLP，只有运行时才能知道输入的形状。另一方面，模型结构本身可能也会变化。</li><li>随着模型增大，模型的加载时间可能成为瓶颈。已有还没有触及</li></ul></li><li><p>Advancedauto-tuning：受限于时间，现有优化都是在找一些局部最优。但局部最优的组合大概率不是全局最优</p><ul><li>目前ML方法还有潜力，可以考虑在auto-tuning中进一步应用，而不只是costmodel。</li></ul></li><li><p>Polyhedral model：在auto-tuning中引入多面体模型</p><ul><li>可以复用之前结果</li><li>可以减少搜索空间</li><li>挑战是如何在稀疏情况下应用之。</li></ul></li><li><p>Subgraphpartitioning：把计算图拆分成不同子图。可以把不同子图分布并行到异质的设备上。</p></li><li><p>Quantization：增强量子化可以在编译时进一步提升优化的空间，挑战是：</p><ul><li>如何简单的增量开发</li><li>量子化操作如何和其他的优化步骤交互。</li></ul></li><li><p>Unifiedoptimizations：如何同时采用不同编译器的优化，已由编译器大多聚焦于一些方面的优化。MLIR通过dialect某种程度上可以复用不同编译器的优化。</p></li><li><p>Differentiable programming：让编译器支持可微程序</p></li><li><p>Privacy protection：能不能在中间输出层加噪音，保护隐私。</p></li><li><p>Training support：已有编译方法专注于部署。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 深度学习编译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给hexo添加近期文章与更新情况换月</title>
      <link href="/839511f0.html"/>
      <url>/839511f0.html</url>
      
        <content type="html"><![CDATA[<h2 id="近期文章">近期文章</h2><p>今天想要在侧边栏添加近期文章，在网上找了一些已有方法，发现他们都有一个问题：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="HTML"><figure class="iseeu highlight /html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;# recent posts #&#125;</span><br><span class="line">&#123;% if theme.recent_posts %&#125;</span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;links-of-blogroll motion-element &#123;&#123; &quot;</span><span class="attr">links-of-blogroll-</span>&quot; + <span class="attr">theme.recent_posts_layout</span>  &#125;&#125;&quot;&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;links-of-blogroll-title&quot;</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!-- modify icon to fire by szw --&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-history fa-&#123;&#123; theme.recent_posts_icon | lower &#125;&#125;&quot;</span> <span class="attr">aria-hidden</span>=<span class="string">&quot;true&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">      &#123;&#123; theme.recent_posts_title &#125;&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">&quot;links-of-blogroll-list&quot;</span>&gt;</span></span><br><span class="line">      &#123;% set posts = site.posts.sort(&#x27;-date&#x27;) %&#125;</span><br><span class="line">      &#123;% for post in posts.slice(&#x27;0&#x27;, &#x27;5&#x27;).toArray() %&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;recent_posts_li&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;&#123;&#123; url_for(post.path) &#125;&#125;&quot;</span> <span class="attr">title</span>=<span class="string">&quot;&#123;&#123; post.title &#125;&#125;&quot;</span> <span class="attr">target</span>=<span class="string">&quot;_blank&quot;</span>&gt;</span>&#123;&#123; post.title &#125;&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">      &#123;% endfor %&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></div><p>里面的循环都没有添加<code>toArray()</code>函数，不知道前面的人怎么跑起来的……最终效果如下：</p><p><img src="../files/images/近期文章.png"></p><h2 id="本月更新情况添加不同月份">本月更新情况添加不同月份</h2><p>之前在 <a href="/c344b452.html" title="给hexo添加日历">给hexo添加日历</a>里面讲了怎么添加本月的更新情况。由于现在七月要到了，我们还想看上个月的更新情况，所以就添加一个按钮。简单实现一个逻辑：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="HTML"><figure class="iseeu highlight /html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;button&quot;</span> <span class="attr">value</span>=<span class="string">&quot;上月&quot;</span> <span class="attr">onclick</span>=<span class="string">&quot;tips(-1)&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure></div><p>如果点击按钮，就触发下列script：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVASCRIPT"><figure class="iseeu highlight /javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><br><span class="line">    <span class="keyword">var</span> now_id = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">var</span> max_id = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">tips</span>(<span class="params">num</span>)&#123;</span><br><span class="line">        <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(now_id).<span class="property">hidden</span> = <span class="string">&quot;hidden&quot;</span>;</span><br><span class="line">        now_id += num;</span><br><span class="line">        <span class="keyword">if</span> (now_id &gt; max_id) &#123;now_id = max_id;&#125;</span><br><span class="line">        <span class="keyword">if</span> (now_id &lt; <span class="number">0</span>) &#123;now_id = <span class="number">0</span>;&#125;</span><br><span class="line">        <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(now_id).<span class="property">hidden</span> = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure></div><p>会把现在的id加一或者减一，然后用不同table的hidden属性切换显示……简单粗暴。需要给每个月份计算出table，然后富裕一个id。最终成品大概长这样:</p><p><img src="../files/images/更新情况按钮.png"></p>]]></content>
      
      
      <categories>
          
          <category> 开发记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-29总结</title>
      <link href="/76d8b942.html"/>
      <url>/76d8b942.html</url>
      
        <content type="html"><![CDATA[<p>今天发现宝藏up主李沐，他有很多论文讲解视频，理解很深刻，讲得很好。今天看了他的：</p><ul><li>pathways论文讲解：讲如何将训练分布到多个tpu pod上</li><li>GPT，GPT2，GPT3系列讲解：有趣的是，带入了历史，从研究者的角度分析为什么做这个、怎么写能发成果</li><li>CLIP论文讲解：讲了CLIP模型的出现，基石是什么(已有方法已经很好)，clip为什么能火(数据规模更大)。思考很独到，很多纯method以外的收获。</li></ul><p>同时，今日研读<code>DDIM</code>论文，明天和<code>ANALYTIC-DPM</code>一起和大家做论文分享。以及我突然想起来<code>Lisa Xiang</code>那篇的坑还没填完，这几天填一下。</p><p>另，明天就要返乡了，今日收拾行李，累。</p><p>另，明天很忙：</p><ul><li>9:00-10:00 科研组会</li><li>10:30-12:00 职场沟通力课程</li><li>13:30 - 15:00 论文分享会</li><li>14:00: 校园巴士</li><li>16:00-16:30 高铁</li><li>19:00- 20:00 小学期组会</li><li>20:00-21:00 MLC课程学习会</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大三下学期总结</title>
      <link href="/3784f7a3.html"/>
      <url>/3784f7a3.html</url>
      
        <content type="html"><![CDATA[<h2 id="总体">总体</h2><p>​前几天陆陆续续把这学期上的几门专业课都讲了，剩下的几门要不就是我的了解甚少，要么就是提前退课跑路，感觉都没有写课程总结的必要。那么今天就写一下学期总结。</p><p>​这学期整体感觉没有上学期事情多，但是这学期看牙、看心脏、学期两头感冒发烧，治病花了不少时间……感觉活过大三上，后面就没什么难的了。</p><h2 id="课程">课程</h2><p>总体课程上大概是这样：</p><ul><li><p>操作系统 ：见 <a href="/1a09bd40.html" title="OS课程总结">OS课程总结</a></p></li><li><p>体系结构：见 <a href="/1e98a428.html" title="《计算机系统结构》课程总结">《计算机系统结构》课程总结</a></p></li><li><p>现代密码学：见 <a href="/8618fb6d.html" title="《现代密码学》课程总结">《现代密码学》课程总结</a></p></li><li><p>词汇的力量：王英老师开的课，清华最硬核英语课。上下来会有3000左右的词汇量增长，可惜我退了。主要是课程内容太难了、太多了：</p><ul><li><p>要背很多课文</p></li><li><p>要被很多单词</p></li><li><p>要了解很多词根和变形</p></li><li><p>要看很多纪录片</p></li><li><p>要做很多听写</p></li><li><p>要写很多感想、观后感、读后感</p></li><li><p><strong>要多次考试、测词汇量</strong></p></li></ul></li></ul><p>​ 虽然总分是153分，<span class="math inline">\(\geq90\)</span>就记A-，但我还是绷不住，感觉，真的，大三真受不了这个，还是然后一字班零字班来吧。</p><ul><li><p>存储技术基础：陆游游老师开的，前半学期讲存储技术原理，后半学期我退了。我的感觉，这课前半学期=数据库一个学期。码量巨大，压死我的最后一根稻草是期中作业实现一个存储系统，有很多很卷的要求还有相对得分机制，就LSM🌲、B+🌲只是起步，20、21SOTA都是基操。只能说：run，asfast as you can</p></li><li><p>搜索引擎技术基础：这课是刘奕群老师开的，我当时看到名字里带个奕就选了。讲的是一个真正的搜索引擎的开发中遇到的问题，还有性能评价之类的。这课的大作业是实现一个搜索引擎，我当时选的是图片搜索引擎，做了一些CLIP模型、多模态方面的探索，感觉还挺有意思的。后面如果有机会可以写一下这个大作业。</p></li><li><p>网络空间安全导论：OK这个课是开卷考试，我一次都没去听过……期末的前一天晚上我猛看到2:00，然后去考试。讲的很杂，都是网络安全各个层级的安全问题。有几次实验，对着网上的代码拟合一下就都ok。还没出分，不知道会不会给我个，惊喜。话说这课还</p><ul><li>有个基础版，尹霞老师开的，我没上，大二的课。</li><li>有个进阶版，我下学期选了，看看和这课有没有承接关系。</li></ul></li><li><p>相声艺术鉴赏：文素课，老师是”大逗相声“团队的李寅飞，上课的口音听着比较逗哏，讲的也蛮有意思的。期中作业是小组作业，分析相声中的某个点。然后整个后半学期每节课两个组展示期中作业，剩下的时间老师根据同学讲的点进行展开、点评。我只能说：敢这么讲的老师应该是什么方面都懂。期末作业是实现一个相声或者写个论文。然后我和另一个同学组队写、录了个相声，还惨遭发b站。</p></li></ul><h2 id="科研">科研</h2><p>​科研方面，我这学期其实没做什么，主要是在读一些论文。嗯，我刚才统计了一下，应该是看了127篇，平均1天1篇的样子，啥都看，不是局限在NLG，也不局限在NLP。我的感觉是，读论文还是很有用的，可以极大地拓展视野，然后了解同行都在做什么，以及什么值得同行做。说实话，我现在感觉我还蛮喜欢读论文的，怎么说，难道是图书管理员体质……</p><p>​可能我只是比较逃避写代码。不过这个暑假我应该要开题做一篇了，嗯，希望能快点做完。</p><h2 id="生活">生活</h2><p>​ 这学期我入手RTX3060一块，其实需求也还好，不算是刚需。现在在玩老头环需要3060，不过这个话反过来也成立：现在有了3060所以要玩老头环。主要是现在显卡降价，跌回原价。我之前一直说跌回原价就买，这事可能甚至得追溯到大一奖学金发的慢。</p><p>​再就是，自我检讨一下：这学期摄影和弹琴频率下降很多，胡适之呀胡适之，你怎么开摆了。</p><p>​ 最后，本学期观赛不少。如果后面有兴致，我可能写一下队伍的点评吧：</p><ul><li>LCK方面，重点关注GEN.G, T1, DK, NS, DRX五个队的比赛，还有季后赛</li><li>LPL方面，重点关注 RNG, V5, EDG, WBG, TES, LNG, JDG,BLG几个队的互殴，还有季后赛</li><li>MSI方面，重点关注 T1和RNG还有G2。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 大三下 </tag>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-28总结</title>
      <link href="/d0afb2f6.html"/>
      <url>/d0afb2f6.html</url>
      
        <content type="html"><![CDATA[<p>今天读了一篇新论文，写了论文阅读笔记。同时，我决定周四回家。可能要周三的时候做核酸、收拾行李，想想还是有很多事要干呀…</p><p>另，今日观赛：</p><ul><li>we打v5：我可以理解we被碾压，但怎么smlz第二把才上？这波是dream和smlz的恩怨局，smlz第二把选出猫和老鼠对阵dream的加里奥，在上路一波老鼠开大狂喷dream没喷死，遂寄。</li><li>lng打tes：lng纯纯的打不过。但说实话，knight的阿狸玩的确实好，基本把lng秀烂了。tarzan很尽力，抢了一个打另一个小龙。但最后在绝对的实力面前还是只能惨淡收场。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读-SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</title>
      <link href="/6b94db09.html"/>
      <url>/6b94db09.html</url>
      
        <content type="html"><![CDATA[<h2 id="sde-与-ode">SDE 与 ODE</h2><h3 id="sde">SDE</h3><p>sde是随机微分方程，可以理解成一个加噪声的过程： <spanclass="math display">\[dx = f(x,t)dt + g(x,t)dw\]</span> 可以认为是一个原始分布<spanclass="math inline">\(P_x(0)\)</span>随着时间的推移进行变化。同时dw是一个布朗运动的过程，g是scale函数。每一个是时间概率也会加上一个布朗噪声。</p><p>对于一个确定的初始分布，这个扩散过程最终会把x的分布变成一个和初始分布无关的量。</p><p>下面观察几种已有方法：</p><p>观察传统SMLD的扩散公式： <span class="math display">\[x_i = x_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} z_{i-1},\quad i =1,2..N\]</span> 如果考虑连续形式，将 <span class="math inline">\(x_i =x(t_i),x_{i-1} = x(t_{i-1}), t_{i-1} \tot_i\)</span>时，通过求导和逼近，可以有： <span class="math display">\[dx = \sqrt{\frac{d[\sigma^2(t)]}{dt}}dw\]</span>这是一个一届的近似，这是一个非常简单的sde方程。有如下特征：</p><ul><li>方差趋近于无穷</li></ul><p>另一方面，对于DDPM模型的公式： <span class="math display">\[x_i =  \sqrt{1 - \beta_i} x_{i-1} + \sqrt{\beta_i}z_{i-1},\quad i =1,2..N\]</span> 同样有等价的SDE方程： <span class="math display">\[dx = -\frac{1}{2}\beta(t)xdt + \sqrt{\beta(t)}dw\]</span> 这个方程的方差趋向于1.</p><h3 id="ode">ODE</h3><p>所谓的ode，就是想让这个扩散过程和随机变量z无关。</p><h2 id="解sde">解SDE</h2><p>对于一个SDE，有一个对应的解： <span class="math display">\[dx = [f(x,t) - g^2(t)\bigtriangledown_x \log p_t(x)]dt + g(t)d\hat{w}\]</span>如果忽略上面方程中dw的项，逆过程就会和随机变量无关，是ode。然而，这样采样效果会变差。</p><p>也就是说：</p><ul><li><p>一方面，SDE的逆扩散也是SDE。</p></li><li><p>另一方面，如果我们有了一个网络去拟合<spanclass="math inline">\(\bigtriangledown_x \logp_t(x)\)</span>，那么我们就可以通过上面的公式把一个随机分布恢复去噪成<spanclass="math inline">\(P_0\)</span></p></li></ul><p>对于SMLD和DDPM，作者证明了论文中给出的解法都是符合上面的通解公式的。其中DDPM的<em>ancestralsampling</em>是通解公式的一种一阶近似。</p><p>作者还给出了一种效果更好的去噪方法：PC</p><p><img src="../files/images/diffusion_model/pc.png"></p><p>可以看出:</p><ul><li>原始的 SMLD只有红色部分</li><li>原始的DDPM只有蓝色部分(这个和原始的ancestralsampling是一阶近似)。</li></ul><p>​总体而言，这个PC方法相当于是在离散时间下先走按照原来的导数方向走一步概率。由于时间粒度太粗，这个实际SDE解有变差，因此再用多步的Langevindynamics纠正这个偏差。也就是：预测-纠正</p><p>​作者通过实验证明了这种带纠正的方法PC同时提升了SMLD和DDPM的效果。注意：计算量增加了</p><h2 id="可控生成">可控生成</h2><p>也就是在给定y下预测<span class="math inline">\(P_t(x(t)|y)\)</span>。y可以是标签，也可以是一个遮罩。由于： <spanclass="math display">\[P_t(x(t)| y) \propto P_t(x) \times P_t(y|x(t))\]</span> 可以发现: <span class="math display">\[\bigtriangledown_x\log P_t(x(t)| y) = \bigtriangledown_x\log P_t(x(t)) +\bigtriangledown_x\log P_t(y | x(t))\]</span></p><ul><li>左边是不考虑y的正常 score model输出，</li><li>右边是一个额外的分类器的输出。这个分类器需要对每一个时间t能有输出，因此可以先把训练集中数据做出不同t下的噪声形式，再训练分类器。</li></ul><p>生成时，我们将上面的公式作为score带入SDE的通解就可以进行可控地生成了。</p><p><img src="../files/images/diffusion_model/label.png"></p><p><img src="../files/images/diffusion_model/mask.png"></p><h2 id="总结与点评">总结与点评</h2><p>​ 总体而言，这篇论文读着很”爽“，让我感觉”不愧是outstandingpaper“，”ICLR亦有差距“。这篇文章比较详细地给出了DDPM的数学背景，并且联系起了score-basedmodel和DDPM。另一方面，作者提出的PC方法简单易用，又真的可以极大提高效果，good。</p><p>我在想，既然这些DDPM、SLMD方法都是对于SDE方程的一阶近似解，那么：</p><ul><li>一阶近似解有无数种，哪种最好？</li><li>有没有更高阶逼近的解？</li></ul><p>另外，我也在思考：</p><ul><li>想要用在文本生成领域中，还有什么可以借鉴的地方？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本月更新(Recent Update)</title>
      <link href="/e1e3e0e0.html"/>
      <url>/e1e3e0e0.html</url>
      
        <content type="html"><![CDATA[<script src="https://github.com/TransformersWsz/TransformersWsz.github.io/releases/download/echarts/echarts.min.js"></script><div id="echarts6110" style="width: 100%;height: 500px;margin: 0 auto"></div><script type="text/javascript">        // 基于准备好的dom，初始化echarts实例        var myChart = echarts.init(document.getElementById('echarts6110'));        // 指定图表的配置项和数据        var option =  option = {title: {    text: ""},tooltip: {    trigger: 'axis',    axisPointer: {    type: 'cross',    label: {        backgroundColor: '#6a7985'    }    }},legend: {    data: ['blogs', 'read', 'cite', 'publish', 'paper-scan-in-arxiv/10', 'paper-recommend-in-arxiv']},toolbox: {    feature: {    saveAsImage: {}    }},grid: {    left: '3%',    right: '4%',    bottom: '3%',    containLabel: true},xAxis: [    {    type: 'category',    boundaryGap: false,    data: ['2023-10-01', '2023-10-02', '2023-10-03', '2023-10-04', '2023-10-05', '2023-10-06', '2023-10-07', '2023-10-08', '2023-10-09', '2023-10-10', '2023-10-11', '2023-10-12', '2023-10-13', '2023-10-14', '2023-10-15', '2023-10-16', '2023-10-17', '2023-10-18', '2023-10-19', '2023-10-20', '2023-10-21', '2023-10-22', '2023-10-23', '2023-10-24', '2023-10-25', '2023-10-26', '2023-10-27', '2023-10-28', '2023-10-29', '2023-10-30', '2023-10-31', '2023-11-01', '2023-11-02', '2023-11-03', '2023-11-04', '2023-11-05', '2023-11-06', '2023-11-07', '2023-11-08', '2023-11-09', '2023-11-10', '2023-11-11', '2023-11-12', '2023-11-13', '2023-11-14', '2023-11-15', '2023-11-16', '2023-11-17', '2023-11-18', '2023-11-19', '2023-11-20', '2023-11-21', '2023-11-22', '2023-11-23', '2023-11-24', '2023-11-25', '2023-11-26', '2023-11-27', '2023-11-28', '2023-11-29', '2023-11-30', '2023-12-01', '2023-12-02', '2023-12-03', '2023-12-04', '2023-12-05', '2023-12-06', '2023-12-07', '2023-12-08', '2023-12-09', '2023-12-10', '2023-12-11', '2023-12-12', '2023-12-13', '2023-12-14', '2023-12-15', '2023-12-16', '2023-12-17', '2023-12-18', '2023-12-19', '2023-12-20', '2023-12-21', '2023-12-22', '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26', '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30', '2023-12-31', '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-06', '2024-01-07', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12', '2024-01-13', '2024-01-14', '2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19', '2024-01-20', '2024-01-21', '2024-01-22', '2024-01-23', '2024-01-24', '2024-01-25', '2024-01-26', '2024-01-27', '2024-01-28', '2024-01-29', '2024-01-30', '2024-01-31', '2024-02-01', '2024-02-02', '2024-02-03', '2024-02-04', '2024-02-05', '2024-02-06', '2024-02-07', '2024-02-08', '2024-02-09', '2024-02-10', '2024-02-11', '2024-02-12', '2024-02-13', '2024-02-14', '2024-02-15', '2024-02-16', '2024-02-17', '2024-02-18', '2024-02-19', '2024-02-20', '2024-02-21', '2024-02-22', '2024-02-23', '2024-02-24', '2024-02-25', '2024-02-26', '2024-02-27', '2024-02-28', '2024-02-29', '2024-03-01', '2024-03-02', '2024-03-03', '2024-03-04', '2024-03-05', '2024-03-06', '2024-03-07', '2024-03-08', '2024-03-09', '2024-03-10', '2024-03-11', '2024-03-12', '2024-03-13', '2024-03-14', '2024-03-15', '2024-03-16', '2024-03-17', '2024-03-18', '2024-03-19', '2024-03-20', '2024-03-21', '2024-03-22', '2024-03-23', '2024-03-24', '2024-03-25', '2024-03-26', '2024-03-27', '2024-03-28', '2024-03-29', '2024-03-30', '2024-03-31', '2024-04-01', '2024-04-02', '2024-04-03', '2024-04-04', '2024-04-05', '2024-04-06', '2024-04-07', '2024-04-08', '2024-04-09', '2024-04-10', '2024-04-11', '2024-04-12', '2024-04-13', '2024-04-14', '2024-04-15', '2024-04-16', '2024-04-17', '2024-04-18', '2024-04-19', '2024-04-20', '2024-04-21', '2024-04-22', '2024-04-23', '2024-04-24', '2024-04-25', '2024-04-26', '2024-04-27', '2024-04-28', '2024-04-29', '2024-04-30', '2024-05-01', '2024-05-02', '2024-05-03', '2024-05-04', '2024-05-05', '2024-05-06', '2024-05-07', '2024-05-08', '2024-05-09', '2024-05-10', '2024-05-11', '2024-05-12', '2024-05-13', '2024-05-14', '2024-05-15', '2024-05-16', '2024-05-17', '2024-05-18', '2024-05-19', '2024-05-20', '2024-05-21', '2024-05-22', '2024-05-23', '2024-05-24', '2024-05-25', '2024-05-26', '2024-05-27', '2024-05-28', '2024-05-29', '2024-05-30', '2024-05-31', '2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', '2024-06-05', '2024-06-06', '2024-06-07', '2024-06-08', '2024-06-09', '2024-06-10', '2024-06-11', '2024-06-12', '2024-06-13', '2024-06-14', '2024-06-15', '2024-06-16', '2024-06-17', '2024-06-18', '2024-06-19', '2024-06-20', '2024-06-21', '2024-06-22', '2024-06-23', '2024-06-24', '2024-06-25', '2024-06-26', '2024-06-27', '2024-06-28', '2024-06-29', '2024-06-30', '2024-07-01', '2024-07-02', '2024-07-03', '2024-07-04', '2024-07-05', '2024-07-06', '2024-07-07', '2024-07-08', '2024-07-09', '2024-07-10', '2024-07-11', '2024-07-12', '2024-07-13', '2024-07-14', '2024-07-15', '2024-07-16', '2024-07-17', '2024-07-18', '2024-07-19', '2024-07-20', '2024-07-21', '2024-07-22', '2024-07-23', '2024-07-24', '2024-07-25', '2024-07-26', '2024-07-27', '2024-07-28', '2024-07-29', '2024-07-30', '2024-07-31', '2024-08-01', '2024-08-02', '2024-08-03', '2024-08-04', '2024-08-05', '2024-08-06', '2024-08-07', '2024-08-08', '2024-08-09', '2024-08-10', '2024-08-11', '2024-08-12', '2024-08-13', '2024-08-14', '2024-08-15', '2024-08-16', '2024-08-17', '2024-08-18', '2024-08-19', '2024-08-20', '2024-08-21', '2024-08-22', '2024-08-23', '2024-08-24', '2024-08-25', '2024-08-26', '2024-08-27', '2024-08-28', '2024-08-29', '2024-08-30', '2024-08-31', '2024-09-01', '2024-09-02', '2024-09-03', '2024-09-04', '2024-09-05', '2024-09-06', '2024-09-07', '2024-09-08', '2024-09-09', '2024-09-10', '2024-09-11', '2024-09-12', '2024-09-13', '2024-09-14', '2024-09-15', '2024-09-16', '2024-09-17', '2024-09-18', '2024-09-19', '2024-09-20', '2024-09-21', '2024-09-22', '2024-09-23', '2024-09-24', '2024-09-25', '2024-09-26', '2024-09-27', '2024-09-28', '2024-09-29', '2024-09-30', '2024-10-01', '2024-10-02', '2024-10-03', '2024-10-04', '2024-10-05', '2024-10-06', '2024-10-07', '2024-10-08', '2024-10-09', '2024-10-10', '2024-10-11', '2024-10-12', '2024-10-13', '2024-10-14', '2024-10-15', '2024-10-16', '2024-10-17', '2024-10-18', '2024-10-19', '2024-10-20', '2024-10-21', '2024-10-22', '2024-10-23', '2024-10-24', '2024-10-25', '2024-10-26', '2024-10-27', '2024-10-28', '2024-10-29', '2024-10-30', '2024-10-31', '2024-11-01', '2024-11-02', '2024-11-03', '2024-11-04', '2024-11-05'],    axisLabel: {        interval: 40,        rotate: 45 // 设置标签旋转角度为45度    }    }],yAxis: [    {    type: 'value'    }],series: [        {        name: 'obj_0',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: '100 citation', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: '100 citation', // 这里是竖线的名字                    xAxis: '2023-10-15' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_1',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'Gemini', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'Gemini', // 这里是竖线的名字                    xAxis: '2023-12-06' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_2',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'publish 10 paper', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'publish 10 paper', // 这里是竖线的名字                    xAxis: '2024-01-26' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_3',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'Sora', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'Sora', // 这里是竖线的名字                    xAxis: '2024-02-15' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_4',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'Twitter 100 followers', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'Twitter 100 followers', // 这里是竖线的名字                    xAxis: '2024-02-22' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_5',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'scan 10000 paper', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'scan 10000 paper', // 这里是竖线的名字                    xAxis: '2024-04-18' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_6',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'GPT-4o', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'GPT-4o', // 这里是竖线的名字                    xAxis: '2024-05-13' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_7',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'read 500 paper', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'read 500 paper', // 这里是竖线的名字                    xAxis: '2024-07-27' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {        name: 'obj_8',        type: 'line',        lineStyle: {            color: 'black', // 竖线颜色设置为黑色        },        markLine: {            symbol: ['none', 'none'], // 不显示标记线两端的标记            label: {                show: true, // 显示标签                position: 'end', // 在线的末端显示标签                formatter: 'o1', // 自定义显示的文本                rotate: 30, // 旋转角度            },            lineStyle: {                color: 'black',            },            data: [                {                    name: 'o1', // 这里是竖线的名字                    xAxis: '2024-09-13' // 假设你想在 2023-12-15 这个日期上添加竖线                }            ]        }    },        {    name: 'blogs',    type: 'line',    emphasis: {        focus: 'series'    },    itemStyle: {        borderWidth: 0    },    data: [118, 118, 119, 120, 122, 123, 123, 123, 124, 125, 126, 127, 128, 128, 128, 129, 130, 131, 132, 132, 134, 134, 134, 135, 137, 138, 139, 139, 139, 140, 141, 142, 143, 144, 144, 144, 144, 144, 147, 148, 149, 150, 150, 151, 152, 153, 154, 155, 155, 155, 156, 157, 158, 159, 159, 159, 159, 159, 162, 163, 164, 165, 165, 165, 166, 167, 168, 169, 170, 170, 170, 171, 172, 173, 174, 175, 176, 176, 177, 179, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 187, 188, 189, 189, 191, 191, 191, 192, 193, 193, 195, 196, 196, 196, 198, 198, 199, 200, 200, 201, 201, 202, 203, 203, 205, 206, 206, 206, 207, 208, 209, 210, 211, 211, 211, 212, 213, 214, 214, 216, 217, 217, 218, 219, 220, 221, 222, 222, 222, 222, 222, 222, 223, 224, 224, 224, 227, 229, 230, 232, 233, 234, 234, 234, 236, 237, 238, 239, 239, 239, 239, 241, 242, 242, 243, 243, 244, 245, 246, 247, 248, 248, 250, 250, 251, 252, 252, 252, 253, 254, 255, 256, 257, 258, 258, 260, 260, 260, 261, 262, 263, 264, 265, 265, 265, 265, 265, 265, 267, 267, 267, 270, 271, 272, 273, 273, 273, 273, 275, 275, 277, 277, 277, 279, 280, 280, 280, 280, 283, 283, 283, 283, 283, 286, 287, 288, 289, 290, 290, 290, 291, 292, 293, 293, 294, 294, 294, 296, 296, 298, 299, 300, 300, 300, 301, 302, 303, 304, 305, 305, 305, 306, 307, 308, 308, 309, 310, 310, 311, 312, 312, 312, 313, 313, 313, 315, 316, 316, 318, 318, 318, 318, 320, 321, 321, 323, 323, 323, 323, 324, 325, 326, 326, 326, 326, 326, 329, 329, 331, 331, 331, 331, 331, 331, 331, 331, 336, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 337, 338, 340, 342, 343, 344, 345, 346, 347, 347, 347, 348, 349, 350, 351, 352, 352, 352, 353, 353, 353, 355, 356, 356, 356, 357, 358, 359, 360, 360, 361, 361, 362, 363, 364, 365, 366, 366, 366, 367, 367, 369, 370, 371, 371, 371, 371, 371, 371, 371, 371, 371, 373, 376, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 380, 382, 383, 385, 385, 385, 386, 387, 388, 389, 390, 390, 390, 390, 392]    },    {    name: 'read',    type: 'line',    emphasis: {        focus: 'series'    },    itemStyle: {        borderWidth: 0    },    data: [354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 355, 355, 355, 357, 357, 358, 358, 359, 359, 359, 360, 360, 360, 360, 364, 364, 364, 369, 370, 370, 370, 373, 373, 373, 373, 373, 373, 381, 382, 382, 382, 384, 384, 384, 385, 388, 388, 388, 388, 388, 388, 392, 396, 396, 396, 398, 398, 398, 398, 405, 405, 405, 407, 407, 410, 410, 413, 413, 413, 413, 417, 417, 417, 417, 417, 417, 417, 418, 419, 419, 419, 419, 419, 419, 419, 419, 419, 419, 419, 419, 419, 419, 419, 421, 423, 423, 423, 423, 425, 426, 426, 426, 426, 426, 426, 426, 426, 427, 428, 428, 428, 429, 429, 429, 429, 430, 430, 430, 431, 431, 431, 431, 432, 432, 432, 432, 433, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 435, 435, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 437, 437, 438, 439, 441, 441, 441, 444, 446, 448, 448, 448, 448, 448, 449, 450, 452, 454, 455, 455, 455, 456, 460, 460, 460, 460, 460, 460, 460, 460, 460, 460, 460, 460, 460, 462, 464, 464, 464, 466, 466, 466, 468, 470, 472, 474, 476, 476, 476, 480, 481, 482, 486, 486, 486, 486, 488, 488, 488, 488, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 491, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 504, 505, 506, 508, 509, 509, 509, 509, 509, 515, 518, 518, 518, 518, 518, 518, 518, 518, 519, 522, 522, 522, 523, 527, 527, 528, 528, 528, 528, 529, 530, 531, 531, 532, 532, 532, 532, 532, 532, 533, 536, 536, 536, 536, 536, 536, 536, 536, 536, 536, 536, 547, 547, 547, 547, 547, 547, 547, 547, 547, 551, 551, 551, 551, 547, 549, 551, 551, 552, 552, 552, 553, 553, 555, 555, 556, 556, 556, 556]    },    {    name: 'cite',    type: 'line',    emphasis: {        focus: 'series'    },    itemStyle: {        borderWidth: 0    },    data: [173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 173, 186, 188, 188, 188, 188, 188, 188, 190, 190, 193, 193, 193, 197, 198, 198, 198, 200, 200, 200, 200, 201, 201, 203, 204, 204, 204, 205, 205, 212, 220, 220, 220, 220, 221, 222, 222, 225, 227, 227, 227, 227, 229, 233, 233, 235, 235, 235, 241, 249, 254, 254, 254, 254, 254, 254, 257, 257, 262, 265, 265, 265, 270, 273, 275, 276, 276, 276, 276, 276, 275, 281, 288, 289, 289, 289, 289, 291, 295, 297, 301, 301, 301, 301, 304, 308, 309, 310, 310, 310, 312, 319, 324, 330, 330, 330, 330, 331, 331, 334, 334, 334, 334, 334, 338, 339, 341, 343, 346, 346, 346, 346, 350, 350, 351, 352, 352, 352, 352, 352, 352, 353, 355, 355, 355, 357, 364, 367, 367, 368, 368, 368, 368, 370, 370, 370, 371, 371, 371, 371, 372, 372, 376, 376, 376, 376, 376, 378, 379, 381, 382, 382, 382, 384, 385, 386, 386, 391, 391, 391, 394, 397, 401, 403, 406, 406, 406, 406, 410, 410, 412, 417, 417, 417, 419, 424, 429, 437, 439, 439, 439, 441, 451, 460, 460, 466, 466, 466, 470, 472, 474, 477, 480, 480, 480, 484, 486, 488, 492, 492, 492, 492, 493, 495, 496, 497, 499, 499, 499, 502, 502, 503, 503, 504, 504, 504, 505, 506, 508, 510, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 542, 543, 544, 544, 546, 550, 550, 550, 551, 551, 552, 552, 556, 556, 556, 560, 560, 560, 560, 561, 561, 561, 564, 564, 564, 566, 566, 566, 566, 566, 566, 567, 568, 569, 569, 569, 574, 574, 580, 582, 584, 584, 584, 586, 588, 590, 592, 595, 595, 595, 610, 615, 615, 615, 615, 615, 615, 615, 615, 620, 655, 655, 655, 655, 651, 653, 655, 655, 662, 662, 662, 663, 663, 670, 673, 676, 676, 676, 676]    },    {    name: 'publish',    type: 'line',    emphasis: {        focus: 'series'    },    itemStyle: {        borderWidth: 0    },    data: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]    },    {    name: 'paper-scan-in-arxiv/10',    type: 'line',    emphasis: {        focus: 'series'    },    itemStyle: {        borderWidth: 0    },    data: [0.0, 3.0, 7.0, 10.0, 13.0, 16.0, 16.0, 16.0, 17.0, 34.2, 38.2, 41.2, 44.2, 44.2, 44.2, 45.2, 58.7, 67.0, 74.4, 81.8, 81.8, 81.8, 92.6, 112.9, 124.1, 133.0, 141.0, 141.0, 141.0, 147.7, 161.8, 167.9, 173.6, 178.2, 178.2, 178.2, 183.6, 191.0, 196.9, 201.7, 208.6, 208.6, 208.6, 212.0, 223.9, 234.8, 246.6, 262.0, 262.0, 262.0, 264.7, 274.6, 278.3, 282.2, 282.2, 282.2, 282.2, 287.0, 295.7, 300.9, 305.6, 310.0, 310.0, 310.0, 313.9, 321.7, 326.1, 330.3, 339.2, 339.2, 339.2, 343.3, 350.5, 355.3, 359.5, 363.5, 363.5, 363.5, 367.8, 377.0, 383.7, 388.1, 391.2, 391.2, 391.2, 395.0, 395.0, 402.2, 402.2, 406.9, 406.9, 406.9, 411.1, 415.9, 418.3, 421.2, 424.0, 424.0, 424.0, 425.7, 433.7, 437.5, 441.1, 447.1, 447.1, 447.1, 452.8, 452.8, 469.1, 472.6, 477.5, 477.5, 477.5, 482.0, 489.5, 493.8, 499.4, 504.0, 504.0, 504.0, 508.2, 516.7, 522.4, 527.6, 533.0, 533.0, 533.0, 540.1, 562.2, 569.9, 575.7, 583.8, 583.8, 583.8, 588.4, 597.7, 602.8, 607.7, 614.5, 614.5, 614.5, 623.3, 649.8, 660.6, 671.1, 681.6, 681.6, 681.6, 693.0, 709.5, 717.9, 727.5, 734.2, 734.2, 734.2, 739.0, 756.5, 763.9, 769.1, 774.9, 774.9, 774.9, 781.8, 793.3, 798.6, 805.9, 811.7, 811.7, 811.7, 819.2, 830.6, 836.7, 842.3, 848.9, 848.9, 848.9, 855.6, 870.4, 878.8, 885.4, 892.3, 892.3, 892.3, 898.5, 914.4, 924.5, 931.9, 939.1, 939.1, 939.1, 943.7, 955.5, 961.7, 966.5, 972.4, 972.4, 972.4, 977.0, 990.7, 995.4, 1001.1, 1007.0, 1007.0, 1007.0, 1012.0, 1022.4, 1030.7, 1035.4, 1041.0, 1041.0, 1041.0, 1045.1, 1055.0, 1061.4, 1068.5, 1073.2, 1073.2, 1073.2, 1080.1, 1088.3, 1092.4, 1096.0, 1100.2, 1100.2, 1100.2, 1105.1, 1117.3, 1121.5, 1124.3, 1129.1, 1129.1, 1129.1, 1133.4, 1144.0, 1148.0, 1148.0, 1167.6, 1167.6, 1167.6, 1174.8, 1182.0, 1190.1, 1197.3, 1204.9, 1204.9, 1204.9, 1212.6, 1227.9, 1237.2, 1246.2, 1254.6, 1254.6, 1254.6, 1262.6, 1277.6, 1292.4, 1300.7, 1309.5, 1309.5, 1309.5, 1316.7, 1347.7, 1363.8, 1363.8, 1386.1, 1386.1, 1386.1, 1396.1, 1414.9, 1425.0, 1435.1, 1442.9, 1442.9, 1442.9, 1449.7, 1465.2, 1474.8, 1483.5, 1483.5, 1483.5, 1483.5, 1495.6, 1506.2, 1513.1, 1517.6, 1522.2, 1522.2, 1522.2, 1527.1, 1538.3, 1548.0, 1548.0, 1560.9, 1560.9, 1560.9, 1565.9, 1576.4, 1581.8, 1586.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1592.7, 1600.0, 1606.8, 1612.1, 1616.2, 1619.8, 1619.8, 1619.8, 1624.3, 1634.5, 1641.2, 1647.0, 1654.8, 1654.8, 1654.8, 1658.4, 1666.0, 1671.0, 1676.3, 1680.3, 1680.3, 1680.3, 1683.3, 1683.3, 1700.2, 1705.4, 1710.0, 1710.0, 1710.0, 1713.3, 1719.9, 1724.8, 1728.6, 1731.8, 1731.8, 1731.8, 1735.7, 1745.8, 1753.6, 1758.9, 1764.4, 1764.4, 1764.4, 1770.0, 1788.7, 1796.6, 1803.9, 1812.3, 1812.3, 1812.3, 1817.6, 1834.8, 1842.2, 1852.5, 1865.3, 1865.3, 1865.3, 1877.4, 1899.8, 1899.8, 1899.8, 1899.8, 1899.8, 1899.8, 1899.8, 1899.8, 1913.1, 1928.7, 1928.7, 1928.7, 1928.7, 1941.2, 1961.0, 1974.5, 1986.7, 2001.2, 2001.2, 2001.2, 2018.4, 2061.5, 2087.6, 2113.6, 2135.9, 2135.9, 2135.9, 2157.3, 2204.5]    },    {    name: 'paper-recommend-in-arxiv',    type: 'line',    emphasis: {        focus: 'series'    },    itemStyle: {        borderWidth: 0    },    data: [0, 2, 5, 7, 9, 11, 11, 11, 11, 14, 17, 19, 21, 21, 21, 21, 23, 26, 29, 31, 31, 31, 32, 35, 36, 37, 38, 38, 38, 38, 41, 42, 44, 45, 45, 45, 46, 48, 49, 52, 53, 53, 53, 56, 58, 61, 63, 65, 65, 65, 65, 68, 70, 72, 72, 72, 72, 74, 76, 79, 81, 84, 84, 84, 86, 89, 92, 92, 94, 94, 94, 96, 96, 97, 98, 100, 100, 100, 103, 106, 107, 108, 110, 110, 110, 112, 112, 114, 114, 117, 117, 117, 118, 119, 121, 123, 124, 124, 124, 125, 129, 131, 133, 135, 135, 135, 138, 138, 143, 144, 147, 147, 147, 150, 153, 154, 155, 158, 158, 158, 158, 160, 161, 164, 168, 168, 168, 170, 172, 175, 176, 178, 178, 178, 180, 183, 185, 187, 189, 189, 189, 191, 194, 196, 198, 201, 201, 201, 204, 207, 210, 212, 214, 214, 214, 215, 217, 220, 221, 223, 223, 223, 225, 226, 228, 231, 234, 234, 234, 235, 239, 241, 243, 246, 246, 246, 248, 251, 253, 256, 259, 259, 259, 262, 265, 266, 267, 271, 271, 271, 274, 277, 279, 280, 283, 283, 283, 283, 285, 286, 289, 290, 290, 290, 290, 294, 295, 296, 297, 297, 297, 297, 300, 302, 304, 305, 305, 305, 306, 310, 312, 313, 314, 314, 314, 315, 315, 315, 315, 317, 317, 317, 318, 319, 319, 319, 322, 322, 322, 325, 327, 329, 331, 334, 334, 334, 335, 338, 340, 341, 344, 344, 344, 345, 348, 351, 354, 355, 355, 355, 357, 361, 364, 364, 367, 367, 367, 368, 370, 372, 375, 377, 377, 377, 378, 381, 382, 384, 384, 384, 384, 386, 388, 390, 392, 394, 394, 394, 395, 397, 398, 398, 401, 401, 401, 402, 405, 406, 407, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 409, 410, 412, 413, 414, 415, 415, 415, 416, 418, 420, 422, 423, 423, 423, 424, 425, 426, 427, 429, 429, 429, 430, 430, 434, 437, 438, 438, 438, 438, 441, 442, 443, 444, 444, 444, 444, 446, 447, 451, 454, 454, 454, 455, 459, 460, 461, 464, 464, 464, 466, 470, 471, 474, 480, 480, 480, 482, 487, 487, 487, 487, 487, 487, 487, 487, 490, 493, 493, 493, 493, 494, 497, 500, 501, 503, 503, 503, 508, 512, 516, 520, 524, 524, 524, 529, 534]    }]};        // 使用刚指定的配置项和数据显示图表。        myChart.setOption(option);</script><script type="text/javascript">    var now_id = 0;    var max_id = 29;    function tips(num){        document.getElementById(now_id).hidden = "hidden";        now_id -= num;        if (now_id > max_id) {now_id = max_id;}        if (now_id < 0) {now_id = 0;}        document.getElementById(now_id).hidden = "";    }</script><table id="0" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年十一月November</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00E100"><b></b>😁</td><td>2</td></tr><tr><td>3</td><td>4</td><td bgcolor="#00B900"><b></b>😁😁</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td></tr><tr><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td></tr><tr><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td><td>30</td></tr><tr></tr></table><table id="1" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td bgcolor="#00D300"><b></b>😁😁</td><td bgcolor="#00B100"><b></b>😁😁😁</td><td bgcolor="#00BC00"><b></b>😁😁</td><td>9</td><td>10</td><td>11</td><td>12</td></tr><tr><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td></tr><tr><td>20</td><td>21</td><td bgcolor="#00D100"><b></b>😁😁</td><td bgcolor="#00CF00"><b></b>😁😁</td><td bgcolor="#00F900"><b></b>😁</td><td bgcolor="#00EA00"><b></b>😁😁</td><td>26</td></tr><tr><td>27</td><td bgcolor="#00D500"><b></b>😁</td><td bgcolor="#00E200"><b></b>😁</td><td bgcolor="#00DB00"><b></b>😁</td><td bgcolor="#00D500"><b></b>😁</td></tr></table><table id="2" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td>1</td><td bgcolor="#00F800"><b></b>😁</td><td>3</td><td>4</td><td bgcolor="#00C500"><b></b>😁😁</td><td bgcolor="#00F500"><b></b>😁</td><td>7</td></tr><tr><td>8</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00E300"><b></b>😁</td><td bgcolor="#00F700"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td><td>13</td><td bgcolor="#00F700"><b></b>😁</td></tr><tr><td>15</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00EB00"><b></b>😁</td><td bgcolor="#00F300"><b></b>😁</td><td bgcolor="#00E700"><b></b>😁</td><td bgcolor="#00E900"><b></b>😁</td><td>21</td></tr><tr><td>22</td><td bgcolor="#00F900"><b></b>😁</td><td>24</td><td bgcolor="#00D100"><b></b>😁😁</td><td bgcolor="#00F700"><b></b>😁</td><td bgcolor="#00E900"><b></b>😁</td><td>28</td></tr><tr><td>29</td><td>30</td></tr></table><table id="3" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年八月August</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00E800"><b></b>😁😁</td></tr><tr><td bgcolor="#00EE00"><b></b>😁😁</td><td bgcolor="#00F900"><b></b>😁</td><td bgcolor="#00F100"><b></b>😁</td><td bgcolor="#00ED00"><b></b>😁</td><td bgcolor="#00E700"><b></b>😁</td><td bgcolor="#00F900"><b></b>😁</td><td>24</td></tr><tr><td>25</td><td bgcolor="#00F700"><b></b>😁</td><td bgcolor="#00F000"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00F100"><b></b>😁</td><td>31</td></tr><tr></tr></table><table id="4" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年七月July</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00E300"><b></b>😁😁</td><td bgcolor="#00E100"><b></b>😁</td><td>3</td><td bgcolor="#00E600"><b></b>😁😁</td><td>5</td><td>6</td></tr><tr><td>7</td><td bgcolor="#00ED00"><b></b>😁</td><td bgcolor="#00ED00"><b></b>😁</td><td bgcolor="#00F100"><b></b>😁</td><td>11</td><td>12</td><td>13</td></tr><tr><td>14</td><td bgcolor="#00D700"><b></b>😁😁😁</td><td>16</td><td bgcolor="#00E000"><b></b>😁😁</td><td>18</td><td>19</td><td>20</td></tr><tr><td>21</td><td>22</td><td>23</td><td>24</td><td bgcolor="#00B000"><b></b>😁😁😁😁😁</td><td bgcolor="#00EF00"><b></b>😁</td><td>27</td></tr><tr><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="5" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年六月June</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>2</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00E500"><b></b>😁</td><td bgcolor="#00F100"><b></b>😁</td><td bgcolor="#00F900"><b></b>😁</td><td bgcolor="#00E900"><b></b>😁</td><td>8</td></tr><tr><td>9</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00E400"><b></b>😁</td><td bgcolor="#00E800"><b></b>😁</td><td>13</td><td bgcolor="#00E700"><b></b>😁</td><td bgcolor="#00F800"><b></b>😁</td></tr><tr><td>16</td><td bgcolor="#00EB00"><b></b>😁</td><td bgcolor="#00E600"><b></b>😁</td><td>19</td><td>20</td><td bgcolor="#00E500"><b></b>😁</td><td>22</td></tr><tr><td>23</td><td bgcolor="#00DF00"><b></b>😁😁</td><td bgcolor="#00ED00"><b></b>😁</td><td>26</td><td bgcolor="#00D800"><b></b>😁😁</td><td>28</td><td>29</td></tr><tr><td>30</td></tr></table><table id="6" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年五月May</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td>1</td><td>2</td><td bgcolor="#00D700"><b></b>😁😁</td><td bgcolor="#00F500"><b></b>😁</td></tr><tr><td>5</td><td>6</td><td>7</td><td bgcolor="#00CD00"><b></b>😁😁😁</td><td>9</td><td>10</td><td>11</td></tr><tr><td>12</td><td bgcolor="#00E500"><b></b>😁😁😁</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00ED00"><b></b>😁</td><td>18</td></tr><tr><td>19</td><td bgcolor="#00F800"><b></b>😁</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00FE00"><b></b>😁</td><td>23</td><td bgcolor="#00E600"><b></b>😁</td><td>25</td></tr><tr><td>26</td><td bgcolor="#008B00"><b></b>😁😁</td><td>28</td><td bgcolor="#00E000"><b></b>😁😁</td><td bgcolor="#00E900"><b></b>😁</td><td bgcolor="#00E600"><b></b>😁</td></tr></table><table id="7" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年四月April</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00E400"><b></b>😁</td><td bgcolor="#00EA00"><b></b>😁</td><td bgcolor="#00F900"><b></b>😁</td><td>4</td><td bgcolor="#00D800"><b></b>😁😁</td><td>6</td></tr><tr><td>7</td><td bgcolor="#00DF00"><b></b>😁</td><td bgcolor="#00E100"><b></b>😁</td><td bgcolor="#00F000"><b></b>😁</td><td bgcolor="#00F400"><b></b>😁</td><td bgcolor="#00E500"><b></b>😁</td><td>13</td></tr><tr><td>14</td><td>15</td><td>16</td><td>17</td><td bgcolor="#00EC00"><b></b>😁😁</td><td>19</td><td>20</td></tr><tr><td bgcolor="#00D700"><b></b>😁😁😁</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00DA00"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td><td>25</td><td>26</td><td>27</td></tr><tr><td bgcolor="#00F100"><b></b>😁😁</td><td>29</td><td bgcolor="#00E700"><b></b>😁😁</td></tr></table><table id="8" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年三月March</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00F000"><b></b>😁</td><td bgcolor="#008B00"><b></b>😁</td></tr><tr><td>3</td><td>4</td><td bgcolor="#00E600"><b></b>😁😁</td><td bgcolor="#00EA00"><b></b>😁</td><td bgcolor="#00F700"><b></b>😁</td><td bgcolor="#00F000"><b></b>😁</td><td>9</td></tr><tr><td>10</td><td>11</td><td bgcolor="#00E600"><b></b>😁😁</td><td bgcolor="#00E500"><b></b>😁</td><td>14</td><td bgcolor="#00E800"><b></b>😁</td><td>16</td></tr><tr><td bgcolor="#00E100"><b></b>😁</td><td bgcolor="#00F900"><b></b>😁</td><td bgcolor="#00DC00"><b></b>😁</td><td bgcolor="#00EF00"><b></b>😁</td><td bgcolor="#00EE00"><b></b>😁</td><td>22</td><td bgcolor="#008B00"><b></b>😁😁</td></tr><tr><td>24</td><td bgcolor="#00F000"><b></b>😁</td><td bgcolor="#00E100"><b></b>😁</td><td>27</td><td>28</td><td bgcolor="#00EE00"><b></b>😁</td><td bgcolor="#00E800"><b></b>😁</td></tr><tr><td bgcolor="#00E100"><b></b>😁</td></tr></table><table id="9" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年二月February</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td bgcolor="#00E600"><b></b>😁</td><td bgcolor="#00E200"><b></b>😁</td><td>3</td></tr><tr><td>4</td><td bgcolor="#00EB00"><b></b>😁</td><td bgcolor="#00EB00"><b></b>😁</td><td bgcolor="#00E900"><b></b>😁</td><td>8</td><td bgcolor="#00E600"><b></b>😁😁</td><td bgcolor="#008B00"><b></b>😁</td></tr><tr><td>11</td><td bgcolor="#00EE00"><b></b>😁</td><td bgcolor="#00E800"><b></b>😁</td><td bgcolor="#00F200"><b></b>😁</td><td bgcolor="#00EC00"><b></b>😁</td><td bgcolor="#00F000"><b></b>😁</td><td>17</td></tr><tr><td>18</td><td>19</td><td>20</td><td>21</td><td bgcolor="#00ED00"><b></b>😁</td><td bgcolor="#00E400"><b></b>😁</td><td>24</td></tr><tr><td>25</td><td bgcolor="#00BA00"><b></b>😁😁😁</td><td bgcolor="#00E200"><b></b>😁😁</td><td bgcolor="#00E900"><b></b>😁</td><td bgcolor="#00B300"><b>激情: </b>😁😁</td></tr></table><table id="10" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2024年一月January</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00F400"><b></b>😁</td><td bgcolor="#00EB00"><b></b>😁</td><td bgcolor="#00ED00"><b></b>😁</td><td>4</td><td bgcolor="#00E800"><b></b>😁😁</td><td>6</td></tr><tr><td>7</td><td bgcolor="#00F800"><b></b>😁</td><td bgcolor="#00DA00"><b></b>😁</td><td>10</td><td bgcolor="#00D700"><b></b>😁😁</td><td bgcolor="#00EB00"><b></b>😁</td><td>13</td></tr><tr><td>14</td><td bgcolor="#008B00"><b></b>😁😁</td><td>16</td><td bgcolor="#00CE00"><b></b>😁</td><td bgcolor="#00F700"><b></b>😁</td><td>19</td><td bgcolor="#00EA00"><b></b>😁</td></tr><tr><td>21</td><td bgcolor="#00E400"><b></b>😁</td><td bgcolor="#00E700"><b></b>😁</td><td>24</td><td bgcolor="#00EF00"><b></b>😁😁</td><td bgcolor="#00DB00"><b></b>😁</td><td>27</td></tr><tr><td>28</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00EC00"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td></tr></table><table id="11" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年十二月December</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00E200"><b></b>😁</td><td>2</td></tr><tr><td>3</td><td bgcolor="#00ED00"><b></b>😁</td><td bgcolor="#00E200"><b></b>😁</td><td bgcolor="#00E400"><b></b>😁</td><td bgcolor="#00FC00"><b></b>😁</td><td bgcolor="#00EC00"><b></b>😁</td><td>9</td></tr><tr><td>10</td><td bgcolor="#00EE00"><b></b>😁</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td><td bgcolor="#00F100"><b></b>😁</td><td bgcolor="#00EF00"><b></b>😁</td><td bgcolor="#008B00"><b></b>😁</td></tr><tr><td>17</td><td bgcolor="#00E700"><b></b>😁</td><td bgcolor="#008B00"><b></b>😁😁</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td><td>22</td><td bgcolor="#00EC00"><b></b>😁</td></tr><tr><td>24</td><td bgcolor="#00EE00"><b></b>😁</td><td>26</td><td bgcolor="#00EC00"><b></b>😁</td><td>28</td><td bgcolor="#00DE00"><b></b>😁</td><td>30</td></tr><tr><td bgcolor="#00DA00"><b>平淡: </b>😁</td></tr></table><table id="12" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年十一月November</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td bgcolor="#00F800"><b></b>😁</td><td bgcolor="#00EE00"><b></b>😁</td><td bgcolor="#00F700"><b></b>😁</td><td>4</td></tr><tr><td>5</td><td>6</td><td>7</td><td bgcolor="#00DF00"><b></b>😁😁😁</td><td bgcolor="#00EC00"><b></b>😁</td><td bgcolor="#00F700"><b></b>😁</td><td bgcolor="#00A600"><b></b>😁</td></tr><tr><td>12</td><td bgcolor="#00DB00"><b></b>😁</td><td bgcolor="#00EF00"><b></b>😁</td><td bgcolor="#00D900"><b></b>😁</td><td bgcolor="#00E800"><b></b>😁</td><td bgcolor="#00ED00"><b></b>😁</td><td>18</td></tr><tr><td>19</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00E600"><b></b>😁</td><td bgcolor="#00F400"><b></b>😁</td><td bgcolor="#00EF00"><b></b>😁</td><td>24</td><td>25</td></tr><tr><td>26</td><td>27</td><td bgcolor="#008B00"><b></b>😁😁😁</td><td bgcolor="#00E300"><b></b>😁</td><td bgcolor="#00EB00"><b></b>😁</td></tr></table><table id="13" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td bgcolor="#00D600"><b></b>😁</td><td>2</td><td bgcolor="#00F100"><b></b>😁</td><td bgcolor="#00ED00"><b></b>😁</td><td bgcolor="#00E500"><b></b>😁😁</td><td bgcolor="#00EE00"><b></b>😁</td><td>7</td></tr><tr><td>8</td><td bgcolor="#00FB00"><b></b>😁</td><td bgcolor="#00F000"><b></b>😁</td><td bgcolor="#00EA00"><b></b>😁</td><td bgcolor="#00F100"><b></b>😁</td><td bgcolor="#00EF00"><b></b>😁</td><td>14</td></tr><tr><td>15</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00EE00"><b></b>😁</td><td bgcolor="#00E900"><b></b>😁</td><td bgcolor="#00E900"><b></b>😁</td><td>20</td><td bgcolor="#00B900"><b></b>😁😁</td></tr><tr><td>22</td><td>23</td><td bgcolor="#00F500"><b></b>😁</td><td bgcolor="#00F000"><b></b>😁😁</td><td bgcolor="#00F800"><b></b>😁</td><td bgcolor="#00F600"><b></b>😁</td><td>28</td></tr><tr><td>29</td><td bgcolor="#00FE00"><b></b>😁</td><td bgcolor="#00F800"><b></b>😁</td></tr></table><table id="14" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>1</td><td>2</td></tr><tr><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td></tr><tr><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td></tr><tr><td>24</td><td bgcolor="#009300"><b></b>😁</td><td>26</td><td>27</td><td>28</td><td bgcolor="#00B100"><b>希望: </b>😁😁</td><td>30</td></tr><tr></tr></table><table id="15" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年八月August</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td bgcolor="#00A200"><b></b>😁</td></tr><tr><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td></tr><tr><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td></tr><tr><td>20</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td></tr><tr><td>27</td><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="16" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年七月July</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>2</td><td>3</td><td>4</td><td bgcolor="#009600"><b></b>😁</td><td>6</td><td>7</td><td>8</td></tr><tr><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td></tr><tr><td>16</td><td>17</td><td>18</td><td>19</td><td bgcolor="#00A800"><b></b>😁</td><td>21</td><td>22</td></tr><tr><td bgcolor="#00C600"><b></b>😁</td><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td><td bgcolor="#00A200"><b></b>😁</td></tr><tr><td>30</td><td bgcolor="#00B400"><b></b>😁</td></tr></table><table id="17" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年六月June</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td bgcolor="#00A700"><b></b>😁</td><td>17</td></tr><tr><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td><td>24</td></tr><tr><td>25</td><td>26</td><td>27</td><td bgcolor="#00C900"><b></b>😁</td><td>29</td><td bgcolor="#00E200"><b></b>😁</td></tr></table><table id="18" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年五月May</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td bgcolor="#00B900"><b></b>😁</td><td bgcolor="#00EB00"><b>累: </b>😁</td></tr><tr><td>7</td><td>8</td><td>9</td><td bgcolor="#00AE00"><b></b>😁</td><td>11</td><td>12</td><td>13</td></tr><tr><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td></tr><tr><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td><td>27</td></tr><tr><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="19" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年四月April</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td></tr><tr><td>9</td><td>10</td><td bgcolor="#008B00"><b></b>😁</td><td bgcolor="#008B00"><b></b>😁</td><td>13</td><td>14</td><td bgcolor="#008B00"><b></b>😁</td></tr><tr><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td></tr><tr><td>23</td><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td></tr><tr><td>30</td></tr></table><table id="20" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年三月March</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td><td bgcolor="#008B00"><b>复工: </b>😁😁😁</td></tr><tr><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td></tr><tr><td>12</td><td>13</td><td bgcolor="#00BD00"><b></b>😁</td><td bgcolor="#008B00"><b></b>😁</td><td>16</td><td bgcolor="#00EC00"><b>开心: </b>😁</td><td>18</td></tr><tr><td>19</td><td bgcolor="#00CC00"><b></b>😁</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td></tr><tr><td>26</td><td>27</td><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="21" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年二月February</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td></tr><tr><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td></tr><tr><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td></tr><tr><td>26</td><td>27</td><td>28</td></tr></table><table id="22" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2023年一月January</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td></tr><tr><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td></tr><tr><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td></tr><tr><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td><td>27</td><td>28</td></tr><tr><td>29</td><td>30</td><td>31</td></tr></table><table id="23" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年十二月December</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td bgcolor="#00EB00"><b>润: </b>😁</td><td bgcolor="#00AB00"><b></b>😁</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>11</td><td>12</td><td bgcolor="#00EB00"><b>阳: </b>😁</td><td>14</td><td>15</td><td>16</td><td>17</td></tr><tr><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td><td>24</td></tr><tr><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td><td>30</td><td>31</td></tr><tr></tr></table><table id="24" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年十一月November</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>6</td><td>7</td><td bgcolor="#00C400"><b>开心: </b>😁😁</td><td bgcolor="#00C300"><b></b>😁</td><td>10</td><td>11</td><td>12</td></tr><tr><td bgcolor="#009700"><b></b>😁</td><td>14</td><td bgcolor="#008B00"><b></b>😁😁</td><td bgcolor="#00E700"><b>失眠: </b>😁</td><td>17</td><td>18</td><td>19</td></tr><tr><td bgcolor="#00F500"><b>困: </b>😁</td><td>21</td><td>22</td><td>23</td><td bgcolor="#00F900"><b>开心: </b>😁</td><td>25</td><td>26</td></tr><tr><td>27</td><td>28</td><td bgcolor="#00F600"><b>还行: </b>😁</td><td>30</td></tr></table><table id="25" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>2</td><td bgcolor="#00F400"><b>困: </b>😁</td><td>4</td><td bgcolor="#00F700"><b>冷: </b>😁</td><td>6</td><td>7</td><td>8</td></tr><tr><td>9</td><td>10</td><td bgcolor="#00F200"><b>冷: </b>😁</td><td bgcolor="#00B800"><b></b>😁</td><td bgcolor="#00A400"><b>开心: </b>😁😁😁</td><td bgcolor="#00BC00"><b></b>😁</td><td>15</td></tr><tr><td bgcolor="#00F800"><b>平淡: </b>😁</td><td>17</td><td bgcolor="#008B00"><b></b>😁😁</td><td bgcolor="#00FB00"><b>开心: </b>😁</td><td>20</td><td>21</td><td>22</td></tr><tr><td>23</td><td bgcolor="#00F300"><b>开心: </b>😁</td><td>25</td><td>26</td><td>27</td><td>28</td><td>29</td></tr><tr><td>30</td><td>31</td></tr></table><table id="26" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td bgcolor="#00E900"><b>期待: </b>😁</td><td bgcolor="#00A200"><b></b>😁😁</td></tr><tr><td>18</td><td>19</td><td>20</td><td bgcolor="#00EA00"><b>累: </b>😁</td><td>22</td><td bgcolor="#00D900"><b></b>😁</td><td bgcolor="#00EF00"><b>累: </b>😁</td></tr><tr><td>25</td><td bgcolor="#00F100"><b>开心: </b>😁</td><td bgcolor="#00F300"><b>开心: </b>😁</td><td>28</td><td bgcolor="#00A800"><b></b>😁😁</td><td>30</td></tr></table><table id="27" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年八月August</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td>1</td><td bgcolor="#00F900"><b>失望: </b>😁</td><td bgcolor="#00FB00"><b>困: </b>😁</td><td>4</td><td>5</td><td>6</td></tr><tr><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td></tr><tr><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td></tr><tr><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td><td>27</td></tr><tr><td bgcolor="#00EC00"><b>蚌埠住了: </b>😁</td><td>29</td><td>30</td><td>31</td></tr></table><table id="28" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年七月July</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#008B00"><b>累: </b>😁😁😁</td><td bgcolor="#008B00"><b>忐忑: </b>😁😁😁</td></tr><tr><td bgcolor="#00F700"><b>开摆: </b>😁</td><td bgcolor="#008B00"><b>失眠: </b>😁😁</td><td bgcolor="#00FA00"><b>开心: </b>😁</td><td>6</td><td bgcolor="#008B00"><b></b>😁</td><td>8</td><td bgcolor="#00E900"><b>工作: </b>😁😁</td></tr><tr><td bgcolor="#008B00"><b>平淡: </b>😁😁😁</td><td bgcolor="#00C100"><b></b>😁</td><td bgcolor="#00F800"><b>开心: </b>😁</td><td bgcolor="#008B00"><b>干活: </b>😁😁😁😁</td><td bgcolor="#00AE00"><b></b>😁</td><td>15</td><td bgcolor="#00F800"><b>休假: </b>😁</td></tr><tr><td bgcolor="#00BD00"><b>累: </b>😁😁</td><td bgcolor="#00B300"><b>累: </b>😁😁</td><td>19</td><td bgcolor="#00F800"><b>平淡: </b>😁</td><td>21</td><td>22</td><td>23</td></tr><tr><td>24</td><td>25</td><td>26</td><td bgcolor="#00F500"><b>内耗: </b>😁</td><td>28</td><td bgcolor="#00FD00"><b>努力: </b>😁</td><td bgcolor="#00FC00"><b>累: </b>😁</td></tr><tr><td>31</td></tr></table><table id="29" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips(-1)"></form></td><td colspan="5">2022年六月June</td><td><form action><input type="button" value="下月Next Month" onclick="tips(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td></tr><tr><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td><td bgcolor="#00D100"><b></b>😁😁😁</td><td bgcolor="#00F400"><b></b>😁</td></tr><tr><td bgcolor="#00D900"><b></b>😁😁</td><td bgcolor="#009C00"><b></b>😁😁😁</td><td bgcolor="#00D100"><b></b>😁😁😁</td><td bgcolor="#00F800"><b></b>😁</td><td>23</td><td>24</td><td>25</td></tr><tr><td bgcolor="#00BF00"><b>生病: </b>😁😁</td><td>27</td><td bgcolor="#00BC00"><b>失望: </b>😁😁😁</td><td bgcolor="#00CC00"><b>努力: </b>😁😁</td><td bgcolor="#00F500"><b></b>😁</td></tr></table>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-27总结</title>
      <link href="/26e7c21f.html"/>
      <url>/26e7c21f.html</url>
      
        <content type="html"><![CDATA[<p>今天是快手小学期的第一天，感觉和我想象的差距还是蛮大的，日程如下：</p><ul><li>11.00-12.00：开营仪式，请了几个学长分享快手的情况。期间被hr打电话改材料，签协议。</li><li>12.00-16.00:中间陆陆续续拉了4个群，发了一些自己的自我介绍，然后选组长之类的社交工作</li><li>16:00-16.30:和负责老师开会聊了聊，拉了两个群，发现被调剂到了另一个项目</li><li>19:00-20:00 和负责学姐开会聊了聊，拉了一个群。</li></ul><p>​快手的行政系统还是比较复杂的。比如我们的小学期项目，来自计算机系和软件学院的同学们被分到了8个大组。每个大组大概7-8个人，然后有一个负责的hr学长。另一方面，对于项目来说，大概每个项目有3-4个同学，这个和大组的分配基本是正交的。</p><p>​在评价体系上，我们需要出勤打卡，写工作日报、工作周报，拿到个人分。另一方面，需要进行一些大组上的组间评比，拿到大组分。一整套体系下来需要做的、需要了解的东西很多，需要认识的人也很多，需要开的会也很多。整体感觉上这个体系非常严谨，只能说，剩余价值直接给榨干。</p><p>​虽然安排上工作时间是9:00-19:00，但从开会等情况来看，大家还是会加班到比较晚……可能这就是科技公司吧。工作内容方面我就不说了，负责我的学长和学姐都是清华的，比较了解我系的课程设置，这点上还是比较赞的。不知道后面项目做起来会不会比较有意思。</p><p>​另：今天学会新词，qua。互联网公司的项目一般按季度为一个周期，大家都把这个叫做qua。”上个qua大家做了……“</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-26总结</title>
      <link href="/8090c9ab.html"/>
      <url>/8090c9ab.html</url>
      
        <content type="html"><![CDATA[<p>​最近几天生病了，有点低烧，精神状态不佳，博客停更了几天，但今天病好了。所以后面几天应该会继续更新diffusionmodel相关的文章。最近又看了一些相关的论文，感觉这个方向可以做的事情还有很多。</p><p>​这几天玩了一些老头环，感觉这个游戏的内容还是非常丰富的，每一块地图都有很多的隐藏，我打算明天写一个老头环体验的第一期。</p><p>​ 最近观赛：</p><ul><li>dk输给了t1：感觉nuguri打的还可以，但canyon怎么开始犯病了…</li><li>rng输给了v5：rng也没什么失误，经典蓝色方必胜。dream虽然是小将，但没怎么犯病，期待rookie的回归。我只能说，常规赛的v5，永远的战神</li><li>blg输给了edg：只能说，游兵散勇，亦有差距。blg已经是东部战区倒数第三了，能不能支棱一下呀，uzi之前直播说blg该换教练了，话说是不是除了教练基本都换过一轮了……流水的队员，铁打的刘青松，我真的哭死。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客阅读-Generative Modeling by Estimating Gradients of the Data Distribution</title>
      <link href="/593c50eb.html"/>
      <url>/593c50eb.html</url>
      
        <content type="html"><![CDATA[<p>主要是阅读了下面这篇博客：</p><blockquote><p><a href="https://yang-song.github.io/blog/2021/score/">GenerativeModeling by Estimating Gradients of the Data Distribution</a></p></blockquote><p>这是去年ICLR论文作者对其论文的讲解</p><blockquote><p>SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIALEQUATIONS</p></blockquote><p>生成模型都是想要通过训练集获取数据的分布(比如人脸数据集就是人脸长什么样)，再通过采样一个和源数据集分布相同的数据进行生成。</p><h2 id="经典方法">经典方法</h2><p>作者谈到了经典生成模型里两种形式:</p><ul><li><strong>likelihood-based models</strong>:直接获取源数据集中数据的分布，或者用某种上下界来近似。auto-regressive，energe-based model, flow model, VAE都是这种情况<ul><li>问题是：需要保证分布是归一化的，因此很难大规模地应用在各种任务上（需要改模型结构）。或者，用某种上下界来估计分布，因此会有偏差。</li></ul></li><li><strong>implicit generativemodels</strong>：隐式表示数据集中数据的分布。比如GAN，通过一个分类器的得分来表示和原数据集分布的区别，进而让模型来学习。<ul><li>问题是：需要对抗训练，很难收敛。生成的内容也比较容易崩溃。</li></ul></li></ul><h2 id="score-function-score-based-model-score-matching">score function,score-based model, score matching</h2><p>我们定义这个score: <span class="math display">\[score = \bigtriangledown_x \log p(x)\]</span>好处在于，因为是对于导数的模拟，因此，这个score不需要归一化。不管怎么样都是一个归一的分布。scorebased model就是在逼近这个分布： <span class="math display">\[s_\theta(x) \approx \bigtriangledown_x \log p(x)\]</span></p><h4 id="如何逼近">如何逼近？</h4><p>可以看出，scoremodel是对概率分布的对数导数做估计，需要找一个满足这个条件的分布方法——Fisher-deivergence:<span class="math display">\[\text{Fisher Distance} = E_{P(x)} || \bigtriangledown_x\log p(x) -s_\theta||^2_2\]</span> 这个可以衡量两个分布的距离：</p><ul><li>越“接近“的分布距离越小</li><li>相同的分布距离是0</li></ul><p>通过优化fisher距离，我们就能让score basedmodel学到训练集的数据分布了。同时，通过一些scorematching的算法，我们不需要知道原始数据集的<spanclass="math inline">\(\bigtriangledown_x\logp(x)\)</span>就能优化，也不需要对抗的训练。</p><h2 id="langevin-dynamics">Langevin dynamics</h2><p>训练完成以后，如何采样呢？ <span class="math display">\[x_{i+1} = x_i + \epsilon \bigtriangledown_x\log p(x) + \sqrt{2\epsilon}z_i\]</span></p><p><span class="math display">\[z_i \in N(0,1), x_0 \in N(0,1),\]</span></p><p>通过多部迭代，可以保证一个随机的点最终会以<spanclass="math inline">\(p(x)\)</span>的概率存在在空间中</p><p>其中上面的导数部分用score basedmodel近似。这样，我们不需要额外的计算就可以采样了</p><h2 id="score-based-model-问题与解决">score-based model 问题与解决</h2><p>刚才的 fisher-rao distance存在的问题是：</p><ul><li>由于使用2阶距离，对于本身p比较小的位置。训练时的导数过小，可能学不到。</li></ul><p>也就是说，模型对于低密度区域的p(x)近似效果不好。这个问题实际的后果是：</p><ul><li>在采样时需要从一个随机的点开始，然而最开始没落到高密度区域的话，模型很难让通过几步变化走到高密度区域。也就是说，开始随机到低密度区域(这是大概率事件，20%的区域享有80%的概率)，很可能会生成炸。</li></ul><p>对于上面的问题，一个解决办法思路：</p><ul><li>能不能对原始分布叠加上一些随机噪音。让高低密度区域不要区分的这么明显，进而让score model 学到所有位置的分布</li></ul><p>这个思路很好，但是有几个衍生的问题：</p><ul><li>"噪声"取多大？</li></ul><p>有一个折中的办法：</p><ul><li>多步加噪声，每次都是上一轮分布再叠加一个噪声。这样后面的分布趋向于随机分布。前面的分布和原始分布非常接近。</li></ul><p>这种情况下，我们的score-based model 也需要相应的改变： <spanclass="math display">\[S_\theta(x) \rightarrow S_\theta(x,t)\]</span></p><p><span class="math display">\[S_\theta(x,t) \approx \bigtriangledown_x\log p(x,t)\]</span></p><p>可以逼近每一个采样步骤的分布。在学习这个scoremodel时，我们需要考虑所有的噪声分布，加权： <spanclass="math display">\[\sum_{i=1}^L \lambda_i E_{P(x,i)} || \bigtriangledown_x\log p(x,i) -S_\theta(x,i)||^2_2\]</span> 其中，权重一般就选取每一轮高斯噪声的方差： <spanclass="math display">\[\lambda_i = \sigma_i^2\]</span> 同时，在采样的过程中，我们修改原始的Langevin dynamics采样:<span class="math display">\[x_{i+1} = x_i + \epsilon S_\theta(x,T-i) + \sqrt{2\epsilon} z_i\]</span>让每一步点都按照不同的分布进行行走。最终逐步去噪到一个原始分布的点。这个算法叫做<strong>annealedLangevin dynamics</strong></p><p>博客后面关于微分方程的部分我打算在论文阅读完以后写上</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-22总结</title>
      <link href="/74dfedb8.html"/>
      <url>/74dfedb8.html</url>
      
        <content type="html"><![CDATA[<p>今天摆了一天，玩了一段时间老头环，看了两场比赛：</p><ul><li>dk打gen.g：dk输了，虽然Nuguri换回来了，但感觉总体运营在细节上还是有差距，可能是因为新队员还需要磨合。第一把dk尝试了上半区为主的打法，canyon一直在帮上路，下路选出抗压英雄。结果由于下路劣势过大，并且上中野也没有打出应有的压制力，被拿下了。第二把dk更是兵败如山倒，前期阵容前期劣。</li><li>blg打tes:今天blg换出新请来的援兵icon，算了一下blg现在基本凑出王炸组合了。结果每个选手虽然都挺强，但打起架来还是游兵散勇没有配合，基本被tes无伤拿下，反倒是wayward打出不少亮眼操作。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-20-21总结</title>
      <link href="/f5d3b3af.html"/>
      <url>/f5d3b3af.html</url>
      
        <content type="html"><![CDATA[<p>​首先，有点泪目，由于各种各样的原因，这两天都没完老头环，玩游戏之前总会遇到各种各样的“有件事”。不知道能不能在小学期开始之前炼个几级出来。虽然没打，但我竟然看了两集攻略视频，我觉得我已经完全掌握了这个游戏（​最近两天只发了一篇论文阅读笔记，但其实读了不少论文，最少4篇，都是关于diffusionmodel的，我正在思考下一篇工作要不要做一下diffusionmodel相关的，感觉这个领域还有很多可以深挖的点。 ​另外，今天还研读了一下博弈论经典的纳什均衡问题。思考了囚徒困境、讨价还价问题、时间代价的讨价还价、贴现因子等等情况。感觉好有意思，竟然可以在”所有人都是理性“的条件下推出恒定的结论，而且结论好优美。说起这个，下学期我是有打算去旁听一下《经济学原理》,这个课里面好像有很多博弈论的知识，希望在这个课上可以更深度地了解这方面的知识。</p><p>​最后，今日吃到金枪鱼面包，很好吃。想这个想了一年多了，结果早上竟然一次都没起来，最后终于吃到还是在暑假，有点泪目。以上。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《体系结构cache测量实验》</title>
      <link href="/d49e0552.html"/>
      <url>/d49e0552.html</url>
      
        <content type="html"><![CDATA[<p>体系结构课的评测见这里：<a href="/1e98a428.html" title="《计算机系统结构》课程总结">《计算机系统结构》课程总结</a></p><p>这次实验的要求是通过侧信道方法测量：</p><ul><li>cache size</li><li>cache line size</li><li>cache相联度</li><li>cache替换方法(选做)</li></ul><p>​ 当时觉得好神奇。后来在网安课上学到了<code>flush-flush,prime-probe</code>等侧信道方法，又一次感觉这次实验很有意思。我觉得所谓实验，不应该是实现、复现xx方法，而应该是针对某个特定的问题，设计自己的方法，又通过结果去验证方法的正确性、分析设计的优缺点，进而去改进。这就像是一个小的科研训练，锻炼我们实际的解决问题的能力。</p><p>​其中，我觉得比较有意思的最后一个测量。当时我构造很多按照4B地址对齐的数据，为了保证它们可以对应到一个set里，然后通过构造不同的访问方式，提前计算不同替换方法下预计的缺失率，进而通过缺失率大小，估计用的时间，再实际测量用的时间。最后拟合出很多条”假设缺失率-时间“曲线，最接近直线的一条就是测量出的替换方法。</p><p>由于当时的报告由于不可逆因素没了，我就放一个pdf在这里…</p><div class="pdfobject-container" data-target="/files/体系结构第一次实验报告.pdf" data-height="500px"></div>]]></content>
      
      
      <categories>
          
          <category> 课程与作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 大三下 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《计算机系统结构》课程总结</title>
      <link href="/1e98a428.html"/>
      <url>/1e98a428.html</url>
      
        <content type="html"><![CDATA[<p>​</p><p>​ 之前对这个课的印象就是：</p><blockquote><p>后半学期每节课只有6个人去</p></blockquote><p>​后面实际上下来感觉没有说的这么阴间。感觉还是挺对不起老师的。就去过两节，由于没有录屏也就没有拟合过上课内容。考试前一天晚上熬夜拟合了所有的PPT，做了往届题就往上怼了。但最后考试难度还行，只能说逃过一劫。</p><p>​这个课有个午餐会，和老师吃了一顿饭，老师在课上猛批了一下子人工智能，安利了一波做编译和做体系结构。后面和快手小学期的老师聊，发现行情和他说的倒也大差不差。总之，这个老师比较喜欢夹带私货，人还挺好的，爱好多，上课也认真。</p><h2 id="tldr">TL;DR</h2><p>一个方案、一种架构、一种机制，如果给人以简洁美、线条美、层次美，那么它注定就是好的。</p><h2 id="课程内容">课程内容</h2><p>这个课给我的感觉像是”大合集“，里面的讲的大多数内容之前都学过：</p><ul><li>基础知识部分，讲了编码方式：汇编小学期讲过X86,mips,造机学过riscv32编码</li><li>huffman编码：新生研讨课讲过</li><li>讲cache原理：造机课讲过，而且实现了一遍</li><li>讲流水线：<ul><li>普通流水线：造机课讲过，而且实现了一遍</li><li>多功能流水线和调度：这个是新活，猛画自动机</li></ul></li><li>讲令牌环与tomasulo，硬件指令级并行：这个是新活，好神奇，学完以后感觉”怎么造机的时候不知道，要不然实现一遍肯定很好玩“</li><li>循环展开与软件指令级并行：编原讲过一些编译优化，但没讲循环展开。循环以后感觉”没我想的那么复杂“。</li></ul><blockquote><p>上学期mini-decaf我开了一个编译优化分支瞎搞，当时循环展开我想的是：有些确定性的循环可以直接预计算完，这个是循环展开……</p></blockquote><h2 id="课程评分">课程评分</h2><p>55分考试 + 15分大作业 + 10分小作业 + 5分书面作业 + 10分雨课堂</p><ul><li>大作业是三选一：tomasulo实现、分支预测软件卷性能、树莓派实现硬件tomasulo。都还行，”硬件“实验也没有造机硬，我选的是软件实现tomasulo，大概长这样。</li></ul><p><img src="../files/images/toma_front.png"></p><ul><li><p>小作业是cache测量，通过一些侧信道观察方法，找到一些电脑本机的cache配置。这个实验我觉得很有意思，虽然实际执行起来好像同学们有很多歧义，我打算写一个blog讲讲这个实验。</p><p><a href="/d49e0552.html" title="《体系结构cache测量实验》">《体系结构cache测量实验》</a></p></li><li><p>书面作业对着PPT和课本拟合一下就行</p></li><li><p>这学期考试挺简单，基本都是基础的内容。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 课程与作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 课程总结 </tag>
            
            <tag> 大三下 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读:Diffusion-LM Improves Controllable Text Generation(1)</title>
      <link href="/75bfb84b.html"/>
      <url>/75bfb84b.html</url>
      
        <content type="html"><![CDATA[<p>​ 今天更新博客的第一篇论文阅读笔记，是<code>prefix tuning</code>作者<code>Xiang Lisa Li</code>的新作。尝试在可控文本生成领域中引入在图像领域很火的diffusion model 的方法。这篇文章不是第一篇文本做 diffusionmodel的（大家都从ViT尝到甜头了，领域迁移非常快…），但是第一篇引入连续diffusion model做文本的。</p><p>​ 这篇文章需要比较多的前置知识，我打算分两期介绍。这是第一期，介绍diffusion model。因此准确来说，这篇博客应该是</p><blockquote><p>Denoising Diffusion Probabilistic Model</p></blockquote><p>的论文阅读笔记</p><h2 id="diffusion-model-总体">Diffusion model 总体</h2><p><img src="../files/images/diffusion_model/对比.png"></p><p>​上面图中的几兄弟在paraphrase等可控文本生成任务中用的都挺多，我后面有机会可能都会讲讲，最起码flow-based models我这几天应该会安排一篇。</p><p>​扩散模型的思路是有点类似VAE，但又不太一样。效果是不是最好不知道，但它在上面几兄弟中是最新的。它定义了两个过程：扩散过程和去噪过程。前者是把一个对象通过逐步添加高斯噪声，最后变成一个完全随机的分布。后者则是通过一个训练好的去噪模型把一个随机变量逐渐去噪，最后返回到原对象。最近比较火的图像生成模型大体上都是用了diffusion model的思路。</p><p>​ 虽然diffusionmodel提出的比较早(2020年)，但是火起来好像也就是2022年。文本领域的diffusionmodel更是刚刚起步，还有很多可以提升的空间。</p><h2 id="diffusion-model的原理">Diffusion model的原理</h2><p>它的原理很简单，但公式的推导过程却比较复杂。基本用到了以下几方面的知识：</p><ul><li><p>马尔科夫链</p></li><li><p>贝叶斯公式</p></li><li><p>参数重整化：把一个<span class="math inline">\(x \inN(\mu,\sigma)\)</span>的分布变成<span class="math inline">\(z \inN(0,1), x = \sigma z +\mu\)</span>.这么做的原因是我们在采样x的时候往往还会和别的过程同步进行，需要一起算loss，<spanclass="math inline">\(\mu,\sigma\)</span>是模型的输出，如果直接采样的话，loss就会被detach。通过重整化，我们把z视为常量，可以让导数图正确传播。</p></li><li><p>KL散度。经典的分布差异的表示，之前在</p><blockquote><p>Learning Disentangled Textual Representations via StatisticalMeasures of Similarity</p></blockquote><ul><li>分享过MI熵、KL散度、维因斯坦距离、fisher-rao距离、SinkhornDivergence、Jeffrey Divergence等方法，这里不再赘述</li><li>需要注意，这里用到了高斯分布下kl散度的化简方法(显然这个散度不对称)：</li></ul><p><span class="math display">\[KL(P_1,P_2) = \log(\frac{\sigma_2}{\sigma_1}) + \frac{\sigma_1^2 +(\mu_1 - \mu_2)^2}{2\sigma_2^2} - 0.5\]</span></p></li></ul><p>中间推导全部略过，从结果来看：</p><p><img src="../files/images/diffusion_model/规律.png"></p><p>这个告诉我们，只要随机选取一个时间t，选取一个随机数，我们就能对参数模型<span class="math display">\[z_\theta(\text{某一个输入}x_t,\text{时间t}) = \text{x对应的输出}x_{t-1}\]</span> 进行优化。最终训练过程如图：</p><p><img src="../files/images/diffusion_model/process.png"></p><p>这里有几个细节，需要注意一下：</p><ul><li>对于每次采样，它不是把<spanclass="math inline">\(t=0,1,...\)</span>全部采样一遍算loss，而是取一个随机的t。原因是这样可以”稳定训练过程“</li><li>在infer过程中，最后一次的随机变量z=0,也就是说去噪的最后一步不能添加噪声。</li><li>公式里面的 <span class="math inline">\(\alpha_t = 1 - \beta_t,\overline{\alpha_t} = \prod_{i \leq t}\alpha_i\)</span>.其中<spanclass="math inline">\(\beta_t\)</span>要递增的选取，并且趋近于1，这样才能保证多次加噪完的<spanclass="math inline">\(x_T\)</span>会变成一个随机分布 <spanclass="math inline">\(X_T \in N(0,1)\)</span></li></ul><h2 id="diffusion-model-的分析">Diffusion model 的分析</h2><p>我们来对比一下 diffusion model 和VAE。可以看出：</p><ul><li>VAE需要一个专门的先验网络来对x生成随机变量z，在infer时扔掉这个网络；而扩散模型的加噪过程是确定的、无参的，不需要一个专门的加噪网络。</li><li>diffusion model 每一步的隐变量都和原来的<spanclass="math inline">\(X_0\)</span>是同一维度的，同时需要多步采样(可能多达1000步)才能把完全随机变量恢复到原来的<span class="math inline">\(X_0\)</span></li></ul><p>个人总结了一下，diffusion model 有以下的优点：</p><ul><li>由于去噪的过程是多步的，我们可以在这个多步的过程中实现更好的、更粒度的控制</li><li>效果会比传统VAE更好。</li><li>它可以更好的handle从类似于”半抽象“的过程恢复，不需要要求去噪的起点一定是随机噪声</li></ul><p>但是，他也有以下的缺点：</p><ul><li>太慢了，1000步？而且作者说步数少了训练会出问题？今年好多论文都是<ul><li>怎么提升采样效率</li><li>通过降低图像尺寸改善速度</li></ul></li><li>训练过程不稳定，估计不好训。这个对于实际开题的问题很大，毕竟不是每个人都是deepmind调参神（</li><li>噪声的隐空间不编码任何有效信息，只是单纯的噪声。</li></ul><h2 id="个人点评">个人点评</h2><ul><li><p>从NLP的角度出发，diffusion model首先需要handle 词语 -&gt; wordembedding 的问题。</p><ul><li>一个显然的方法是借鉴VQVAE的思路，把中间变量clap成最接近的wordembedding。可能带来的新问题是：直接转变噪声过程，会导致扩散模型那个不等式不再成立？？这个可能得实际训训看结果怎么样</li></ul></li><li><p>这个扩散模型有点问题：只能从随机噪声变成一个”有某种控制信息下的一段文本“。但我们很多需求是平行预料的，比如styletransfer，这就回到了刚才说的缺点，我们得想办法在隐空间中编码信息：比如语义信息等。</p><ul><li>我似乎看到今年CVPR有类似的工作？</li></ul><blockquote><p>https://arxiv.org/pdf/2111.15640.pdf</p></blockquote><ul><li>DALLE-2 里面有个 CLIP-Imageencoder也是在干这件事？我应该会在近期读一下dalle-2的论文，写一个阅读笔记，那篇里面再聊吧</li></ul></li><li><p>慢的问题，可能在NLP更严重。大概率，diffusionmodel里面的原子模块得用 transformer，两两叠加是不是直接凉凉……</p></li><li><p>我在思考能不能在长文本中引入 diffusionmodel，这个好像会对每一句都搞得不错，和 plan生成、文本检索一起叠buff，可能能整个大的……这个可能也得后面再试。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> diffusion model </tag>
            
            <tag> 可控文本生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OS课程总结</title>
      <link href="/1a09bd40.html"/>
      <url>/1a09bd40.html</url>
      
        <content type="html"><![CDATA[<p>​OS课基本是贵系知名专业课了，前有四大原理，后有os、体系结构，构成了大三学期。恰逢昨天OS出分了，就来聊聊OS。</p><h2 id="tldr">TL;DR</h2><p>之前在大伯的课上学到一句话：每门课上下来，最起码你得有一句话能记一辈子，这课就算没白上。因此，我每节课都最少要记住一句话。os课就这句：</p><blockquote><p>操作系统的东西，好多都是先有实现再有理论，看明白实现，理论自然就懂了。</p></blockquote><h2 id="老师">老师</h2><p>​我选的是陈渝老师的课，这学期向勇老师没开课。我们是限选课优先志愿，所以不存在选不上课。当然，老师和老师也有区别，听说向勇老师的理念是“通过提高挂科率增强同学积极性”，所以选的人是寥寥无几，因此陈渝老师这边相对就火一些。</p><p>​之前听得传闻对这个课的评价都一般，但实际上下来还不错。我已经不是第一次见陈渝老师了，之前的编译原理也是他教。虽然两个课加起来一共就去听了3节，但对他还是有些了解。感觉他是贵系为数不多“比较懂同学们的”老师。之前有节网课说了一句话让我们印象深刻：</p><blockquote><p>能来会议里答题的同学们都很不错，说明大家都在认真听课；当然不来答题的同学们也很不错，能来会议里听课；当然没在会议的同学们也很不错，不来听课就能把这个知识，都学会。</p></blockquote><p>​ 只能说，不用签到的老师、支持自学的老师，都是好老师。</p><h2 id="课程">课程</h2><ul><li>课程本身还挺硬核的，讲了很多os的知识，知识很多，难度也不小。当然，必修课，你也没得选。</li><li>听说老师上课没有拓展很多，只看PPT的自学效果不错。</li><li>助教团队挺多，都是本科生，比较懂同学们的需求，答疑速度较快。</li></ul><h2 id="评价">评价</h2><p>40分作rCore业 + 20分期中 + 40分期末，会调分(向上)。</p><p>​rCore是模仿初版linux实现的一个操作系统,很多结构体和机制都差不多。在写实验角度来看，比起mini-Decaf实验，rCore码量更大，需要手动merge之前代码，体验差一些；但y1s1,在贵系大作业里算好的了。很多时候，操作系统的东西看代码比上课学理论扎实多了。最起码，我在造机课实现了TLB和页表，但并不知道实际上操作系统再怎么工作，写完lab3就懂了。</p><p>​期中期末传统艺能，都很难，考的很偏。但是按照排名给分，并且大家分数分布给的挺明白，不至于迷迷瞪瞪最后来个评价等级。</p><p>​虽然老师第一节课说给分差，但最后给分还行，不知道是不是疫情影响的原因…</p>]]></content>
      
      
      <categories>
          
          <category> 课程与作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 课程总结 </tag>
            
            <tag> 大三下 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-19总结</title>
      <link href="/f857bea1.html"/>
      <url>/f857bea1.html</url>
      
        <content type="html"><![CDATA[<p>今天不出所料的没读论文。前两天和快手那边的老师对接，可能后续需要了解一些TVM相关的内容，我后续可能也会读一些相关的论文。我正在犹豫要不要把论文阅读版块分成很多子category，这样也许，会降低可读性，不过到时候再看吧。</p><ul><li><p>今天给博客添加了一个阅读数量排行的功能，网上已有的代码都不能跑，稍微修改适配了一番。但现在其实也有bug，那个结果有时能加载出来，有时不能。经过调试，似乎是post请求随即返回，可能是leanCloud的接口稳定性bug，加了一个1s的超时重加载大概解决了80%的情况，遂不管之。</p></li><li><p>把评论区改成了可以匿名评论的valine平台</p></li><li><p>今天发现一件怪事：google竟然能搜到我的网站了？？有点奇怪，不知道它是怎么爬到我的内容的。</p></li></ul><p>老头环玩了玩，进度停留在比较前面的阶段。我后面打算进度提高一些以后再总体的评测一下。这个过程也不知道是多久，也许是明天，也许永远也不会来</p><p>今日新闻：BLG输AL了，来自FOFO的最后一波灵车…</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《现代密码学》课程总结</title>
      <link href="/8618fb6d.html"/>
      <url>/8618fb6d.html</url>
      
        <content type="html"><![CDATA[<h2 id="tldr">TL;DR</h2><p>密码学里需要保密的不是加密算法，而是密钥。</p><h2 id="课程情况">课程情况</h2><p>《现代密码学》这个课程虽然我是大三选的，但其实这是一个大二大三都想选的课程。同时面向计算机系和叉院都开设。</p><p>​课程容量大概是100人，但当时选课的人却不少，我记得我好像还是从150个人中靠着二志愿“杀出重围的”。一般选课时不好选的课，退课时总也舍不得退。这学期我在退课时本来在纠结退密码学，但最后也没有退。可能就有这方面的原因吧。</p><p>​我就去线下上了两节课，后面就再也没去过，期末周的时候看了一遍录屏……老师语速不快，并且是那种说话对时间分布比较平均的类型。也就是说，看回放加速1.5倍也会比较友好，我当时打到了1.7倍，应该是没问题。</p><h2 id="课程内容">课程内容</h2><p>​ 课程本身覆盖面是比较广的。</p><ul><li>从古典密码开始讲起，一直讲到德军Enigma密码机与波兰数学家与图灵对密码机的破解。</li><li>对称密码体制，feistel结构与SPN结构等等，以及经典的des、aes和对应的差分分析、线性分析等等。</li><li>加密算法体质，讲md5结构、sponge结构，以及对应的生日攻击、后缀攻击等等</li><li>公钥密码体质，这一部分挺有意思的。我之前确实不知道公钥密码的原理，属实是打开眼界了一番。讲了怎么构造有后门的数学难题，在此基础上在应用层讲了rsa、diffle-hellmen、elgmal等等算法</li></ul><p>​我认为，这个课涉及的面很广，每个面都还有很多值得深入挖掘的地方。这个课用到的内容很基础，但保护的是计算机很底层的安全问题。总体，还是比较建议选这个课的。(相比其他的牛鬼蛇神竞争对手……)</p><h2 id="考核">考核</h2><p>2021春季学期，这个课是40分的作业，10分的签到，50分的考试。</p><h3 id="作业">作业</h3><p>本学期实行了作业改革。</p><ul><li>第一次作业是破解几种古典密码，照着课本拟合就行。</li><li>第二次大作业是设计一种对称密码或者hash函数，5人一组，老师会根据大家的设计结果进行评估，再综合安全性和新颖性给出设计分<ul><li>我们当时设计了基于以太坊的加密算法<code>Scrypt</code>的一个hash函数，结果老师说功耗太大不利于在FPGA运行，被扣了两分，有点阴间…（如果后面有时间，我也许会讲讲我们组的算法设计）</li></ul></li><li>第三次大作业分组不变，是来破解某一组的第二次大作业。虽然听起来很卷，但其实不是，老师会在第三次提交之前给第二次打分，不会因为算法被别的组破解导致第二次大作业稀烂。不过，如果第三次作业没有破解出来，可能你们组第三次作业分会被扣一些<ul><li>我们组运气好，当时的被攻击者的sponge结构f函数不可逆，而且混淆扩散速度比较慢，被我们用数学方法构造出了碰撞…老师给了我们好多分。</li></ul></li></ul><p>听说之前的后两次大作业是实现sha。新版的大作业我更喜欢一些，比起实现，还是创新做着更有动力。</p><h3 id="考试">考试</h3><p>考试题挺基础的，大家考的也都不错，以上。</p>]]></content>
      
      
      <categories>
          
          <category> 课程与作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 课程总结 </tag>
            
            <tag> 大三下 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6-18总结</title>
      <link href="/5e20b515.html"/>
      <url>/5e20b515.html</url>
      
        <content type="html"><![CDATA[<p>今日开发：给博客添加了一个开发记录的功能。</p><p>今日论文：没读。</p><p>今天是618，但今年也没啥想买的：</p><ul><li>显卡，早在5月就买了3060</li><li>电动牙刷，现在这个虽然有点破，但倒也还能用</li><li>Lego，不知道为什么不太想买了</li></ul><p>​但总之618还是想买点什么，恰逢前两天在b站看到《艾尔登法环》的视频，就买了个游戏来。暑假到了，我打算趁着暑假来玩一玩《艾尔登法环》。</p><p>​说起来，我需要玩的游戏倒也真不少，《荒野大嫖客》还在电脑里躺着，《枪火重生》只过了前两关，《蔚蓝》和《奥日》的全收集计划刚刚完成了不到<spanclass="math inline">\(1/3\)</span>……《艾尔登法环》倒是先入库了。也许我并不是喜欢玩游戏，而是单纯的喜欢买游戏吧。</p><p>​不能这样下去了！我打算在正式入职快手之前玩一玩《艾尔登法环》，可能把后续的游戏进度和一些游戏设计方面的感想做出一个category 放到博客里。</p><p>​不止这个，再有就是暑假结束了，我打算梳理一些这学期的几门课，不管有没有退课，把上课的体验也作出一个category 放到博客里，可能在接下来几天会陆续放出吧。</p><p>​明天是暑假的第一个周天，我也衷心的希望会是暑假论文阅读计划开始的第一天…</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给hexo添加日历</title>
      <link href="/c344b452.html"/>
      <url>/c344b452.html</url>
      
        <content type="html"><![CDATA[<p>今天，我想要给hexo添加一个置顶的更新记录信息，显示每天是否有更新，以及更新的字数等信息，如图：</p><p><img src="./../files/images/本月更新.png"></p><p>其实我的本意是想要给我的论文阅读笔记模块添加本月的论文阅读打卡记录，但总之是要先实现一个基础的打卡记录功能。在网上找了一圈，没有找到相似的功能，要不就是要外挂一些别的打卡平台来实现，太麻烦。就打算自己实现一个。</p><p>大体实现思路如下： -扫描<code>_posts</code>文件夹中所有已有的博客，看看哪些是这个月写/更新的记录下来。-用一个<code>python</code>程序更新它的信息，并用html的<code>table</code>语法实现。- 最后，用一个博客来存下当天的更新信息，并把该博客置顶。 -每次deploy前，都执行一遍这个代码。</p><p>总体逻辑还是比较清晰的，<code>python</code>语法也没什么难度。我目前大概做了这些事：- 用笑脸😁个数来说明当天更新了几篇博客 -用背景颜色来说明当天更新的字数(最多5000，越多越绿)</p><p>把总体代码放出来 <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_leap_year</span>(<span class="params">year</span>):</span><br><span class="line">    <span class="comment"># 判断是否为闰年</span></span><br><span class="line">    <span class="keyword">if</span> year % <span class="number">4</span> == <span class="number">0</span> <span class="keyword">and</span> year % <span class="number">100</span> != <span class="number">0</span> <span class="keyword">or</span> year % <span class="number">400</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_num_of_days_in_month</span>(<span class="params">year, month</span>):</span><br><span class="line">    <span class="comment"># 给定年月返回月份的天数</span></span><br><span class="line">    <span class="keyword">if</span> month <span class="keyword">in</span> (<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">31</span></span><br><span class="line">    <span class="keyword">elif</span> month <span class="keyword">in</span> (<span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">11</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">30</span></span><br><span class="line">    <span class="keyword">elif</span> is_leap_year(year):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">29</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">28</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_total_num_of_day</span>(<span class="params">year, month</span>):</span><br><span class="line">    <span class="comment"># 自1800年1月1日以来过了多少天</span></span><br><span class="line">    days = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1800</span>, year):</span><br><span class="line">        <span class="keyword">if</span> is_leap_year(y):</span><br><span class="line">            days += <span class="number">366</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            days += <span class="number">365</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, month):</span><br><span class="line">        days += get_num_of_days_in_month(year, m)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> days</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_start_day</span>(<span class="params">year, month</span>):</span><br><span class="line">    <span class="comment"># 返回当月1日是星期几，由1800.01.01是星期三推算</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span> + get_total_num_of_day(year, month) % <span class="number">7</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 月份与名称对应的字典</span></span><br><span class="line">month_dict = &#123;<span class="number">1</span>: <span class="string">&#x27;一月&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;二月&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;三月&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;四月&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;五月&#x27;</span>, <span class="number">6</span>: <span class="string">&#x27;六月&#x27;</span>,</span><br><span class="line">              <span class="number">7</span>: <span class="string">&#x27;七月&#x27;</span>, <span class="number">8</span>: <span class="string">&#x27;八月&#x27;</span>, <span class="number">9</span>: <span class="string">&#x27;九月&#x27;</span>, <span class="number">10</span>: <span class="string">&#x27;十月&#x27;</span>, <span class="number">11</span>: <span class="string">&#x27;十一月&#x27;</span>, <span class="number">12</span>: <span class="string">&#x27;十二月&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_month_name</span>(<span class="params">month</span>):</span><br><span class="line">    <span class="comment"># 返回当月的名称</span></span><br><span class="line">    <span class="keyword">return</span> month_dict[month]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_month_title</span>(<span class="params">year, month</span>):</span><br><span class="line">    <span class="comment"># 打印日历的首部</span></span><br><span class="line"></span><br><span class="line">    cal.write(<span class="string">&#x27;         &#x27;</span> + <span class="built_in">str</span>(get_month_name(month)) +  <span class="string">&#x27;   &#x27;</span> + <span class="built_in">str</span>(year) + <span class="string">&#x27;          \n&#x27;</span>)</span><br><span class="line">    cal.write(<span class="string">&#x27;星期日 | 星期一 | 星期二  | 星期三 | 星期四 | 星期五 | 星期六 \n&#x27;</span>)</span><br><span class="line">    cal.write(<span class="string">&#x27;---| ---| ---| ---| ---| ---| ---|\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_to_color</span>(<span class="params">count</span>):</span><br><span class="line">    <span class="comment"># 将0 - 5000 字映射到 0,255,0 -&gt; 0,139,0</span></span><br><span class="line">    count = <span class="built_in">min</span>(count,<span class="number">5000</span>)</span><br><span class="line">    count = <span class="built_in">max</span>(count,<span class="number">0</span>)</span><br><span class="line">    data = <span class="number">139</span> + <span class="built_in">float</span>(<span class="number">5000</span> - count) / <span class="number">5000</span> * (<span class="number">255</span> - <span class="number">139</span>)</span><br><span class="line">    data = <span class="built_in">int</span>(data)</span><br><span class="line">    data = <span class="built_in">min</span>(data,<span class="number">255</span>)</span><br><span class="line">    data = <span class="built_in">max</span>(data, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;#00<span class="subst">&#123;<span class="built_in">format</span>(data,<span class="string">&#x27;02X&#x27;</span>)&#125;</span>00&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_table</span>(<span class="params">year, month, blog_count, word_count</span>):</span><br><span class="line">    cal.write(<span class="string">&quot;&lt;table style=&#x27;text-align:center&#x27;&gt;&quot;</span>)</span><br><span class="line">    cal.write(<span class="string">&quot;&lt;tr &gt; &lt;td colspan=&#x27;7&#x27;&gt;&quot;</span>+<span class="built_in">str</span>(year)+<span class="string">&quot;年&quot;</span>+<span class="built_in">str</span>(get_month_name(month)) +<span class="string">&quot;&lt;/td&gt;&lt;/tr&gt;&quot;</span>)</span><br><span class="line">    cal.write(<span class="string">&#x27;&lt;tr&gt;&lt;td&gt; 星期日 &lt;/td&gt;&lt;td&gt; 星期一 &lt;/td&gt;&lt;td&gt; 星期二  &lt;/td&gt;&lt;td&gt; 星期三 &lt;/td&gt;&lt;td&gt; 星期四 &lt;/td&gt;&lt;td&gt; 星期五 &lt;/td&gt;&lt;td&gt; 星期六 &lt;/td&gt; &lt;/tr&gt;&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    cal.write(<span class="string">&quot;&lt;tr&gt;&quot;</span>)</span><br><span class="line">    i = get_start_day(year, month)</span><br><span class="line">    <span class="keyword">if</span> i != <span class="number">7</span>:</span><br><span class="line">        <span class="comment"># cal.write(&#x27; &#x27;) # 打印行首的两个空格</span></span><br><span class="line">        cal.write(<span class="string">&#x27;  &lt;td&gt;&lt;/td&gt;&#x27;</span> * (i %<span class="number">7</span>))   <span class="comment"># 从星期几开始则空5*几个空格</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, get_num_of_days_in_month(year, month)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> blog_count[j - <span class="number">1</span>] &gt; <span class="number">0</span>:</span><br><span class="line">            cal.write(<span class="string">f&quot; &lt;td bgcolor=<span class="subst">&#123;count_to_color(word_count[j-<span class="number">1</span>])&#125;</span>&gt;&quot;</span> + <span class="string">&quot;😁&quot;</span>*blog_count[j - <span class="number">1</span>] + <span class="string">&#x27; &lt;/td&gt;&#x27;</span>)<span class="comment"># 宽度控制，4+1=5</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cal.write( <span class="string">&quot;&lt;td&gt;&quot;</span>+ <span class="built_in">str</span>(j)+ <span class="string">&#x27; &lt;/td&gt;&#x27;</span>)<span class="comment"># 宽度控制，4+1=5</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">7</span> == <span class="number">0</span>:  <span class="comment"># i用于计数和换行</span></span><br><span class="line">            cal.write(<span class="string">&#x27;&lt;/tr&gt;\n&lt;tr&gt;&#x27;</span>)   <span class="comment"># 每换行一次行首继续空格</span></span><br><span class="line">    cal.write(<span class="string">&quot;&lt;/table&gt;&quot;</span>)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_month_body</span>(<span class="params">year, month, blog_count, word_count</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    打印日历正文</span></span><br><span class="line"><span class="string">    格式说明：空两个空格，每天的长度为5</span></span><br><span class="line"><span class="string">    需要注意的是print加逗号会多一个空格</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    i = get_start_day(year, month)</span><br><span class="line">    <span class="keyword">if</span> i != <span class="number">7</span>:</span><br><span class="line">        <span class="comment"># cal.write(&#x27; &#x27;) # 打印行首的两个空格</span></span><br><span class="line">        cal.write(<span class="string">&#x27;  |&#x27;</span> * (i %<span class="number">7</span>))   <span class="comment"># 从星期几开始则空5*几个空格</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, get_num_of_days_in_month(year, month)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> blog_count[j - <span class="number">1</span>] &gt; <span class="number">0</span>:</span><br><span class="line">            cal.write(<span class="string">&quot; &lt;font color = &#x27;Hotpink&#x27; &gt;&quot;</span> + <span class="built_in">str</span>(j) + <span class="string">&quot;/&quot;</span>+ <span class="built_in">str</span>(blog_count[j - <span class="number">1</span>]) +<span class="string">&quot;/&quot;</span>+ <span class="built_in">str</span>(word_count[j-<span class="number">1</span>])+ <span class="string">&#x27; &lt;/font&gt; |&#x27;</span>)<span class="comment"># 宽度控制，4+1=5</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cal.write(<span class="built_in">str</span>(j)+ <span class="string">&#x27; |&#x27;</span>)<span class="comment"># 宽度控制，4+1=5</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">7</span> == <span class="number">0</span>:  <span class="comment"># i用于计数和换行</span></span><br><span class="line">            cal.write(<span class="string">&#x27;\n |&#x27;</span>)   <span class="comment"># 每换行一次行首继续空格</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_md</span>(<span class="params">f</span>):</span><br><span class="line">    <span class="comment">#将博客内容返回成前导和实际串</span></span><br><span class="line">    start_pos = <span class="number">1</span></span><br><span class="line">    end_pos = -<span class="number">1</span></span><br><span class="line">    cc = <span class="number">0</span></span><br><span class="line">    data = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> k, cont <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;---&quot;</span> <span class="keyword">in</span> cont:</span><br><span class="line">            cc += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> cc == <span class="number">2</span>:</span><br><span class="line">                end_pos = k</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="number">1</span>:end_pos]], [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> data[end_pos+<span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_word_count</span>(<span class="params">f_body</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> f_body:</span><br><span class="line">        count += <span class="built_in">len</span>(i)</span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_writing_freq</span>(<span class="params">root_dir = <span class="string">&quot;./source/_posts&quot;</span>, year = <span class="number">2022</span>, month = <span class="number">6</span></span>):</span><br><span class="line">    <span class="comment"># 返回每天更新的博客数量</span></span><br><span class="line">    blog_count = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(get_num_of_days_in_month(year,month))] <span class="comment">#统计开始前每天更新为0</span></span><br><span class="line">    word_count = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(get_num_of_days_in_month(year,month))] <span class="comment">#统计开始前每天更新为0</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(root_dir):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> name.endswith(<span class="string">&quot;md&quot;</span>) <span class="keyword">or</span> name == <span class="string">&quot;本月更新.md&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        name = os.path.join(root_dir,name)</span><br><span class="line">        <span class="comment">#print(name)</span></span><br><span class="line"></span><br><span class="line">        f = <span class="built_in">open</span>(name,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        f_head,f_body = parse_md(f)</span><br><span class="line">        <span class="comment">#print(f_head)</span></span><br><span class="line">        <span class="keyword">for</span> cont <span class="keyword">in</span> f_head:</span><br><span class="line">            ma = re.findall(<span class="string">f&quot;date: <span class="subst">&#123;year&#125;</span>-<span class="subst">&#123;<span class="built_in">format</span>(month,<span class="string">&#x27;02d&#x27;</span>)&#125;</span>-(\d+)&quot;</span>,cont)</span><br><span class="line">            <span class="keyword">if</span> ma != []:</span><br><span class="line">                date_pos = <span class="built_in">int</span>(ma[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">                word_count[date_pos] += get_word_count(f_body)</span><br><span class="line">                blog_count[date_pos] += <span class="number">1</span></span><br><span class="line">                <span class="comment">#print(get_word_count(f_body))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> blog_count,word_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time = datetime.now()</span><br><span class="line">year = <span class="built_in">int</span>(time.strftime(<span class="string">&quot;%Y&quot;</span>)) <span class="comment">#今天的年</span></span><br><span class="line">month = <span class="built_in">int</span>(time.strftime(<span class="string">&quot;%m&quot;</span>)) <span class="comment"># 今天月份</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bc, wc = get_writing_freq(root_dir = <span class="string">&quot;./source/_posts&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = <span class="built_in">open</span>(<span class="string">&quot;./source/_posts/本月更新.md&quot;</span>,<span class="string">&quot;r&quot;</span>)</span><br><span class="line">f_head,f_data = parse_md(data)</span><br><span class="line"></span><br><span class="line">cal = <span class="built_in">open</span>(<span class="string">&quot;./source/_posts/本月更新.md&quot;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">cal.write(<span class="string">&quot;---\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> f_head:</span><br><span class="line">    cal.write(i+<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">cal.write(<span class="string">&quot;---\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">print_table(year, month,bc,wc)</span><br><span class="line">cal.close()</span><br></pre></td></tr></table></figure></div></p><p>其中，更新后的deploy方法如下： <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SH"><figure class="iseeu highlight /sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python generate_freq.py</span><br><span class="line">hexo clean</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></div></p>]]></content>
      
      
      <categories>
          
          <category> 开发记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo折腾记</title>
      <link href="/fa20dd9d.html"/>
      <url>/fa20dd9d.html</url>
      
        <content type="html"><![CDATA[<p>今天卡发了半天hexo建站，还是遇到了不少bug，网上已有的教程大多是针对老版本的<code>hexo</code>和<code>Next</code>来说的，新版本的开发还是有一些新的trick。我在开发中大概遇到了这些问题：</p><h4 id="字数统计插件">字数统计插件</h4><p>使用了<code>hexo-symbols-count-time</code>字数统计插件，其中中文占两个字符，每分钟阅读速度275。发现所有的文章都显示<code>预计时间1min</code>，还以为是bug，找了好久最终原因竟然是：- 所有<span class="math inline">\(\leq 1min\)</span>他都会显示约1min，是因为我前面的文章太短了…</p><h4 id="访问量统计">访问量统计</h4><p>使用 <code>不蒜子</code>的访问量统计工具，但是显示全是0。最后发现问题竟然是： - 本地<code>localhost</code> 不获取访问量，需要上 <code>deploy</code>才能正确显示… - 以及那个接口今年已经炸了，需要换个mirror:<code>//cdn.jsdelivr.net/npm/busuanzi@2.3.0</code>, 全文搜索<code>busuanzi-count</code> 替换即可</p><h4 id="引用静态文件">引用静态文件</h4><p>发现使用 hash地址作为url以后所有的静态文件都不能引用了？ -最后发现好像是路径地址计算的问题：所有的引用都要从<code>./source</code>作为地址的起点进行计算，同时<code>./_posts</code>文件夹似乎会被忽略？？最终解决办法是在<code>source</code>下新建了<code>./source/file</code>文件夹，把静态变量放到里面，再渲染</p><h4 id="相关文章推荐">相关文章推荐</h4><p>这个是最蠢的…我用 <code>hexo-related-popular-posts</code>库来实现相关文章推荐的功能，发现不管怎么样都推荐不了相关文章： -结果是他的推荐算法是根据tags的相关性来的，但是我那会只有一篇博客，所以算不出来相关文章，组建就被<code>disbale</code>了…</p>]]></content>
      
      
      <categories>
          
          <category> 开发记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 探索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四子棋大作业</title>
      <link href="/ede8f462.html"/>
      <url>/ede8f462.html</url>
      
        <content type="html"><![CDATA[<p>由于之前数据丢失问题，markdown源码已经不得而知了，就放一个pdf的文件吧…</p><div class="pdfobject-container" data-target="/files/connect_four.pdf" data-height="500px"></div>]]></content>
      
      
      <categories>
          
          <category> 课程与作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/ff05b5bf.html"/>
      <url>/ff05b5bf.html</url>
      
        <content type="html"><![CDATA[<h2 id="缘起">缘起</h2><p>今天是考完试的第二天，也是博客上线的第一天。其实我早有写点东西的打算：</p><ul><li>不仅是可以记录一下生活，分享一下胡乱折腾的东西，讲讲一些作业有意思的解法，以及分享一下论文的阅读笔记之类的。更重要的是，希望能借着分享的机会，让我对自己写的东西有一些更深邃的认知与认识，毕竟讲出来可以在脑子里留下90%，自己看一遍只能留下30%。</li><li>还有一个客观原因，就是我这学期电脑崩了一次，丢失了非常多的代码、报告等信息，而且我之前也没有给mac做过时间机器……这件事让我感觉到在备份一些东西的重要性。</li></ul><p>我暂时给博客起了个名字叫做"随缘随笔"，随笔是内容，随缘是状态。在后面我会定期或者不定期维护更新，如果看到有一些相关的评论，我也会快速回复。</p><h2 id="博客内容">博客内容</h2><p>在博客中，我目前想到写这几方面的内容：</p><ul><li>论文阅读笔记</li><li>一些作业的有趣解法</li><li>我喜欢的照片</li><li>一些日记</li><li>随笔：随笔的内容会比较杂乱，比如说这一篇。在随笔中，我可能会分享一些最近看到的有趣的事、有趣的游戏、有趣的电影等等。</li></ul><p>后面如果想到有什么好玩的、值得写一写的东西，我也会更新到博客上。</p><h2 id="暑假计划">暑假计划</h2><p>博客内容是一个大的计划，正好现在期末考试完，暑假到了。我在这个暑假打算先试运行一下博客，看能不能对自己有一些督促的作用。目前，我对于暑假的博客更新大概有四个计划：</p><ul><li><p>首先是我打算在暑假保持高深度、高质量的论文阅读，大约都是NLG方向的论文，我打算借着这个平台分享一些自己论文的阅读笔记，这个部分我打算放在一个名为“论文阅读笔记”的目录里。</p></li><li><p>另一方面，随着最后一门OS课考试结束，我大学的GPA时代也算是彻底结束了。之前的几年里，我还是在很多大作业中投入了不少的精力的。我打算趁着这个暑假把一些我觉得有趣的作业和我的实现细节分享出来。后面，如果我发现了什么有意思的东西，我也会分享出来。这一部分，我打算放在名为“作业”的目录里。</p></li><li><p>我有一个相机，也会不定期的做些摄影，我也有打算把我自己喜欢的一些照片(不一定是我的作品)放出来，可能会再加一点自己的点评。这一部分，我暂时还没有想好是做成一个目录，还是专门做一个板块。可能后面随着博客的完善，会在更晚的时候更新。</p></li><li><p>暑假阶段，我可能回去快手做小学期，做一些可能和机器学习相关的任务，我打算记录一下这段时间的经历，放到一个名为“小学期”的目录里。</p></li></ul><h2 id="技术细节">技术细节</h2><p>博客本身的技术简单，基于<code>hexo</code>和<code>next-theme</code>简单完成了markdown的编辑功能，后面接入了<code>gitalk</code>的评论。在后面，可能会根据实际的运营情况看看是不是要加上一些功能。</p><p>开发记录：<a href="/fa20dd9d.html" title="hexo折腾记">hexo折腾记</a></p><p>开发记录：<a href="/c344b452.html" title="给hexo添加日历">给hexo添加日历</a></p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
