<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/ava.JPG">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/ava.JPG">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1JEq-xmnmQSqcShMUdIO7vp6ILBVcY3cCm88rjBzuwA">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-flat-top.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.yynnyy.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="最近连续刷到几篇在预训练阶段，改变训练模式，通过thought augment，或者干脆直接就做on-policy rl的工作。这些新的方法，和从2020年开始大家就在做的paraphrase&#x2F;synthetic有本质区别吗？是比pretrain模式更好的模式吗？当然，这几篇工作在算力等级上存在明显的差距，所以没法直接对比。而且这种级别的设计差异其实也是没法对比的，变量太多，大家一般只能选一种。今">
<meta property="og:type" content="article">
<meta property="og:title" content="Synthetic, Paraphrase, Explain or Predict? 会比Pretrain+RL更优雅吗">
<meta property="og:url" content="https://www.yynnyy.cn/8b6b4f96">
<meta property="og:site_name" content="随缘随笔 &lt;&#x2F;br&gt; Insights Flow">
<meta property="og:description" content="最近连续刷到几篇在预训练阶段，改变训练模式，通过thought augment，或者干脆直接就做on-policy rl的工作。这些新的方法，和从2020年开始大家就在做的paraphrase&#x2F;synthetic有本质区别吗？是比pretrain模式更好的模式吗？当然，这几篇工作在算力等级上存在明显的差距，所以没法直接对比。而且这种级别的设计差异其实也是没法对比的，变量太多，大家一般只能选一种。今">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.yynnyy.cn/files/images/thought_augment/think-augment.png">
<meta property="article:published_time" content="2025-09-27T07:35:41.000Z">
<meta property="article:modified_time" content="2025-09-27T10:06:28.270Z">
<meta property="article:author" content="叶奕宁 &lt;&#x2F;br&gt; Yining_Ye">
<meta property="article:tag" content="计算机">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="Reasoning">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.yynnyy.cn/files/images/thought_augment/think-augment.png">

<link rel="canonical" href="https://www.yynnyy.cn/8b6b4f96.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Synthetic, Paraphrase, Explain or Predict? 会比Pretrain+RL更优雅吗 | 随缘随笔 <br> Insights Flow</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-N9BVC2B53T"></script>
    <script data-pjax>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-N9BVC2B53T');
      }
    </script>


  <script data-pjax>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9c4405386c515442d48e196ee239baf3";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="随缘随笔 </br> Insights Flow" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">随缘随笔 <br> Insights Flow</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页(Home Page)</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于(About me)</a>

  </li>
        <li class="menu-item menu-item-arxiv-insights">

    <a href="/arxiv_insights" rel="section"><i class="fa fa-cloud fa-fw"></i>Arxiv Insights</a>

  </li>
        <li class="menu-item menu-item-top-viewed">

    <a href="/top/" rel="section"><i class="fa fa-signal fa-fw"></i>top-viewed</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>所有笔记(All Blogs)</a>

  </li>
        <li class="menu-item menu-item-switchlanguage">

    <a href="/tags/English" rel="section"><i class="fa fa-language fa-fw"></i>Show only English blogs</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索(Blog Search)
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.yynnyy.cn/8b6b4f96">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/ava.JPG">
      <meta itemprop="name" content="叶奕宁 </br> Yining_Ye">
      <meta itemprop="description" content="高效率工作，低效率生活</br> work swiftly, live softly">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="随缘随笔 </br> Insights Flow">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Synthetic, Paraphrase, Explain or Predict? 会比Pretrain+RL更优雅吗
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-27 15:35:41 / 修改时间：18:06:28" itemprop="dateCreated datePublished" datetime="2025-09-27T15:35:41+08:00">2025-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文阅读笔记</span></a>
                </span>
            </span>

          
            <span id="/8b6b4f96" class="post-meta-item leancloud_visitors" data-flag-title="Synthetic, Paraphrase, Explain or Predict? 会比Pretrain+RL更优雅吗" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/8b6b4f96#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/8b6b4f96" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>


        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <div>
            <img src="/../../files/images/thought_augment/think-augment.png" itemprop="contentUrl">
        </div>
        <p>最近连续刷到几篇在预训练阶段，改变训练模式，通过thought
augment，或者干脆直接就做on-policy
rl的工作。这些新的方法，和从2020年开始大家就在做的paraphrase/synthetic有本质区别吗？是比pretrain模式更好的模式吗？当然，这几篇工作在算力等级上存在明显的差距，所以没法直接对比。而且这种级别的设计差异其实也是没法对比的，变量太多，大家一般只能选一种。今天我们只是来浅浅了解一下几种工作都是如何开展的吧</p>
<blockquote>
<p>参考文献：</p>
<ul>
<li><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2509.19249">Reinforcement Learning
on Pre-Training Data</a></p></li>
<li><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2509.20186">Thinking Augmented
Pre-training</a></p></li>
<li><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2412.08905">Phi-4 Technical
Report</a></p></li>
</ul>
</blockquote>
<span id="more"></span>
<h2 id="motivation-and-the-strong-baseline">Motivation, and the Strong
Baseline</h2>
<p>语言是人类智慧的结晶，高质量的语料往往简洁而充满智慧。LLM的训练方式是直接预测整个语料，并行地去“看到前文，预测下一个token”。这会带来两个问题：</p>
<ol type="1">
<li>预训练语料是无状态、无目的、任意口吻的。但大家在使用LLM时往往是有目的的、希望模型用比较helpful/hramless的方式说话。这里其实带来了训练测试不一致，导致从gpt1到chatgpt，花了5年才发现“rlhf”做对齐，才让模型的知识和推理能力更好地服务于真实世界。</li>
<li>已有的语料往往是人类思考的结果，而不是过程。而且在很多情况下，还会有高度的抽象。这种情况下，语料里面预测每个token的难度，实际上差距非常大，有的token简单、有的token困难。让模型硬学着去预测一些困难但有价值(valuable
and
hard-to-learn)的数据，模型就不得不倒向memorization，丢失了泛化性，没有利用好高质量训练数据本身的价值。显然，我们希望模型从语料中学到的不是背诵，而是领悟背后那深刻的思想。</li>
</ol>
<p>为了解决train on raw
corpus的问题，大家一般用的方法是paraphrase：找到另一个更大的模型（或者是GPT这种distiallation
teacher），让它直接把语料重写成没有语病、更流畅、包含更多解释的形式，然后目标模型在重写后的语料上训练。这种方法简单好用，但很多人也会把它叫做蒸馏。是因为这个过程中，主要需要靠一个正常情况下都比自己更强的模型来重写数据，所以并不是语料变好了，而是“偷学”了强模型的知识。其直接论据是，用更小的模型重写数据做预训练，效果好像就不那么好了。据了解，某些团队pretrain的收益主要来自于强大的distillation
teacher的 1T token level paraphrase。</p>
<blockquote>
<p>不过最近有些新的工作，尤其是在o1领域的，发现似乎paraphrase模型太强了也不行？但这还不太solid，并且和主线关系不大，以后有机会再探讨</p>
</blockquote>
<p>除了paraphrase，另一套方案叫做synthetic。这仍然要找一个teacher
model来，但不是重写，而是直接从无到有地开始写文档，然后让目标模型学习。这种方法实操的难度更大，因为很容易就会让产生的数据dieversity受限、或者风格收敛，进而导致模型训练崩溃（前几年有些研究，管这个叫“疯牛病”）。预训练崩溃是一个很痛苦、很丢人的事情，所以这么干的人并不多。</p>
<p><img src="../../files/images/thought_augment/phi4-token.png"></p>
<p>Phi系列模型，是这个领域中做的比较极端的例子：它作为一个14B的模型，却足足找
GPT4 写了300B token。而从他的recipe来看（注意每个数据源，都需要用
unique-token <span class="math inline">\(\times\)</span>
epoch做计算），40%来自于合成数据，15%来自于praphrase，（20%
code是保住code能力必须的东西），而原始互联网语料只剩下了15%.
从结果来说，phi-4很强，作为一个2024年底放出来的开源模型工作，现在都到2025年下半年了，同参数量的新出的模型，仍然不太敢把phi4作为baseline去对比。很多时候，一个简单的招数，在花了很多钱以后，还是很好用的。</p>
<p>phi4给出了做合成数据时的四个原则，我原封不动地贴在这里。知易行难，真把合成数据走好，是一个很困难的事情</p>
<ul>
<li><p>Diversity: The data should comprehensively cover subtopics and
skills within each domain. This requires curating diverse seeds from
organic sources.</p></li>
<li><p>Nuance and Complexity: Effective training requires nuanced,
non-trivial examples that reflect the complexity and the richness of the
domain. Data must go beyond basics to include edge cases and advanced
examples.</p></li>
<li><p>Accuracy: Code should execute correctly, proofs should be valid,
and explanations should adhere to established knowledge, etc.</p></li>
<li><p>Chain-of-Thought: Data should encourage systematic reasoning,
teaching the model various approaches to the problems in a step-by-step
manner. This fosters coherent outputs for complex tasks.</p></li>
</ul>
<blockquote>
<p>当然，phi4 还做了很多别的创新，具体可以参考tech
report。这里我们只说和主线相关的内容</p>
</blockquote>
<h2 id="new-paradigm">New Paradigm?</h2>
<p>时间来到2025年，gpt5都出来了，大家有新的招数吗？还真有，而且仅仅这一周我就看到了两个工作。第一个工作来自于亚研院。其实这个工作在6月份就有个前文(<a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2506.08007">Reinforcement
Pre-Training</a>)，但内容和一会要讲的另一篇类似，咱们先按下不表</p>
<p><img src="../../files/images/thought_augment/augment_thought.png"></p>
<p>这篇工作中，作者同样讲到了valuable but
hard-to-learn的问题，并提出自己的想法：能不能在parapharse和synthetic中找到一个中间模式——在原始数据中，让另一个模型写一个explain放在后面，然后带着原始语料一起用crossentropy
loss训？这个方法叫做thought-augment，作者举了一个很好的例子，我把它放在了文章的首图。</p>
<p>这个过程和paraphrase改格式目的不太一样，主要希望distillation
teacher对数据添加更多的解释和推理，使得student
model可以把一些有价值的、高难度的数据更好地学会。我们可以从两个不同的视角来理解这个过程：</p>
<ol type="1">
<li><p>蒸馏视角：对于小模型，永远无法学会复杂的语料，所以pretrain的过程中，很大一部分算力都是在浪费，并让模型退化成了memorization模式。但给了explain以后，小模型也可以开始学会了，这个过程让pretrain质变了。</p></li>
<li><p>训练效率视角：从这个视角里，我们认为模型的预训练是一直尝试在领悟数据的含义，并在scale够大后，仍然是可以把数据学会的，只是训练效率很低，可能前5T
token都是在浪费。如果可以“让语料和自己双向奔赴”，那么是不是1T
token就可以学得很好了呢？如果从这个视角理解，可能teacher
model不需要真的比student大很多，甚至是一个更小的、或者上一代的自己也可以</p>
<blockquote>
<p>“提高训练效率”并不是一个听起来没什么创新性的东西。恰恰相反，我觉得提升已有、已经work的方法的训练效率，比创新方法更重要。真实世界里，绝大多数情况下，都是在优化已有的东西。因为新东西大概率不会更好，或者仅仅在reported
domain里更好</p>
</blockquote></li>
</ol>
<p>作者没有对比paraphrase
baseline，而是对比了原始语料pretrain的baseline，作者用qwen 7B来给100B
token语料进行thought-augment，原始语料横跨reason和knowledge两个domain。然后来训练8B
model。</p>
<p>做了两个setting：</p>
<ol type="1">
<li>scratch：从随机初始化开始训练。这里又可以进一步区分，是语料瓶颈(10epoch)，还是算力瓶颈
(最终也在第一个epoch内)。实际的预训练基本都是第二种，因为通过paraphrase，你可以得到无穷无尽的、不一样的文本</li>
<li>mid-train：直接找一个挺强的baseline拿过来接着训。这个主要是为了观察这种模式的收益，是否会随着模型能力增强而递减</li>
</ol>
<p><img src="../../files/images/thought_augment/explain-curve.png" style="zoom:33%;"></p>
<p>⬆️在scratch
setting上可以发现，think-augment训练的loss明显更低，不过这里因为训练集同时有原始语料和explain语料，后者一般ppl都会低很多，所以不太可比。同时在下游任务上，think-augment模式的reward上升非常快，基本有两个数量级的训练效率差异。</p>
<p>⬇️对于midtrain setting中，收益就会相对减少，这主要是因为baseline
non-training的分数就已经比较高了</p>
<p><img src="../../files/images/thought_augment/midtrain.png" style="zoom:33%;"></p>
<p>作者还发现了一些有意思的现象：作者统计了不同数据源中，think-augment部分的长度，发现在更加reason-intensive的领域上，就会更长。从另一个视角来看，这其实是通过数据的形式，对训练集的固有熵做对抗，理论上还可以在pretrain阶段，就激发模型在难的问题上多想、再简单问题上保持高效。</p>
<p>总体而言，这篇工作可以看作是在o1时代的paraphrase的工作，更关注reasoning、重写的方式也更优雅。但从这篇工作的实验setting来看，会存在有几个风险：</p>
<ol type="1">
<li>scratch
setting上，想要做出和baseline的增量收益，其实和原始数据的质量有很大关系。如果原始数据质量非常差的话，甚至baseline都不会收敛。从这里出发，其实一个更好的比较形式是paraphrase
baseline，因为这样可以缓解本身pretrain数据质量带来的影响，同时本文提出的think
augment模式也可以认为是一种更优雅形式的paraphase。</li>
<li>由于think-augment是用o1-style
model来做的，所以这个过程中其实吃到了很多test-time
scaling的红利。但从math这些reasoning-intensive的benchmark来看，testtime红利本身就有很大的收益，没法消融这个收益是来源于
1)ready-to-learn的数据，还是 2) 蒸馏了更多的o1-style数据</li>
</ol>
<p>值得一提的是，think-augment模式虽然效果更好，也更贵。作者用了20k
A100-hour，才做完qwen7B的100B token的重写。如果用工业级pretrain的10T
token，70B /gpt-api teacher
来估算，这花费就是天文数字了。如果把training-token固定的比较方法，改成money-budget固定的比较方法(raw-pretrain得以训练更多token)，可能结果又有不同。</p>
<h2 id="更激进的尝试">更激进的尝试</h2>
<p>我们重新思考一下刚才think augment的想法，就会发现它类似于<a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2403.14589">actre</a>，有一个无法避免的问题：我们希望模型解释语料里面的事实和题目结果，来展现思考过程。但这会引入无法避免的幻觉——distillation
teacher由于提前看到了结果，所以永远可以做强行推理得出结论，但我们又很难以揭穿这种幻觉（因为这大概率需要花更多钱、用一个更强的distillation
teacher）。</p>
<p>从另外一个角度来想：既然think
augment目的是得出语料背后的原始推理过程，那我不如把顺序颠倒，换成一个更像
star/rft 风格的框架：</p>
<ol type="1">
<li>让模型看到前文，看不到后文</li>
<li>生成thought，得出一个结论</li>
<li>和真正的后文做对比，筛出来结果一致的样本</li>
</ol>
<p>这就是接下来腾讯这篇工作的主要想法。作者甚至更进一步，不是离线地这样筛数据，而是用on-policy的grpo来过数据</p>
<p><img src="../../files/images/thought_augment/rl-pretrain.png"></p>
<p>当然，虽然作者把这个工作叫做rl
pretrain，声称是一个有潜力做到pretrain级别的事情，但作者其实开展的setting很小。想法激进，实验也就相对更初步。</p>
<p>实验部分比较亮点的地方是，作者在对比下游任务时，不仅观察了pass@1，还观察了pass@8。这其实是更关注这个"rl
pretrain"过程对模型entropy/diversity的影响。作为对比的是，传统的close-domain
rl，一般最后不是以entropy飞掉结束，就是以entropy掉没结束。对应的pass@8上升趋势远远不及pass<span class="citation" data-cites="1">@1</span>。理论上这种open-domain的训练，是会对于模型提高/保住diversity有帮助的</p>
<blockquote>
<p>对于这个o1-style
RL对于pass@n上限的影响，其实最近也有一些很棒的工作。未来如果有时间的话，我也可以分享给大家</p>
</blockquote>
<p><img src="../../files/images/thought_augment/pass@8.png"></p>
<p>想要真的把这个模式做到pretrain-level，而不是qa-level，就要克服一个真实世界的挑战：如何定义“相等”？对于数学题或者一些知识问答，我可以认为答案正确就think正确，这个假阴假阳率不会高。但如果是开放世界语料，比如story，或者twitter
chat sequence，什么叫一致呢？作者在这里用generative reward
model，让模型来判断是否一致。但其实作者并没有关于grm本身准确率、以及rm准确率对于训练结果影响的分析，这是我比较关心的问题</p>
<h2 id="我的思考">我的思考</h2>
<p>我们不妨分析一下这两篇工作。把think-augment的方式反过来，变成rl-pretrain，什么是必要的组件？假如真的去做工业级的pretrain，算法效果要给训练稳定性和效率让步，那么这种模式引入了什么开销和不稳定性呢？</p>
<ol type="1">
<li>一个专门的模块来判断think和语料的一致性？从Phi-4的原则看，这是必要的，但其实我们确实不知道，如果本身有比较强的不一致时，这类方法是否还有正收益。判断模块会带来额外的计算开销，而且会筛掉一部分数据，让总体的算力空转更明显。</li>
<li>on-policy？腾讯这篇工作主要区别是，永远用比较“近”的参数更新模型。从结果上来，接受率的上升是比较明显的。这对应着筛掉数据的比例在快速下降。然而，on-policy
rollout的问题是，rollout系统会带来训练利用率下降和训练稳定性下降，而如果我是离线地把整个数据集infer一遍，相对就会利用率更高</li>
</ol>
<p><img src="../../files/images/thought_augment/reward.png" style="zoom:33%;"></p>
<p>现在大家常常在说“pretrain-level
RL”，是指在RL阶段投入和预训练同样多的算力，开展pretrain +
rl两阶段对等算力的训练。概念听上去很酷，但实际的开展目标是“万卡集群跑单次close-domain
RL”。这几篇工作给出了另一个理解这个问题的视角：能不能反过来把pretrain做得就更像rl，甚至干脆就是RL呢？</p>
<p>如果从这个维度讨论，其实还有一些工作是可以参与进来的：</p>
<ol type="1">
<li>Open-domain RL：最近有些工作试图从pretrain
dataset中挖掘一些更diverse的rl数据。这个过程本质上是把预训练数据冲构成一个“easy
to
verify”的形式。这样想，就和腾讯这个工作有点像了。这个领域，从o1出来之前，就有人在传统instruction
following搞了，最近又迭代到了o1 setting上</li>
<li>Importance Sampling on SFT：最近有些工作仍然使用single-forward,
single-backward的方式过数。但setting基础，loss就不基础。会把迭代方式做出一个old_policy，让模型优先去学习自己需要的东西。代价是一部分token的梯度被clip掉，没了</li>
</ol>
<p>我认为，可以下一个可以影响世界的、和GPT1一样伟大的学术工作，今年已经诞生了。它很可能是一种SFT/RL耦合的模式，先在语料上做一个pretrain(从RL的话说也可以叫cold-start)，然后让模型自由gen
+
verify，进行一个比较open-domain训练。对我们来说最重要的是：我该怎么发现它，并在2025年就认知到他是对的？🤔</p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/7dbea4cd.html" rel="bookmark">重读STaR，与o1随想</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/a50a8741.html" rel="bookmark">论文阅读[精读]-LANGUAGE MODELING VIA STOCHASTIC PROCESSES(2)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/7cd10148.html" rel="bookmark">Self-Consistency之我见，兼More Agents is All You Need</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/d074522f.html" rel="bookmark">论文阅读[精读]-Let’s Verify Step by Step</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/feddc200.html" rel="bookmark">论文阅读[精读]-RRHF: Rank Responses to Align Language Models with Human Feedback without tears</a></div>
    </li>
  </ul>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/" rel="tag"><i class="fa fa-tag"></i> 计算机</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/Reasoning/" rel="tag"><i class="fa fa-tag"></i> Reasoning</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 强化学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/5ef9ba91" rel="prev" title="2025-09-26-insights">
      <i class="fa fa-chevron-left"></i> 2025-09-26-insights
    </a></div>
      <div class="post-nav-item">
    <a href="/aff2e11c" rel="next" title="2025-09-29-insights">
      2025-09-29-insights <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>



<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>



        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#motivation-and-the-strong-baseline"><span class="nav-number">1.</span> <span class="nav-text">Motivation, and the Strong
Baseline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#new-paradigm"><span class="nav-number">2.</span> <span class="nav-text">New Paradigm?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E6%BF%80%E8%BF%9B%E7%9A%84%E5%B0%9D%E8%AF%95"><span class="nav-number">3.</span> <span class="nav-text">更激进的尝试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%91%E7%9A%84%E6%80%9D%E8%80%83"><span class="nav-number">4.</span> <span class="nav-text">我的思考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="叶奕宁 </br> Yining_Ye" src="/images/ava.JPG">
  <p class="site-author-name" itemprop="name">叶奕宁 <br> Yining_Ye</p>
  <div class="site-description" itemprop="description">高效率工作，低效率生活<br> work swiftly, live softly</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">136</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yeyn19" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yeyn19" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yeyn2001@gmail.com" title="E-Mail → mailto:yeyn2001@gmail.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=AUKaXYkAAAAJ" title="GoogleScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;AUKaXYkAAAAJ" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-leanpub fa-fw"></i>GoogleScholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Yining_Ye" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Yining_Ye" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <!-- modify icon to fire by szw -->
      <i class="fa fa-history fa-" aria-hidden="true"></i>
      近期文章(Recent Update)
    </div>
    <ul class="links-of-blogroll-list">
      
      
        <li class="recent_posts_li">
          <a href="/92691516" title="2025-09-30-insights" target="_blank">2025-09-30-insights</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/aff2e11c" title="2025-09-29-insights" target="_blank">2025-09-29-insights</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/8b6b4f96" title="Synthetic, Paraphrase, Explain or Predict? 会比Pretrain+RL更优雅吗" target="_blank">Synthetic, Paraphrase, Explain or Predict? 会比Pretrain+RL更优雅吗</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/5ef9ba91" title="2025-09-26-insights" target="_blank">2025-09-26-insights</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/b5ce0192" title="2025-09-25-insights" target="_blank">2025-09-25-insights</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/5a0c6aac" title="2025-09-24-insights" target="_blank">2025-09-24-insights</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/7dbea4cd" title="重读STaR，与o1随想" target="_blank">重读STaR，与o1随想</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/f992cb6b" title="论文阅读[精读]-Manyshot-ICL: 在context中重现传统AI的可能性" target="_blank">论文阅读[精读]-Manyshot-ICL: 在context中重现传统AI的可能性</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/fbc665c3" title="论文阅读[精读]-MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training" target="_blank">论文阅读[精读]-MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</a>
        </li>
      
        <li class="recent_posts_li">
          <a href="/61495969" title="从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和" synthetic data"是一个意思吗？" target="_blank">从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和"Synthetic Data"是一个意思吗？</a>
        </li>
      
    </ul>
  </div>




      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>简体中文</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical>
      
        <option value="zh-CN" data-href="/8b6b4f96" selected>
          简体中文
        </option>
      
        <option value="en" data-href="/en/8b6b4f96" selected>
          English
        </option>
      
    </select>
  </div>

        
<font color="#000000">

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">叶奕宁 <br> Yining_Ye</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">309k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:40</span>
</div>

<i class="fa fa-user-md"></i>
<span id="sitetime"></span>
<script language="javascript">
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* 
		Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数
        */
		var t1 = Date.UTC(2022,06,17,12,00,00); //北京时间2018-2-13 00:00:00
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 建站 "+diffYears+" 年 "+diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}
	siteTime();
</script>





</font>

        








      </div>
    </footer>
  </div>

  
  
  <script color="0,0,0" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  
  <script data-pjax>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>









<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'A0NQ8NrwSuSevoacmYvPv0Qk-gzGzoHsz',
      appKey     : '2b7GYcEUoHtDSGAJDIEH1TAB',
      placeholder: "向我留言",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'en,zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

    </div>
</body>
</html>
