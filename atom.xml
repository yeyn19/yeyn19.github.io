<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>随缘随笔 &lt;/br&gt; Insights Flow</title>
  
  
  <link href="https://www.yynnyy.cn/atom.xml" rel="self"/>
  
  <link href="https://www.yynnyy.cn/"/>
  <updated>2024-10-08T08:27:58.249Z</updated>
  <id>https://www.yynnyy.cn/</id>
  
  <author>
    <name>叶奕宁 &lt;/br&gt; Yining_Ye</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2024-10-08-insights</title>
    <link href="https://www.yynnyy.cn/acae49ec"/>
    <id>https://www.yynnyy.cn/acae49ec</id>
    <published>2024-10-08T05:03:55.000Z</published>
    <updated>2024-10-08T08:27:58.249Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR的投稿论文质量就是高，感觉逐渐出现了一些看起来像是o1的工作</p><h2id="learning-how-hard-to-think-input-adaptive-allocation-of-lm-computation"><ahref="https://arxiv.org/pdf/2410.04707"><strong>Learning How Hard toThink: Input-Adaptive Allocation of LM Computation</strong></a></h2><p>JacobAndreas的工作：作者探索了能否根据任务难度动态申请计算空间，然后让模型决定做CoT、self-consistency之类的inference-scaling技术。通过这种方法，可以在保证最优效果的前提下，省下来超过50%的计算资源</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/adaptive.png" style="zoom:33%;" ></p><h2id="beyond-scalar-reward-model-learning-generative-judge-from-preference-data"><ahref="https://arxiv.org/pdf/2410.03742"><strong>Beyond Scalar RewardModel: Learning Generative Judge from Preference Data</strong></a></h2><p>liuyiqun老师转型了，去搞post-training了。这次是generative rewardmodel，是说让rewardmodel也是生成式的、CoT的，而不是从human偏序里蒸馏出来的float。虽然不是第一篇，但我很喜欢这个方向，现在看见就转</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/gen-r.png"></p><h2id="swe-bench-multimodal-do-ai-systems-generalize-to-visual-software-domains"><ahref="https://arxiv.org/pdf/2410.03859"><strong>SWE-bench Multimodal: DoAI Systems Generalize to Visual Software Domains?</strong></a></h2><p>我最开始看到这篇工作，还以为是让模型通过键鼠操作vscode来编程……仔细一看是解决多模态的编程问题。作者构建了一个新的bench，输入的问题描述或者testcase里最少有一张图片，需要多模态模型运用多模态能力来理解或者解决问题</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/swe-m.png"></p><h2id="inference-scaling-for-long-context-retrieval-augmented-generation"><ahref="https://arxiv.org/pdf/2410.04343"><strong>Inference Scaling forLong-Context Retrieval Augmented Generation</strong></a></h2><p>Deepmind的工作：作者结合了之前的many-shotICL，认为其是RAG的inference-timecompute领域的一种scaling方案。另一种方案是使用不同的RAGprompt抽取很多次做一堆infer。对于每个query，都可以使用这两种方案做各种实验，比如100-shot，或者用10个ragprompt做10次10-shot拼成一个100-shot，总体budget一致。作者经过研究，发现如果给所有budget做到optimal的情况下，performance和context-length是可以符合scalinglaw的</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/inference-rag.png"></p><h2 id="grounding-partially-defined-events-in-multimodal-data"><ahref="https://arxiv.org/pdf/2410.05267">Grounding Partially-DefinedEvents in Multimodal Data</a></h2><p>作者标注了一个dense数据集，可以把caption中的各种实体、事件链接到时间信息，并进一步在空间中给出坐标，包含大约15h的标注视频。</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/multivent-G.png" style="zoom:33%;"  ></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;ICLR的投稿论文质量就是高，感觉逐渐出现了一些看起来像是o1的工作&lt;/p&gt;
&lt;h2 id=&quot;learning-how-hard-to-think-input-adaptive-allocation-of-lm-computation&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.04707&quot;&gt;&lt;strong&gt;Learning How Hard to
Think: Input-Adaptive Allocation of LM Computation&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Jacob
Andreas的工作：作者探索了能否根据任务难度动态申请计算空间，然后让模型决定做CoT、self-consistency之类的inference-scaling技术。通过这种方法，可以在保证最优效果的前提下，省下来超过50%的计算资源&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-10-07-10-11/adaptive.png&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;beyond-scalar-reward-model-learning-generative-judge-from-preference-data&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.03742&quot;&gt;&lt;strong&gt;Beyond Scalar Reward
Model: Learning Generative Judge from Preference Data&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;liuyiqun老师转型了，去搞post-training了。这次是generative reward
model，是说让reward
model也是生成式的、CoT的，而不是从human偏序里蒸馏出来的float。虽然不是第一篇，但我很喜欢这个方向，现在看见就转&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-10-07-10-11/gen-r.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;swe-bench-multimodal-do-ai-systems-generalize-to-visual-software-domains&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.03859&quot;&gt;&lt;strong&gt;SWE-bench Multimodal: Do
AI Systems Generalize to Visual Software Domains?&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;我最开始看到这篇工作，还以为是让模型通过键鼠操作vscode来编程……仔细一看是解决多模态的编程问题。作者构建了一个新的bench，输入的问题描述或者testcase里最少有一张图片，需要多模态模型运用多模态能力来理解或者解决问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-10-07-10-11/swe-m.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2024-10-07-insights</title>
    <link href="https://www.yynnyy.cn/5da51261"/>
    <id>https://www.yynnyy.cn/5da51261</id>
    <published>2024-10-08T02:19:48.000Z</published>
    <updated>2024-10-08T02:51:56.249Z</updated>
    
    <content type="html"><![CDATA[<h2id="frame-voyager-learning-to-query-frames-for-video-large-language-models"><ahref="https://arxiv.org/pdf/2410.03226"><strong>Frame-Voyager: Learningto Query Frames for Video Large Language Models</strong></a></h2><p>字节出的一篇关键帧抽取的工作，是一套以终为始的思路：关键帧抽取是为了更好地进行VQA。那可以先随机抽一大堆关键帧组，然后每个组合都回答一次vqa，根据答案的质量(其实是正确答案的ppl)反过来给出关键帧抽取质量的评价，找出来关键帧抽取方案的正负样本</p><blockquote><p>很聪明！不过它的瓶颈看起来是随机抽到好帧的概率，当视频变长以后，这种方案随机采样到真正好帧的可能性就会下降。</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/frame.png" ></p><h2 id="better-instruction-following-through-minimum-bayes-risk"><ahref="https://arxiv.org/pdf/2410.02902"><strong>BetterInstruction-Following Through Minimum Bayes Risk</strong></a></h2><p>Neubig的工作，作者搞了个神奇的方法：大家现在做Post training的大致方法都是生成一堆回答然后给一些评价。这个事情其实可以online地去实现，如果可以给评价loss，那其实就可以用MBR解码找到<spanclass="math inline">\(\sum_{y&#39;}P(y’)·L(y,y&#39;)\)</span>最小的y。这个方法甚至都不需要rewardmodel，就能给一堆样本拿到一个得分。作者首先实验了一下在运行时直接MBR找最好的样本，发现这个样本一般确实是最好的。然后又尝试了self-train，把MBR样本当成正样本，发现训完的模型直接greedy-decoding就和之前用MBR挑出来的样本差不多好</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/mbr.png" ></p><h2 id="exploring-the-benefit-of-activation-sparsity-in-pre-training"><ahref="https://arxiv.org/pdf/2410.03440">Exploring the Benefit ofActivation Sparsity in Pre-training</a></h2><p>师兄的工作，作者发现在模型预训练中也是存在稀疏激活现象的，由此开发了一套集和Densetraining和MoEtraining的pipeline，根据激活情况进行动态的routing，由此训出来的模型既可以作为dense模型保持效果，也可以用MoE的方式加快推理速度</p><p><img src="../../files/images/arxiv-insights/2024-10-07-10-11/sparse.png" ></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;frame-voyager-learning-to-query-frames-for-video-large-language-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.03226&quot;&gt;&lt;strong&gt;Frame-Voyager: Learning
to Query Frames for Video Large Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;字节出的一篇关键帧抽取的工作，是一套以终为始的思路：关键帧抽取是为了更好地进行VQA。那可以先随机抽一大堆关键帧组，然后每个组合都回答一次vqa，根据答案的质量(其实是正确答案的ppl)反过来给出关键帧抽取质量的评价，找出来关键帧抽取方案的正负样本&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;很聪明！不过它的瓶颈看起来是随机抽到好帧的概率，当视频变长以后，这种方案随机采样到真正好帧的可能性就会下降。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-10-07-10-11/frame.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;better-instruction-following-through-minimum-bayes-risk&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.02902&quot;&gt;&lt;strong&gt;Better
Instruction-Following Through Minimum Bayes Risk&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Neubig的工作，作者搞了个神奇的方法：大家现在做Post training
的大致方法都是生成一堆回答然后给一些评价。这个事情其实可以online地去实现，如果可以给评价loss，那其实就可以用MBR解码找到&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;sum_{y&amp;#39;}
P(y’)·L(y,y&amp;#39;)&#92;)&lt;/span&gt;最小的y。这个方法甚至都不需要reward
model，就能给一堆样本拿到一个得分。作者首先实验了一下在运行时直接MBR找最好的样本，发现这个样本一般确实是最好的。然后又尝试了self-train，把MBR样本当成正样本，发现训完的模型直接greedy-decoding就和之前用MBR挑出来的样本差不多好&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-10-07-10-11/mbr.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;exploring-the-benefit-of-activation-sparsity-in-pre-training&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.03440&quot;&gt;Exploring the Benefit of
Activation Sparsity in Pre-training&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;师兄的工作，作者发现在模型预训练中也是存在稀疏激活现象的，由此开发了一套集和Dense
training和MoE
training的pipeline，根据激活情况进行动态的routing，由此训出来的模型既可以作为dense模型保持效果，也可以用MoE的方式加快推理速度&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-10-07-10-11/sparse.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2024-10-04-insights</title>
    <link href="https://www.yynnyy.cn/b692a962"/>
    <id>https://www.yynnyy.cn/b692a962</id>
    <published>2024-10-07T05:43:43.000Z</published>
    <updated>2024-10-07T06:48:14.104Z</updated>
    
    <content type="html"><![CDATA[<p>今天论文都挺好的</p><h2id="from-pixels-to-tokens-byte-pair-encoding-on-quantized-visual-modalities"><ahref="https://arxiv.org/pdf/2410.02155"><strong>From Pixels to Tokens:Byte-Pair Encoding on Quantized Visual Modalities</strong></a></h2><p>这篇更好玩，是omni架构系列的工作，作者的想法是：能不能把bpe的思路应用都图片上，给每个patch打出来一个id，然后做imageencoder。作者发现这样搞效果巨好</p><blockquote><p>不是，哥们……</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/visual-bpe.png" ></p><h2 id="video-instruction-tuning-with-synthetic-data"><ahref="https://arxiv.org/pdf/2410.02713"><strong>Video Instruction TuningWith Synthetic Data</strong></a></h2><p>很经典的llava系工作，作者整合了各种videocaption数据集，最后合成出来了187k videosft数据，然后找了一波超参数，搞了个效果不错的llava-video模型。</p><blockquote><p>这个和之前的llava-interleave的区别在哪</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/video-it.png" ></p><h2id="typedthinker-typed-thinking-improves-large-language-model-reasoning"><ahref="https://arxiv.org/pdf/2410.01952"><strong>TypedThinker: TypedThinking Improves Large Language Model Reasoning</strong></a></h2><p>Qwen团队的工作，幽默老中十一挂论文。沿着self-discover继续深入，作者发现不同的reasoningmodule适合不同的场景，如果让模型给每个query尝试不同的reasoningmodule，然后把答案正确的样本挑出来作为正样本，似乎可以让模型持续地自我迭代学会如何灵活运用各种reasoningmodule。</p><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/typed-thinker.png" style="zoom:33%;"  ></p><h2id="avg-llava-a-large-multimodal-model-with-adaptive-visual-granularity"><ahref="https://arxiv.org/pdf/2410.02745"><strong>AVG-LLaVA: A LargeMultimodal Model with Adaptive Visual Granularity</strong></a></h2><p>一篇挺有趣的工作：作者发现VLM中，图片分辨率是一个很本质的参数，但不同的query实际上对分辨率的要求不同。能不能设计一个router根据query来确定分辨率呢？作者做了一波实验，搞了一个多分辨率级联的系统，发现还真可以</p><blockquote><p>这个方案感觉有点像是自动丢弃visualtoken的方向，这个方向感觉一直没有找到很好的回传梯度的方案</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/gar.png" ></p><h2 id="how-to-train-long-context-language-models-effectively"><ahref="https://arxiv.org/pdf/2410.02660"><strong>How to TrainLong-Context Language Models (Effectively)</strong></a></h2><p>Tianyu学长的工作，看标题就很牛，实际上确实工作量很大。主要结果都在下面了↓</p><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/howto.png" ></p><h2id="adaptive-inference-time-compute-llms-can-predict-if-they-can-do-better-even-mid-generation"><ahref="https://arxiv.org/pdf/2410.02725"><strong>Adaptive Inference-TimeCompute: LLMs Can Predict if They Can Do Better, EvenMid-Generation</strong></a></h2><p>stanford的工作，好像最近挺少见他们的工作不知道为啥。这篇行文挺有deepmind风格的。作者讲的事情是，很多任务中，模型在生成到一半的时候已经犯错了，但已有工作往往是等生成完再打分。能不能让模型有权利生成一半直接掐掉呢？作者探索了一下，发现还真行</p><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/mid-generation.png" ></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天论文都挺好的&lt;/p&gt;
&lt;h2 id=&quot;from-pixels-to-tokens-byte-pair-encoding-on-quantized-visual-modalities&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.02155&quot;&gt;&lt;strong&gt;From Pixels to Tokens:
Byte-Pair Encoding on Quantized Visual Modalities&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;这篇更好玩，是omni架构系列的工作，作者的想法是：能不能把bpe的思路应用都图片上，给每个patch打出来一个id，然后做image
encoder。作者发现这样搞效果巨好&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不是，哥们……&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/visual-bpe.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;video-instruction-tuning-with-synthetic-data&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.02713&quot;&gt;&lt;strong&gt;Video Instruction Tuning
With Synthetic Data&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;很经典的llava系工作，作者整合了各种video
caption数据集，最后合成出来了187k video
sft数据，然后找了一波超参数，搞了个效果不错的llava-video模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个和之前的llava-interleave的区别在哪&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/video-it.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;typedthinker-typed-thinking-improves-large-language-model-reasoning&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.01952&quot;&gt;&lt;strong&gt;TypedThinker: Typed
Thinking Improves Large Language Model Reasoning&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2024-10-03-insights</title>
    <link href="https://www.yynnyy.cn/544eb21b"/>
    <id>https://www.yynnyy.cn/544eb21b</id>
    <published>2024-10-07T03:18:50.000Z</published>
    <updated>2024-10-07T03:49:19.880Z</updated>
    
    <content type="html"><![CDATA[<h2id="leopard-a-vision-language-model-for-text-rich-multi-image-tasks"><ahref="https://arxiv.org/pdf/2410.01744">LEOPARD : A Vision LanguageModel for Text-Rich Multi-Image Tasks</a></h2><p>一个专门瞄准text-rich场景的VLM，作者构建了1M的高质量SFT数据，然后设计了一套自动根据图片质量申请visualtoken数量的pipeline，取得了不错的效果</p><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/text-rich.png" ></p><h2 id="visual-perception-in-text-strings"><ahref="https://arxiv.org/pdf/2410.01733"><strong>Visual Perception inText Strings</strong></a></h2><p>阴间大队的又一力作，作者发现，ascii字符渲染出来的图片天然有文字、图片两种表示，而且其转换是无损的。所以，VLM在这种asciiart场景表现如何呢？作者发现：</p><ul><li>4o表现傲视群雄</li><li>在同时给出两种模态输入以后，没有模型能有提升，大家还是只会使用图片模态。经过SFT，提升也不明显</li></ul><blockquote><p>我感觉作者这个故事这么讲有点小，可以讲一个“探索MLLM对于模态fusion的能力，由此需要去找一个各模态无损压缩的场景……”</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/ascii.png" ></p><h2id="when-a-language-model-is-optimized-for-reasoning-does-it-still-show-embers-of-autoregression-an-analysis-of-openai-o1"><ahref="https://arxiv.org/pdf/2410.01792">When a language model isoptimized for reasoning, does it still show embers of autoregression? Ananalysis of OpenAI o1</a></h2><p>shunyu yao参与的工作，这篇工作其实有个前文叫"Embers ofAutoregression"，大致探索了LLM在各种任务上是不是在罕见词场景下做的更差，然后发现所有模型都在罕见词场景下都非常的差。作者这次试了试o1，看看会不会缓解这个问题：</p><ol type="1"><li>发现o1已经好了很多，不过仍然受到了这个影响。换句话说，通过inferencescaling，可能仍然难以解决预训练分布带来的bias</li><li>o1表现出来一些有趣的性质，在常见词上thinkingtoken很少，罕见词问题thinking token会更多。也许thinkingtoken的数量变化，能发现模型大致衡量到了问题的难度</li></ol><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/eoa.png" ></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;leopard-a-vision-language-model-for-text-rich-multi-image-tasks&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.01744&quot;&gt;LEOPARD : A Vision Language
Model for Text-Rich Multi-Image Tasks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一个专门瞄准text-rich场景的VLM，作者构建了1M的高质量SFT数据，然后设计了一套自动根据图片质量申请visual
token数量的pipeline，取得了不错的效果&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/text-rich.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;visual-perception-in-text-strings&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.01733&quot;&gt;&lt;strong&gt;Visual Perception in
Text Strings&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;阴间大队的又一力作，作者发现，ascii字符渲染出来的图片天然有文字、图片两种表示，而且其转换是无损的。所以，VLM在这种ascii
art场景表现如何呢？作者发现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4o表现傲视群雄&lt;/li&gt;
&lt;li&gt;在同时给出两种模态输入以后，没有模型能有提升，大家还是只会使用图片模态。经过SFT，提升也不明显&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;我感觉作者这个故事这么讲有点小，可以讲一个“探索MLLM对于模态fusion的能力，由此需要去找一个各模态无损压缩的场景……”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/ascii.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;when-a-language-model-is-optimized-for-reasoning-does-it-still-show-embers-of-autoregression-an-analysis-of-openai-o1&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2410.01792&quot;&gt;When a language model is
optimized for reasoning, does it still show embers of autoregression? An
analysis of OpenAI o1&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;shunyu yao参与的工作，这篇工作其实有个前文叫&quot;Embers of
Autoregression&quot;，大致探索了LLM在各种任务上是不是在罕见词场景下做的更差，然后发现所有模型都在罕见词场景下都非常的差。作者这次试了试o1，看看会不会缓解这个问题：&lt;/p&gt;</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2024-10-02-insights</title>
    <link href="https://www.yynnyy.cn/bb8cd925"/>
    <id>https://www.yynnyy.cn/bb8cd925</id>
    <published>2024-10-07T03:04:16.000Z</published>
    <updated>2024-10-07T03:49:19.879Z</updated>
    
    <content type="html"><![CDATA[<h2id="videoclip-xl-advancing-long-description-understanding-for-video-clip-models"><ahref="">VideoCLIP-XL: Advancing Long Description Understanding for VideoCLIP Models</a></h2><p>作者这里的XL，指的不是size，而是<code>extra-length</code>，探索了video-CLIP模型能否处理这种超长的densecaption。由此，作者设计了一套从数据到模型的变化，然后还仿照F1score的方式在description ranking任务上定义了两个指标</p><blockquote><p>我听喜欢这个方向的，顺应了re-caption和caption越来越长的大潮流</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/video-clip.png" ></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;videoclip-xl-advancing-long-description-understanding-for-video-clip-models&quot;&gt;&lt;a href&gt;VideoCLIP-XL: Advancing Long Description Understanding for Video
CLIP Models&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;作者这里的XL，指的不是size，而是&lt;code&gt;extra-length&lt;/code&gt;，探索了video-CLIP模型能否处理这种超长的dense
caption。由此，作者设计了一套从数据到模型的变化，然后还仿照F1
score的方式在description ranking任务上定义了两个指标&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我听喜欢这个方向的，顺应了re-caption和caption越来越长的大潮流&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/video-clip.png&quot;&gt;&lt;/p&gt;
</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2024-10-01-insights</title>
    <link href="https://www.yynnyy.cn/50bb6226"/>
    <id>https://www.yynnyy.cn/50bb6226</id>
    <published>2024-10-06T04:54:18.000Z</published>
    <updated>2024-10-06T11:35:14.316Z</updated>
    
    <content type="html"><![CDATA[<p>国庆节出来172篇工作</p><h2 id="scaling-optimal-lr-across-token-horizons"><ahref="https://arxiv.org/pdf/2409.19913"><strong>Scaling Optimal LRAcross Token Horizons</strong></a></h2><p>Microsoft的工作，但是行文很有openAI的风范。作者想要探索训练大模型时的learningrate选择，能否从小模型试验中predict到大模型最优值呢？下面这张图基本说明了核心结论：</p><ol type="1"><li>在不同的训练token数量下，最优lr都不一样。预计训练的token量越大，最优lr越小</li><li>在确定目标模型大小的情况下，最优lr随着token量的变化可以通过小模型的曲线，和大模型在少token的最优lr寻找上做拟合</li></ol><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/lr.png" ></p><h2id="mm1.5-methods-analysis-insights-from-multimodal-llm-fine-tuning"><ahref="https://arxiv.org/pdf/2409.20566">MM1.5: Methods, Analysis &amp;Insights from Multimodal LLM Fine-tuning</a></h2><p>之前我出过一篇mm1的论文阅读笔记，今天更新了mm1.5,这次在常规升级的基础上，添加了mm1.5-video视频理解和mm1.5-UI，继承了ferret-ui的手机界面理解能力</p><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/mm1.5.png" ></p><h2 id="do-influence-functions-work-on-large-language-models"><ahref="https://arxiv.org/pdf/2409.19998"><strong>Do Influence FunctionsWork on Large Language Models?</strong></a></h2><p>这篇工作瞄准的是前几年的明星方向<code>influence function</code>。这个方向是想要找到对参数改变影响最小的一些训练数据，然后剔除掉给模型涨分。作者在大模型里重新实验了这个方向，发现定义里存在一个谬误：对于LLM来说，参数改变的多少和性能的提升相关性不大。所以,这个领域在大模型时代可能需要做出改变</p><h2 id="instance-adaptive-zero-shot-chain-of-thought-prompting"><ahref="https://arxiv.org/pdf/2409.20441">Instance-adaptive Zero-shotChain-of-Thought Prompting</a></h2><p>作者发现，不同的reasoningprompt有他适合的场景，然后作者想要找到一个很好的映射方法给不同的question找到适合的prompt。</p><blockquote><p>这篇的思路很像我很喜欢的那篇"self-discover"论文</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/feel.png" ></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;国庆节出来172篇工作&lt;/p&gt;
&lt;h2 id=&quot;scaling-optimal-lr-across-token-horizons&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2409.19913&quot;&gt;&lt;strong&gt;Scaling Optimal LR
Across Token Horizons&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Microsoft的工作，但是行文很有openAI的风范。作者想要探索训练大模型时的learning
rate选择，能否从小模型试验中predict到大模型最优值呢？下面这张图基本说明了核心结论：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;在不同的训练token数量下，最优lr都不一样。预计训练的token量越大，最优lr越小&lt;/li&gt;
&lt;li&gt;在确定目标模型大小的情况下，最优lr随着token量的变化可以通过小模型的曲线，和大模型在少token的最优lr寻找上做拟合&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/lr.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;mm1.5-methods-analysis-insights-from-multimodal-llm-fine-tuning&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2409.20566&quot;&gt;MM1.5: Methods, Analysis &amp;amp;
Insights from Multimodal LLM Fine-tuning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;之前我出过一篇mm1的论文阅读笔记，今天更新了mm1.5,这次在常规升级的基础上，添加了mm1.5-video视频理解和mm1.5-UI，继承了ferret-ui的手机界面理解能力&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/mm1.5.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;do-influence-functions-work-on-large-language-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2409.19998&quot;&gt;&lt;strong&gt;Do Influence Functions
Work on Large Language Models?&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;这篇工作瞄准的是前几年的明星方向&lt;code&gt;influence function&lt;/code&gt;。这个方向是想要找到对参数改变影响最小的一些训练数据，然后剔除掉给模型涨分。作者在大模型里重新实验了这个方向，发现定义里存在一个谬误：对于LLM来说，参数改变的多少和性能的提升相关性不大。所以,这个领域在大模型时代可能需要做出改变&lt;/p&gt;</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2024-09-30-insights</title>
    <link href="https://www.yynnyy.cn/3c018487"/>
    <id>https://www.yynnyy.cn/3c018487</id>
    <published>2024-10-06T04:41:25.000Z</published>
    <updated>2024-10-06T11:35:14.315Z</updated>
    
    <content type="html"><![CDATA[<h2id="charting-the-future-using-chart-question-answering-for-scalable-evaluation-of-llm-driven-data-visualizations"><ahref="https://arxiv.org/pdf/2409.18764">Charting the Future: Using ChartQuestion-Answering for Scalable Evaluation of LLM-Driven DataVisualizations</a></h2><p>这个工作名字有点拗口，但其实很新颖：作者想要评测LLM做数据可视化的质量。可视化中的一个重要因素就是信息不丢失，作者想到，能不能用VQA的形式评测？如果另一个VQA模型回答对了问题，说明可视化模型信息没丢失。</p><blockquote><p>这个方向挺小众，不过我感觉这个方法还可以深挖呀，抽象一下：一个任务可以退化成更简单的任务，那可以通过评测退化的任务来给出原任务的一个noisy-reward</p></blockquote><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/gan.png" style="zoom:33%;"  ></p><h2 id="a-survey-on-the-honesty-of-large-language-models"><ahref="https://arxiv.org/pdf/2409.18786"><strong>A Survey on the Honestyof Large Language Models</strong></a></h2><p>一篇LLMhonesty的survey，这个领域一直挺小众的，大家好像叫法也挺多的，很多人叫自己hallucination，还有人叫"knownand unknown"，感觉需要有个谁出来给个具体的定义，整合一下。</p><p><img src="../../files/images/arxiv-insights/2024-09-30-10-04/survey.png" ></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;charting-the-future-using-chart-question-answering-for-scalable-evaluation-of-llm-driven-data-visualizations&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2409.18764&quot;&gt;Charting the Future: Using Chart
Question-Answering for Scalable Evaluation of LLM-Driven Data
Visualizations&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;这个工作名字有点拗口，但其实很新颖：作者想要评测LLM做数据可视化的质量。可视化中的一个重要因素就是信息不丢失，作者想到，能不能用VQA的形式评测？如果另一个VQA模型回答对了问题，说明可视化模型信息没丢失。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个方向挺小众，不过我感觉这个方法还可以深挖呀，抽象一下：一个任务可以退化成更简单的任务，那可以通过评测退化的任务来给出原任务的一个noisy-reward&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/gan.png&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;a-survey-on-the-honesty-of-large-language-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2409.18786&quot;&gt;&lt;strong&gt;A Survey on the Honesty
of Large Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一篇LLM
honesty的survey，这个领域一直挺小众的，大家好像叫法也挺多的，很多人叫自己hallucination，还有人叫&quot;known
and unknown&quot;，感觉需要有个谁出来给个具体的定义，整合一下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../../files/images/arxiv-insights/2024-09-30-10-04/survey.png&quot;&gt;&lt;/p&gt;
</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-Manyshot-ICL: 在context中重现传统AI的可能性</title>
    <link href="https://www.yynnyy.cn/f992cb6b"/>
    <id>https://www.yynnyy.cn/f992cb6b</id>
    <published>2024-05-27T08:30:28.000Z</published>
    <updated>2024-08-09T09:26:46.977Z</updated>
    
    <content type="html"><![CDATA[<p>今天来讲讲<a href="https://arxiv.org/abs/2404.11018">Many-ShotIn-Context Learning</a>，大概是deepmind一个月前的文章，读下来和之前JasonWei那篇"Large Models do In-Context LearningDifferently"的阅读体验有点像，是一篇"暗合scaling天意"的文章。</p><p>看完了我把他和另外两篇论文联系了起来，想到了未来LLM在context重建AI的可能性。最后，推荐大家读一下原文，deepmind论文就像乐高，阅读(拼搭)体验一直很好……</p><p>参考资料：</p><blockquote><p>Many-Shot In-Context Learning</p><p>Many-Shot In-Context Learning in Multimodal Foundation Models</p><p>In-Context Reinforcement Learning with Algorithm Distillation</p></blockquote><span id="more"></span><p>作者团队来自Google Deepmind</p><p><img src="../../files/images/manyshot_icl/authors.png"></p><h2 id="introduction">introduction</h2><p>这篇论文方法上没什么好说的，大家都知道in-context learning:把一个任务的很多input-outputpairs放在prompt里，然后模型就可以在不更新自身参数的情况下，"现场学会"一个任务，并对最后给出的input预测出来结果。</p><p>从high-level的角度讲，我觉得这个能力是因为模型学会了所谓的”worldmodel“。传统的AI领域，大家一般会建模出来一个任务，作为输入空间到输出空间的映射(比如情感分类)，<spanclass="math inline">\(f: \mathcal{X} \rightarrow\mathcal{Y}\)</span>，接下来考虑如何训练一个模型<spanclass="math inline">\(f_\theta\)</span>可以做好一个任务。对于LLM来说，从instructiontuning开始，大家开始认为整个世界就是一个<spanclass="math inline">\(\mathcal{X}\)</span>，所有的所谓任务都只是从<spanclass="math inline">\(\mathcal{X}\)</span>​里面的一个采样，因此只需要学会一个<spanclass="math inline">\(f_\theta(\mathcal{X})\)</span>就可以表征所有的任务，in-contextlearning正是从这个情况下涌现出来的能力。</p><p>作者考虑了一个现实的问题：之前的in-contextlearning评测几乎都是3-shot, 5-shot,8-shot。但是今天的LLM已经可以把自己的context拓展到128k，甚至10M(gemini)。那么，有人试过用更多的样本放在prompt里，效果会更好吗？作者把这个setting就叫做manyshot场景</p><p><img src="../../files/images/manyshot_icl/perf.png"></p><p>作者测试了gemini在不同场景下的manyshot表现，发现几乎都比few-shot场景效果好很多。</p><p>为了解释这个看起来有点神奇的现象，作者又定义了两个阴性对照和阳性对照的setting：</p><ul><li>reinforcedICL：先自己生成一堆input-output，然后根据output正确性筛选出好的样本作为shot</li><li>unsupervised ICL：模型生成一堆input，不拼output，看看能不能提升</li></ul><h2 id="performance">Performance</h2><p><img src="../../files/images/manyshot_icl/trans.png"></p><p><img src="../../files/images/manyshot_icl/sum.png"></p><p><img src="../../files/images/manyshot_icl/verify.png"></p><p>作者在各种场景下尝试了many-shot，然后报告了效果随着shot增加的变化情况，可以看到，几乎在所有场景下，提升shot的数量都会让效果变得更好。</p><p>作者为了进一步探索这个现象，尝试了上面提到的reinforcedICL和unsupervised ICL</p><p><img src="../../files/images/manyshot_icl/math.png"></p><p>并且发现比起用groundtruth样本作为ICL样本，模型自己生成的样本甚至效果要更好。而且，这种样本是可以迁移到别的任务上的，右边的图是用MATH数据集生成的样本来作为GSM8K的manyshot样本。</p><p>为什么unsupervisedICL效果很好，难道只需要看到一些query？作者类似于之前那篇weak-to-strong的思路，给了一个基于直觉的解释：如果模型本来就会做目标任务，可能只需要用一些query帮助他”联想“到预训练数据中的一些知识作为锚点，来让他在做现在的input时发散更多的知识。</p><p>从这个思路出发，对于数学这样的场景，预训练见过很多了，可能非常需要这种”联想“。</p><p>还有一个最有意思的实验设计，和大家分享一下啊：作者想要证明，manyshot的效果来源于去掉了预训练数据中的bias。如果大家想证明这个结论，该如何设计实验？</p><p>作者类似于之前那个"large model performs ICL differently",找到了一个情感分类任务，设计了对照组：</p><ul><li>flip：把标签反过来，即positive变成negative，negative变成positive。这个和预训练知识相反，模型不得不在context中学习</li><li>abstract: 把所有的标签变成A、B、C这种没有语义的东西</li></ul><p>通过这两个对照，作者就能勘测出预训练bias对效果的影响，作者发现：最开始，两个对照组的准确率都不太行，但随着shot增加，三种方法的效果最终收敛到了同一水平。这说明：manyshot场景可以逐步削减模型对于预训练和下游任务的理解偏差，进而提升任务的效果。</p><p><img src="../../files/images/manyshot_icl/bias.png"></p><p>最后，作者报告了一个解释不了的现象：随着shot增加，作者看了groundtruth的ppl，发现越来越低。但是，如果统计acc的话，实际上250-shot场景的acc是不如125的。在predict-scaling这个领域，大家往往喜欢用更弱的模型预测更强模型的效果。从scaling曲线上讲，随着几乎<spanclass="math inline">\(ppl \propto \log(N-shot)\)</span>，预测ppl似乎是可行的。然而，更低的ppl却不能带来更高的得分，这和传统benchmark场景的结论相反。</p><blockquote><p>为什么会这样？我想起来之前在读<ahref="https://arxiv.org/abs/2205.14334">Teaching models to express theiruncertainty inwords</a>中作者提到了ppl和1)模型对答案的信心值2)模型表达这个解答过程的信心值都有关。我们可以思考一下many-shot场景，当前面拼了非常多的样本时，模型对于1)和2)的信心值会倾向于更高还是更低呢？同样的，如果模型对于任何答案的信心值都变得更高了，那么可能就更难以区分出好的答案和坏的答案了</p></blockquote><p><img src="../../files/images/manyshot_icl/nll.png"></p><p>对于上面的问题，作者在附录中还给出模型对于正样本和负样本的NLL。可以观察到，总体而言，似乎样本越多，模型越没法使用NLL区分正/负样本</p><p><img src="../../files/images/manyshot_icl/contrastive.png"></p><h2 id="几个问题和我的思考">几个问题和我的思考</h2><p>看完这篇论文确实收益良多，不过我似乎产生了更多的问题，不知道大家有没有类似的感受。</p><h3 id="最像样本相似度">最像样本相似度</h3><p>首先，我有另一个视角去理解这个现象：我们如果统计manyshot样本中和当前queryembedding最像的top1 similarity。然后画个散点图，其中横坐标是top1similarity，纵坐标是正确与否，然后给每个横坐标区间统计平均正确率，变成柱状图。即样本越像，acc越高。而且，我感觉对于所有shot的场景下，这个柱状图可能会遵循同一个分布……这符合大家对预训练模型的认知："LLMcan't perform zeroshot, performance depends on shots in training data"对偶的说法就是"LLM’s manyshot performance depends on similarity of thein-contextexamples"。不过这个视角解释不了另外一个现象，就是few-shot场景下模型表现对于few-shot样本顺序的敏感性</p><h3 id="和finetune的关系">和finetune的关系</h3><p>再有，如果我们有1024个样本，传统的视角下，我们肯定会想要finetune，但这篇论文可能是只能访问API，没有对比finetune的baseline……如果我们相信了前文对于”概念和联想“的直觉解释，那么finetuneGemini对于Gemini到底意味着什么？</p><blockquote><p>传统AI中，会任务finetune是在学习知识，或者说找到更泛化的概念。但是，如果LLM本身就具有泛化的概念，是否finetune这个过程只是帮助模型建立几个"思维的锚"呢？</p></blockquote><p>这让我想起来曾经在delta tuning看到有个领域叫”Intrinsicdimension“，他们试图找到模型最少可以在更新多少个参数的情况下，达到全参数微调的效果的90%，并把这个数字叫做LLM的Intrinsicdimension(ID)，他们发现基础能力却强，ID就越小。这是不是从某种程度上支撑了上面的猜想：越强的模型，本身的retrieve能力越强，就越不再需要很多所谓的"联想锚点"，可以直接从当前instruction里做联想。</p><p>另一个领域是之前的weak-to-stronggeneralization，他们使用带噪声的数据集训练GPT4，发现GPT4自己把数据的噪声恢复了，在测试集上效果还是很好。这是不是也说明，可能answer并不重要，关键是激发一下模型对于问题的理解。也就是说，如果在weak-to-strong场景下，只在query上加loss，会不会都不用answer模型就学会了？</p><blockquote><p>甚至可以开展一个实验，比如我有5000条数据。其中100条我在answer加loss，剩下4900条我在query加loss，这样训出来的模型和和正常训练的模型，效果上也会有个PGR(performancegap recovered)吗？</p></blockquote><h3id="在context中重建传统ai甚至世界知识">在context中重建传统AI、甚至世界知识</h3><p>最后我想说的是，这篇工作让我发现，随着context的增加，大家其实可以在context中重建传统AI算法。这篇论文在context中添加非常多的样本，叫做manyshot，其实和传统RL中的finetune有点像。我们抽象的看，就是Gemini在接受到这些样本以后，自己抽象出来了一个"完成这个任务的模型"。</p><p>Gemini-1.5report里其实就做了类似的东西：他们找到了一门叫做<strong>Kalamang</strong>的罕见语言，网络上肯定找不到资料。让模型看完这个语言的字典，发现Gemini就可以翻译kalamang了。所以，如果模型有很强的智力，他有能力通过context去理解世界。</p><blockquote><p>这是一个很恐怖的事情：试想，如果某一天来了三体人，他们的三体模型甚至不需要微调，只是(和人一样、纯前向地)看了一下人类世界的一些语料，就可以像人一样完成人可以完成的所有事情。</p></blockquote><p>在学界，其实有一些类似的工作：</p><ul><li><p>RL：deepmind曾经做了一篇叫"in-contextRL"的工作，他们会把ppo、dpo等算法训练的过程记录下来，丢到一个context里，让一个长模型在这个上面做建模，看能不能进行”algorithmdistillation“，学到比原来的算法更高效的"rl算法"。不过这篇论文是训模型的工作，如果我们context很长，我能不能把"一次rl训练过程”作为1-shot，然后进行manyshot，模型就可以learnto learn rl in-context了呢？</p></li><li><p>Align: 进一步地，之前yuchen lin有一篇工作叫做<ahref="https://arxiv.org/pdf/2312.01552">The unlocking spell on basellms</a>，发现其实只需要找到一些in-context样本，就可以让模型在很多case上"看起来像是做过rlhf"。如果模型的context够长，我用manyshot的形式把alpaca-52k丢进去，模型就能做好rlhf吗？</p><blockquote><p>从这个思路继续思考：所谓的"人类偏好"到底激发出了模型的哪些"思维锚"？</p></blockquote></li></ul><p>未来，随着LLM的context越来越长，可能我们现在的所有传统算法(比如推荐)都可以被模型通过in-context的方式去解决。可能rag也不复存在了……"long-contextLLM is many-shot learner, and zero-shot world model"</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天来讲讲&lt;a href=&quot;https://arxiv.org/abs/2404.11018&quot;&gt;Many-Shot
In-Context Learning&lt;/a&gt;，大概是deepmind一个月前的文章，读下来和之前Jason
Wei那篇&quot;Large Models do In-Context Learning
Differently&quot;的阅读体验有点像，是一篇&quot;暗合scaling天意&quot;的文章。&lt;/p&gt;
&lt;p&gt;看完了我把他和另外两篇论文联系了起来，想到了未来LLM在context重建AI的可能性。最后，推荐大家读一下原文，deepmind论文就像乐高，阅读(拼搭)体验一直很好……&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Many-Shot In-Context Learning&lt;/p&gt;
&lt;p&gt;Many-Shot In-Context Learning in Multimodal Foundation Models&lt;/p&gt;
&lt;p&gt;In-Context Reinforcement Learning with Algorithm Distillation&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="预训练模型" scheme="https://www.yynnyy.cn/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</title>
    <link href="https://www.yynnyy.cn/fbc665c3"/>
    <id>https://www.yynnyy.cn/fbc665c3</id>
    <published>2024-03-23T04:10:02.000Z</published>
    <updated>2024-08-09T09:26:46.977Z</updated>
    
    <content type="html"><![CDATA[<p>最近Apple出了自己的30B多模态大模型，涌现出了多模态的in-contextlearning效果，论文里一句"evenbetter"让我想到库克那个嗓音……作者说明了很多在训练中收获到的经验教训，这是我最近几个月看的写法最清楚的一篇论文。正好借此讲讲多模态大模型：目前学界大火的VLM，到底是怎么跑的？</p><span id="more"></span><h2 id="introduction">Introduction</h2><p><img src="../../files/images/mm1/authors.png"></p><p>作者团队来自苹果，看来苹果说的"今年wwdc上大模型"真有希望了？</p><p>在pretrainmodels领域，大家一直都想把之前的所有任务的数据整合到一起，把之前一堆独立模型做的事情统一到一个模型里面,由此诞生了LLM技术。最近大家关注的重点开始转向多模态：除了文本领域的任务，能不能同时把和图片理解相关的任务也统一进去?具体来说，如果输入同时含有图片文本，多个图片文本交错排列，模型能不能在理解图片和文本的基础上，输出文本。这就是MLLM(multimodalLLM)或者VLM(Vision-Language Model)</p><p>虽然学界、业界在这方面的探索不少，但已有的工作基本都不够开放：要么就是闭源模型给个API，要么就是开源模型只给个权重。即使有些工作会开源数据，但是没有人会开放讨论他们对于模型结构的选择、对于训练数据的选择、对于训练超参数的选择以及背后的原因。</p><p>作者在本篇工作中开放的讨论了所有的细节，结论简单来说就是以下几点：</p><ol type="1"><li>在模型结构方面，下面几个要素的重要性递减：vision-encoder的分辨率、vision-encoder大小。VL-connector选择对于最终的表现几乎没有影响</li><li>在数据方面，主要有以下几种数据：text-only数据、image-caption数据、互联网文本-图片交错数据，作者发现：<ol type="1"><li>互联网文本图片交错数据和text-only数据对于in-contextLearning、few-shot Learning至关重要</li><li>image-caption数据对于zero-shot Performance至关重要</li></ol></li></ol><p>最后，从一系列insight出发，作者scaling了训练过程，训练了3B、7B、30B的VLM，甚至在3B和7B规模下尝试了top2的MoE架构，并在各个层面上达到了SOTA的效果</p><blockquote><p>除了没开源weight，基本全给你了……</p></blockquote><h2 id="vlm长啥样">VLM长啥样？</h2><p><img src="../../files/images/mm1/design-choice.png"></p><p>目前的VLM基本都是分三个部分，如上图的左图：</p><ol type="1"><li>image-encoder，负责把图片编码成为embedding，同时尽可能不要损失信息</li><li>VL-connector，负责把image-encoder的输出做一些转换，比如MLP，对齐到LLM的wordembedding空间</li><li>decoder-only LLM：把上一步的输出直接作为多个Token的Wordembedding输入，然后跑后面的LLMnext-token-prediction。会在text输出的部分计算loss</li></ol><h3 id="image-encoder">Image-encoder</h3><p>image-encoder一般也是transformer，是一个ViT。他首先会把图片按照从左到右、从上到下切成不同的小正方形，打成多个patch，每个patch的分辨率都是比如14x14。接下来每个patch会使用一个卷积层编码成为一个vector，然后多个patchvector拼在一起当成多个“token”的word embedding，加上positionembedding后直接进入transformer encoder，输出会是每个patch一个hiddenstate。</p><p>这个encoder有两种训练方法：</p><ol type="1"><li>contrastive loss:大名鼎鼎的CLIP。找到一大堆互联网上的挨着的(图片-文本)对作为正样本，然后随机其他的对作为负样本，然后用刚才的ViT作为image-encoder，用另一个比如T5之类的作为textencoder，拿到两个embedding，跑对比学习的loss：希望正样本-正样本之间的cosine相似度大于负样本-正样本之间的cosine相似度</li><li>reconstructiveloss：这个就是传说中的VAE。用ViT编码完了以后会把图片变成一个embedding，为了保证这个embedding尽量信息无损，会后面再接上一个image-decoder去预测原来的图片长什么样子。预测的越准loss就越小。为了防止模型去memorize每个image的样子，还会有个辅助KLloss去保证所有image的embedding在embedding空间的分布尽可能均匀</li></ol><p>这两种方法有个特点：都是无监督的，只要有一大堆数据就能起效果。前者需要(图片-文本)对，但有个好处是数据可以自己合成，可以人工在text-caption上加上各种丰富准确的细节来提高训练的要求，进而增强模型。后者只需要一大堆图片，在一些dense的任务上表现更好</p><p><img src="../../files/images/mm1/image-encoder.png"></p><p>作者在这里做了消融实验，最左边那列AIM是reconstructionloss，CLIP是对比学习loss。architecture基本都是ViT，H的参数量比L大。imageRes代表的是训练的时候的训练数据的图片分辨率都是多少。Data是指用什么数据做的训练。</p><p>明显可以发现：</p><ol type="1"><li>image res这个变量对最终的效果差距最大</li><li>在CLIP中加入合成数据(VeCap)，会提升模型表现</li><li>两种loss基本都不咋影响效果，都差不多</li></ol><p>作者最终选了CLIP + 高清encoder</p><h3 id="vision-language-connector">Vision-Language Connector</h3><p>这一部分就是输入image-encoder的output，转换到Wordembedding空间，这里面有个至关重要的问题：一张图片应该转换到LLM里的多少个token？</p><ol type="1"><li>显然token更多，保留的细节更多，模型的效果理应更好</li><li>然而，当token多的时候，尤其如果是多个图片(8个、16个)，很多个token其实训不起来，资源消耗太恐怖了</li></ol><p>作者在这里消融实验了一个image对应64、144token两种情况，然后尝试了224、336分辨率的image-encoder，以及不同的pooling策略，大致有这么几种实现：</p><ol type="1"><li>Average Pooling:由于Vit的token数大于目标token数量，就把相邻几个patchhidden-state取平均数，变成一个embedding，让数量变得一样了。再给每个token过一个MLP，作为LLM的一个tokenembedding</li><li>AttentionPooling：作者觉得vit的输出可能和llm的word-embedding不在一个子空间，需要一个Attention层变换一下。于是就加一个额外的Attention层，初始化k个query向量，然后用key、value变换阵把ViT的输出对齐过去(Attention输出在sequence-length维度上和query的长度一致)。这样Attention的输出就是k个tokenembedding，然后k可以取64、144，就对齐过去了</li><li>C-abstractor:把输出用某种卷积层操作(ResNet)转换到word-embedding空间</li></ol><p><img src="../../files/images/mm1/connector.png"></p><p>作者对比了一圈发现：vision token和imageresolution最重要，几种不同的pooling策略几乎没区别。即使在一个testset上好，在另一个上可能会更差</p><h3 id="pretrain-data">Pretrain-data</h3><p><img src="../../files/images/mm1/data.png"></p><p>最后，作者探索了pretrain数据对于结果的影响。这里有个歧义："pretrain"并不是真正的预训练。作者这里实际上会选取一个预训练好的LLM作为text-onlydecoder，然后选取一个在CLIP对比学习loss上预训练好的image-encoder。把他俩用一个新初始化的VL-connector拼在一起。把这个叫做"starttraining"。所以这里探讨的“pretraindata”单指VLM启动训练后的"pretrain"</p><p>VLM的pretrain-data基本分为三种：</p><ol type="1"><li>image-caption:正常VLM的预训练数据，也就是刚才CLIP里的图片文本对。作者会输入图片，让模型预测caption</li><li>interleaved image-text:从互联网上爬下来的语料。这里面都是图文交错的数据，作者直接把文本提取出来，在正常图片的位置放上图片，让模型预测所有文本的next-token-prediction</li><li>text-only：正常LLM预训练的数据，基本上是去掉图片后的互联网语料，以及github之类地方爬下来的各种代码，以及找到的一大堆各个学科的教材啥的</li></ol><p>其中，image-caption数据分为正常版和合成版本。所谓合成数据，就是用GPT-4v或者其他什么开源模型(这里说的VeCap用的vicuna，是一个Llama在gpt-3.5的输出上做过SFT的版本)，要求他给图片生成一些非常详细的caption，包括里面所有Object以及他们之间的位置空间语义承接关系之类的。</p><p>如果大家看过前几天写的DALL.E 3的笔记<a href="/61495969.html" title="从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和&quot;Synthetic Data&quot;是一个意思吗？">dall.e 3阅读笔记</a>，就知道正常的html里来的图片文本对的alt-text质量有多差，所以需要重新clean，去重写等等</p><p><img src="../../files/images/mm1/data-choice.png"></p><p>作者在这一部分做了最多的实验，因为在LLM领域现在大家也普遍认为数据是影响结果最重要的因素，大致发现了如下结论：</p><ol type="1"><li>interleaved data is instrumental for few-shot and text- onlyperformance, while captioning data lifts zero-shotperformance。作者猜测是因为互联网交错数据天生具有一些in-contextLearning的性质</li><li>text-only data helps with few-shot and text-only performance</li><li>Careful mixture of image and text data can yield op- timalmultimodal performance and retain strong text performance.</li><li>Synthetic data helps with few-shot learning.</li></ol><p>最后，作者公布了花了很多钱试出来的配方： caption / interleaved /text-only = 5:5:1</p><h2 id="mm1">MM1</h2><h3 id="pretrain">pretrain</h3><p>由上面的结论，作者把训练做了scaling，选择了如下方案：</p><ol type="1"><li>CLIP loss的ViT-H，378x378分辨率</li><li>144 token，c-abstractor模式的VL-connector</li><li>5:5:1的VLM pretrain-data</li></ol><p>作者在 9M, 85M, 302M, 1.2B几个规模上做了超参数搜索，并认为学习率<spanclass="math inline">\(\eta\)</span>应该和模型规模的对数成反比，由此预测了30B模型的最优Learningrate=2.2e-5。然后weight-decay <spanclass="math inline">\(\lambda\)</span>是lr的1/10 <spanclass="math display">\[\eta = \exp(-0.4214\ln(N) - 0.5535)\\\lambda = 0.1\eta\]</span> 另一面，作者还尝试了MoE架构，用了Mixtral-8x7B那种经典的top2router，在3B和7BLLM规模上做了实验。3B是每两层加一个64选2的FFN，7B是每四层有一个32选2的FFN。这些FFN都是从最开始的denseFFN初始化的，随着router的训练逐渐变得差异化了。为了稳定训练，作者还加了一些平衡routing的loss</p><blockquote><p>这方面，似乎Mixtral 8x7Breport说他们没加任何其他loss，也可以正常训练？不知道是不是VLM领域有新的bug</p></blockquote><p><img src="../../files/images/mm1/eval.png"></p><p>作者发现，这样训练出来的MM1系列模型，在所有规模上基本都是目前最好的VLM。然后MoE可以让Performance"even better"</p><h3 id="sft">SFT</h3><p>目前的公认解决方案，都是在Pretrain完以后，把能找到的学术任务拿过来造一个大号的数据集，让模型做一下supervisedfinetuning，再找一大堆chat数据让模型学着遵循人类乱七八糟的指令（比如"只能回复emoji"），最后再在测试集上测试效果</p><p>类似的，作者的SFT数据构成三部分：</p><ol type="1"><li>学术数据集转换来的数据</li><li>GPT-4v生成的给予图片的qa chat数据</li><li>text-only，训练正常LLM时使用的SFT数据</li></ol><p>所有数据掺在一起，SFT阶段随机喂数据。他们尝试了更高级的策略，发现效果基本没提升，就用了这个最简单的招</p><h3 id="high-resolution">high resolution</h3><p>测试的时候，还有问题：1)测试数据很多有巨多图片输入，或者需要8-shotin-context样本，这就需要最少8个图片。2)很多图片清晰度巨高，比如3840x2160分辨率。</p><p>这方面，有一个解决方案：Sub-image decomposition,比如对于1344×1344的图片，作者分成5张图丢给模型，每个都是672×672。第一张图是降采样的，后面四张分别是左上角到右下角的局部</p><blockquote><p>这里有个trick：这里的复杂度不是<spanclass="math inline">\(n^2\)</span>,而是<spanclass="math inline">\(n^4\)</span>。一个大图片会变成<spanclass="math inline">\(n^2\)</span>个token，后面LLM本身又以<spanclass="math inline">\(n^2\)</span>的复杂度跑前向，所以这个算法把一个大图变成5个小图实际上是省的。</p></blockquote><p><img src="../../files/images/mm1/sft-performance.png"></p><p>作者通过这个方法可以支持任意分辨率的图片，实际上最高到1792x1792还是会崩。然后作者对比了SFT阶段的效果，发现了几个核心：</p><ol type="1"><li>image分辨率对效果至关重要，如果输入的分辨率本身不高，怎么训练效果都不行</li><li>VLM的预训练对效果很重要。作者把预训练各个阶段的ckpt都拿过来做了一次SFT实验，发现VLM预训练越久，对应的SFTPerformance也越好</li></ol><p>另外，作者发现即使SFT数据全都是单张图片输入的，MM1还是具有多张图片输入时的in-context推理能力。并且，在Pretrain阶段探索到的insight，对于SFT阶段的Performance仍然成立。最后，作者在附录里展示了很多MM1的case，感觉这个模型的效果是真的很不错</p><blockquote><p>要是开源就好了</p></blockquote><h2 id="我的思考">我的思考</h2><ol type="1"><li>我印象中MM1是第一个正式描述自己的多张图片、图文交错情况下的in-contextLearning能力的模型。我感觉这个能力对于多模态的Agent很重要，目前我看到的一堆multi-modal的ReAct算法，都是把history用文本的形式拼到prompt了……感觉不是真正的multi-modalagent</li><li>另外最近有一些别的模型，比如Fuyu是一个decoder-only架构，图片的patchembedding不用走vit，直接在一个大decoder里走一次。代价是要重新预训练，好处是跑的很快……不知道哪种才是最好的方案，我猜目前这些方案是因为大家可以直接拿一个llama过来当LLM，效果比较稳？</li></ol><p><img src="../../files/images/mm1/fuyu.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近Apple出了自己的30B多模态大模型，涌现出了多模态的in-context
learning效果，论文里一句&quot;even
better&quot;让我想到库克那个嗓音……作者说明了很多在训练中收获到的经验教训，这是我最近几个月看的写法最清楚的一篇论文。正好借此讲讲多模态大模型：目前学界大火的VLM，到底是怎么跑的？&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多模态" scheme="https://www.yynnyy.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="Computer Vision" scheme="https://www.yynnyy.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和&quot;Synthetic Data&quot;是一个意思吗？</title>
    <link href="https://www.yynnyy.cn/61495969"/>
    <id>https://www.yynnyy.cn/61495969</id>
    <published>2024-03-02T03:08:13.000Z</published>
    <updated>2024-08-09T09:26:46.971Z</updated>
    
    <content type="html"><![CDATA[<p>最近Sora巨火，仿佛开启了AIGC的新时代。Jason Wei表示："Sora is theGPT-2 moment" for videogeneration。我在sora发布的大约第5个小时读了technicalreport，里面最打动我的其实是没提什么细节的recaption技术。让我回想想起了之前读DALL.E3论文时的愉快体验。</p><p>所以今天来分享一下DALL.E3论文里的recaption细节，并讨论几个问题和我的看法：1)OpenAI教你为什么要"先查看原始数据，再做创新"2)Recaption和大家一直在聊的"training on synthetic data"是一回事吗?3)recaption技术是否已经在(或者即将在)被其他领域使用？</p><p>另外，我总结了一下上篇笔记阅读量大的关键：语言表达要浅显易懂些，所以这篇笔记我可以声明一下：<strong>没学过AI也能看懂</strong>(我在博客里加了这个标签"fromscratch"，所有我认为不懂AI或者只知道一点点的人也能看懂的博客都会加上这个标签)</p><p>参考文献：</p><blockquote><p>https://openai.com/sora</p><p>Improving Image Generation with Better Captions</p><p>Automatic Instruction Optimization for Open-source LLM InstructionTuning</p><p>WaveCoder: Widespread And Versatile Enhanced Instruction Tuning withRefined Data Generation</p><p>Reformatted Alignment</p><p>Rephrasing the Web: A Recipe for Compute and Data-Efficient LanguageModeling</p></blockquote><span id="more"></span><h2 id="dall.e-3">DALL.E 3</h2><p>论文的标题明确指出了关键点"BetterCaptions"，说白了就是教你(叫你)去清洗数据。我们也许可以从这篇论文里，大致窥探到OpenAI世界第一的数据工程insights。</p><p><img src="../../files/images/recaption/authors.png"></p><p>作者指出，在DALL.E2以后，text2image得到了学界越来越多的关注，大家想要开发更好的模型结构、使用更大的参数量和训练量。另外，学界很多工作帮助指出DALL.E2中存在的问题：忽略text中的要求、和text中的语义不符，图片里出现诡异的文字等等。</p><p>怎么办呢？继续扩大模型规模，可以在一定程度上缓解这个问题。不过，作者在查看了原始数据后发现了根源：巨量的互联网图片-文本对数据里的图片和文本在大多数情况下并不对应，比如下图，虽然图片相对高质量，但是对应的alttext实际上都和图片没什么关系。实际上，上面提到的text2imagemodel中的问题，其根源在于数据集的质量。</p><h3 id="re-caption">re-caption</h3><p><img src="../../files/images/recaption/caption.png"></p><p>所以，作者有了直觉的想法：</p><ol type="1"><li>先训练一个caption model，可以输入图片数据输出高质量文字描述</li><li>把他们的整个数据集的所有文字描述全部重跑一遍，所以这个过程叫做(re-caption)</li><li>在新生成的数据集上训练text2image model。保证训练数据是高质量的</li></ol><p>这个思路没什么难的，实际上学界也早有了相关的思路，OpenAI只是把这个方法扩展了起来。作者分别标注了short-description和detailed-description两个小数据集，端到端的训练了一个captionmodel，然后把每条预训练数据集里面的图片都生成了SSC和DSC两种。</p><p>然而，作者敏锐的察觉到了上面这种方法里存在的问题：人类世界的caption虽然质量不一定高，但足够泛化。机器生成的caption虽然质量高，但他的多样性受制于captionmodel训练数据的多样性。比如说，如果caption model永远输出"axxx"开头的caption，那么如果用户运行时的输入不是a开头的，模型是不是就会爆炸了？所以作者希望数据集中的caption要尽可能接近人类生成的text</p><p>这个问题其实也好解决：把原始数据和recaption数据掺在一起训练！作者由此开展了一系列实验。另外，作者尝试了使用GPT-4V作为imagecaption，效果实际上也很不错</p><h3 id="blending-synthetic-and-ground-truth-captions">Blending syntheticand ground-truth captions</h3><p><img src="../../files/images/recaption/blend.png" style="zoom:33%;" ></p><p>作者经过一系列实验，发现基本上使用更多的syntheticdata，在in-domain的测试里效果会更好。作者由此训练了DALL.E3发现效果比DALL.E 2有了明显的提升。不过作者也同时指出了，DALL.E3仍然有很多问题，并且这些问题本质上是image captionmodel暂时学习不到的性质</p><ol type="1"><li>get不到caption里面的空间信息，比如谁在谁的前面/上面等等，作者发现captionmodel也往往说不对这些关系</li><li>图片里面的文字会丢失字母等等：作者认为这是textencoder是基于t5的。他的encoder是基于token的，模型需要学会把tokenizer里面的token(含多个char)映射到图片空间，这实际上非常难(比如说"图片要求写一个"play"，每个字母用不同的颜色，每个字母分别倾斜30度"。这个描述本身被tokenizer时"play"会是一个token，)。以后也许会训练基于char的model来解决</li><li>专业知识性的caption说不对，比如各种罕见的鸟的类型生成不对：作者发现这是因为captionmodel也说不出来，因为这需要更多、更高级的世界知识理解能力。作者认为需要更强的captionmodel(比如GPT-4V)</li></ol><h2 id="dall.e-3还有后手">DALL.E 3还有后手？</h2><p>通过上面的讲解，我们应该发现：DALL.E3的训练数据的文字部分，绝大多数(95%)都是recaption出来的。这对模型的影响有多大呢？</p><p><img src="../../files/images/recaption/image-with-better-caption.png"></p><p>作者列出来了dalle.3使用正常caption和详细caption下的实际表现，确实在promptfollowing能力上天差地别，使用更符合DALL.E3训练数据格式的prompt会让他的表现好很多。实际上，很多text2image目标用户过去一年里的很多学习，或者说网上找的教程都是在学习"如何写好的prompt"。这和"模型更好的遵从prompt"是一个双向奔赴的过程，但是只有OpenAI自己知道他们的闭源的recaption训练数据到底长什么样：我们其实很难针对性的写出符合DALL.E3预期的prompt。这也就是上面论文里提到的问题：真实人类需求和训练prompt的分布不一致，会导致模型部署时崩溃风险高。</p><p>怎么办？OpenAI帮我们想了一招：写prompt的也是OpenAI-model就好了！所以我们会发现，现在使用DALL.E3都是基于网页端的，我的需求会被GPT"re-caption"成真实的需求。比如这张图片：</p><ol type="1"><li><p>我写的要求：帮我生成一张图片，描述weak2stronggeneration里GPT2监督GPT4</p></li><li><p>GPT4实际生成的prompt：Imagine a futuristic, abstract scene whereGPT-2, represented as a wise, older robot with classic design elements,is mentoring GPT-4, depicted as a sleek, advanced robot withcutting-edge features. The setting is inside a vast, digital library,filled with glowing books and holographic data streams. GPT-2 is shownpointing towards a holographic display that illustrates complexalgorithms and data structures, while GPT-4 observes attentively, itssensors and circuits illuminated by the holographic light. Theatmosphere is one of collaboration and knowledge transfer, highlightingthe evolution of technology from one generation to the next.</p></li></ol><p><img src="../../files/images/recaption/dalle3-image.webp" style="zoom:33%;" ></p><p>通过这种方案，就不怕分布不一致了。OpenAI的这个设计实际上让我产生了两个联想：</p><ol type="1"><li>对于用户来说，prompt是什么本身比并不重要。prompt只是链接需求和成品的桥梁，用户关心的是自己想要的图片完成没有而不是prompt好不好。</li><li>ChatGPT目前做的事情更像是链接了 用户需求 -&gt;text-prompt。这本身不是最直观的方法，因为GPT和DALLE本身是没有交互的</li><li>我们可以把DALL.E 3部署时产生的数据定义为三元组 (多轮对话,text-prompt,target-image)。如果这个数据量scale起来，数据量达到二元组数据规模以后，是不是可以直接训练一个端到端的模型呢？他可以同时生成文字和图片，理解文字和图片。直接和你的对话去理解你的那种虽然抽象、但乐于通过对话表达出来的需求(比如要多少号多大的猫)，甚至通过多轮的图片生成去一点点猜测你的真实意图。</li></ol><blockquote><p>在DALL.E3论文里用GPT4生成caption的实验里展示了这种野心的一角，这可能才是text2image下沉市场(直接使用图像的用户、而不是基于图片去二次创作的画家们)的更广阔的未来</p></blockquote><h2 id="sora的recaption又玩出了什么">Sora的recaption又玩出了什么？</h2><p>Sora在技术报告里提到使用re-caption技术为视频创造文字描述。实际上这个领域和text2image完全不同：</p><ol type="1"><li>互联网能找到大量的图片、文本对，但很难找到大规模的 视频-文本对</li><li>图片和文本基本上是一一对应关系：有一个很精确详细的文本描述以后，其对应的图像基本也就只能长那个样子，对图片进行裁剪会导致清晰度丢失严重，并且丧失语义丰富性。但视频是可以"降采样的"，一个视频的任何片段还是视频，大概率还会有语义，可以做re-caption。</li><li>除了降采样，实际上还可以拼接。比如Sora里提到了模型有能力融合两个视频变成一个语义通顺的更长视频。这种方式，在理想情况，对于数据的利用效率会从O(n)变为O(n^n)。如果显卡足够，简直无法想象</li></ol><p>所以，sora里面很可能更大比例的使用re-caption技术来获得准确高质量的视频描述。这个描述不仅仅是空间尺度的，甚至还有时间尺度的，比如"在14秒时，屏幕上出现一只猫，在17秒时跳走了"。然而，如何高质量的降采样、拼接，使得视频仍然是保有语义的，OpenAI不知道做了多少数据工程。</p><blockquote><p>"保有语义"这个描述可能不太准确，实际上是：希望视频描述和真实用户在要求sora生成视频的需求尽可能接近。比如可能没人希望生成"两瓶可乐在打架"的视频，虽然确实是有语义的……可能，也有人？</p></blockquote><p>Sora技术报告对这些细节语焉不详，我们只能期待学界的开源工作在这方面做出更多更有趣的探索吧。</p><h2 id="recaptionmy-insights">recaption：my insights</h2><p>我认为，代码、图片、视频，(甚至我研究的tool-call-chain)都是和文字独立的其他模态的数据。他们有自己的模式，目前我们对于语言模态的模型训练效果最好，或者说，在语言模态找到的auto-regressive训练方法对数据的训练效率最高。</p><h3 id="recaption-is-only-training-to-align">recaption is only trainingto align?</h3><p>想要习得一个新模态的能力，我们需要的是pair数据：不放设想一下，我从出生开始，虽然到现在也才20多年，但是我经历的数据是(文字-图像-视频-声音)等等完全对齐的，脑子里甚至在同时产生无穷无尽的CoT来解释目前看到的这一切</p><p><img src="../../files/images/recaption/歪头.jpeg" ></p><p>所以对于模型来说，学习别的模态是一个比学习语言模态更困难的任务：image2text做的好、text2image不容易。所以大家训练dalle想要的做的事情可能<strong>并不是让模型学会图片模态，而是希望模型更好的把自己对于文字模态的知识映射到图片模态</strong>。对于这种类似于“align”的任务，最好地办法就是给出更准确的align数据。什么叫更好的align数据？没有歧义的数据，比如说一个文字描述，你能画出多少图片，他们之间的差异有多大：画一个人、画一个男人、画一个小男孩、画一个华裔小男孩、画马斯克、画埃隆马斯克……可以认为，更详细的文字描述是在减少歧义，同时通过传入更多的信息去<strong>定点地激活对应的文字模态的知识</strong></p><p>当然，OpenAI做recaption的思路其实有点违背了预训练的初衷：更多的数据、更少的干预、更少的归纳偏置。当然，这种初衷是希望模型获得通用的世界知识建模能力，如果我们认为模型(T5-encoder)本来就有世界知识，我们只是在"elicit"激发他的知识，那另说。</p><p>在很多情况下，我们希望模型去学习到很多通用的、语言没法表达的世界知识，比如几何题做辅助线、一堆水滴落在水面的样子等等，我们恰好发现。我们发现，这些知识正好都是目前的多模态模型很差的地方……也许，获得这些能力，是需要我们跳出"align"、re-caption的观点，去开发一种直接瞄准获取多模态通用知识的方案。这种方案，简单来说就是"没有输入，直接输出"，和GPT4的训练方式一样</p><p>我认为，在视频模态训练sora是可行的，原因在这里：对于图片生成，尤其是Diffusionmodel，一个模糊的图片变成清晰一点的图片，在我看来对于语义的依赖性没那么强。反而是视频模态，生成后面的视频，对于前面的依赖性很强，也就是说模型必须在auto-regressive训练中直接捕获到一些世界知识，考虑下面这个：</p><blockquote><p>你从一个鼻子模糊的图片里，参考前面的草图把鼻子画清楚。关键是你猜到这里是鼻子，你本来就会画鼻子，你从中学到的知识并不精确</p><p>看到两个球碰撞前的视频，预测出来碰撞后的样子。你需要理解速度、中心、碰撞点等概念，从中学到的知识表征了世界的更多属性。</p></blockquote><h3 id="recaption-is-not-training-on-synthetic-data">recaption is NOT"training on synthetic data"</h3><p>最近另外一个比较火的关键词是在合成数据上做训练，recaption算是"在合成数据上做训练"吗？我认为不是，因为recaption是在对已有的数据做改写成更符合要求的形式，本来就有数据。Sora中使用虚幻5渲染出来的视频做训练才是真正的"trainingon syntheticdata"。我认为在合成数据上做训练，本质是想要让模型grokking：希望模型通过更多的数据，最终捕获到了更高级的heuristics。</p><p>举几个例子：</p><ol type="1"><li><p>牛顿被苹果砸了，发现了万有引力。只需要一个公式，就能预测所有苹果落地的trace。所以这个高级的heuristics需要更少的计算量就能表征更多的数据</p></li><li><p>小明算1+2+3... +10需要点点算，高斯发现了这个算式可以变成(1+10)*20/2.很快算完了。但他俩结果是一样的</p></li><li><p>虚幻五是一个渲染视频的引擎，如果"虚幻5"本身是一个模型，用户只需要输入操作虚幻五的"actiontrace"，就能渲染出一个视频。从compression的角度看，虚幻五把视频世界知识压缩进了自己的参数里，可以把一个长视频压缩到"actiontrace"这么少的数据量里。Sora很傻，他一直看虚幻5生成的视频，为了对虚幻五进行拙劣的模仿，目前还学的不太对</p></li></ol><blockquote><p>目前的AI还没有牛顿这么聪明，可能被砸一亿个苹果才会突然顿悟，发现公式，这个事情在学界叫做grokking。从这里，我们发现的事实是：在我不知道万有引力时，我可能通过一堆低级方法生成苹果降落的视频，来一直砸牛顿，反正总有一天能砸对。</p><p>在更高级的模型看来，虚幻五也是"傻子"，需要下载好几十GB才能渲染视频，没准他用100M就生成比虚幻五好多了。更高级的模型没准觉得人类也是傻子，测出来卡西米尔效应是-1/12但解释不了，自己知道为什么，只是没办法用自然语言表达……因为人类的数学体系太烂了，整个"数学词表"也就这么大，还是离散的。</p><p>可悲的是，人类自己本来有机会grokking的，但是每个人只能活100年，柯洁学10000年围棋真的还下不过alpha-zero吗……类似的有教育意义的案例，可以参考漫士沉思录对于哈马努金推导的讲解<ahref="https://www.bilibili.com/video/BV1si4y1p75k">全体自然数之和等于-1/12？真相远没有那么简单！</a></p></blockquote><p>所以，1)找到高级heuristics很困难。2)高级和低级heurisitic生成的数据都是正确的。3)越多的原始数据，越多发现高级herusitics的可能。从这三个观点出发，就产生了"trainingon syntheticdata"的想法：盼着模型去grokking。我十分同意这个观点，也相信这才是AGI最可行的路线。</p><p>不过话说回来，不管怎么看，这和recaption都不是一回事，在根本目的上就不一样。</p><h2 id="recaption-is-all-you-need">recaption is all you need?</h2><p>recaption本身有没有在学界的其他领域使用呢？我认为这是一个范式：A是一个比B更难的任务，A-&gt;B的模型效果不好，就反过来训一个 B -&gt; A的模型，为(A-B)pair提供更多的数据。这个思路很简单、很scalable，并且实际上很多工作都在用，我这里举一些例子：</p><ol type="1"><li>WaveCoder：根据query生成code很困难。那么我先找一堆code，对着他生成、改写query，再训练code模型。</li><li>CoachLM：SFT中根据query生成response很困难：那我找到已有的query-response数据对，把query改成更好的、更没有歧义的格式，再训练模型去生成response</li><li>ReformattedAlignment：推理领域，根据query生成CoT很困难。我把原始的数据格式改写一下，让query里告诉模型该怎么写CoT，再生成CoT。由此finetune模型，效果会更好吗？</li></ol><p>我相信，这种范式会在未来变得越来越普遍，甚至扩展到预训练场景(比如CMU今年出的Rephrasingthe Web)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近Sora巨火，仿佛开启了AIGC的新时代。Jason Wei表示：&quot;Sora is the
GPT-2 moment&quot; for video
generation。我在sora发布的大约第5个小时读了technical
report，里面最打动我的其实是没提什么细节的recaption技术。让我回想想起了之前读DALL.E
3论文时的愉快体验。&lt;/p&gt;
&lt;p&gt;所以今天来分享一下DALL.E
3论文里的recaption细节，并讨论几个问题和我的看法：1)OpenAI教你为什么要&quot;先查看原始数据，再做创新&quot;
2)Recaption和大家一直在聊的&quot;training on synthetic data&quot;是一回事吗?
3)recaption技术是否已经在(或者即将在)被其他领域使用？&lt;/p&gt;
&lt;p&gt;另外，我总结了一下上篇笔记阅读量大的关键：语言表达要浅显易懂些，所以这篇笔记我可以声明一下：&lt;strong&gt;没学过AI也能看懂&lt;/strong&gt;(我在博客里加了这个标签&quot;from
scratch&quot;，所有我认为不懂AI或者只知道一点点的人也能看懂的博客都会加上这个标签)&lt;/p&gt;
&lt;p&gt;参考文献：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;https://openai.com/sora&lt;/p&gt;
&lt;p&gt;Improving Image Generation with Better Captions&lt;/p&gt;
&lt;p&gt;Automatic Instruction Optimization for Open-source LLM Instruction
Tuning&lt;/p&gt;
&lt;p&gt;WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with
Refined Data Generation&lt;/p&gt;
&lt;p&gt;Reformatted Alignment&lt;/p&gt;
&lt;p&gt;Rephrasing the Web: A Recipe for Compute and Data-Efficient Language
Modeling&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多模态" scheme="https://www.yynnyy.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="Computer Vision" scheme="https://www.yynnyy.cn/tags/Computer-Vision/"/>
    
    <category term="from scratch" scheme="https://www.yynnyy.cn/tags/from-scratch/"/>
    
  </entry>
  
  <entry>
    <title>2024-02-29总结：研一下开始了</title>
    <link href="https://www.yynnyy.cn/e9916de0"/>
    <id>https://www.yynnyy.cn/e9916de0</id>
    <published>2024-02-29T01:02:36.000Z</published>
    <updated>2024-08-09T09:26:46.966Z</updated>
    
    <content type="html"><![CDATA[<p>今天是2月29日，我迎来了研究生的第二个学期。上次2月29日已经是2020年，而下次2月29日要到2028年了。人生有多少4年，再加好久没有更新，遂写一写最近的生活吧。</p><p>其实我写总结这个track，还是因为最开始看了谭院士的博客 <ahref="https://twd2.me">WandaiBlog</a>：谭院士总是时间驱动，每天写一个sentence-level的总结，陆陆续续竟然坚持了十几年。时间是有惯性的，有点类似于顺着一个人的微信刷pyq，不会到了某个位置突然被卡掉，看下来有种震撼人心的感觉。所以我也想是不是记录一下自己的生活。</p><p>我当时选了另一种形式：事件感想驱动，更大的interval,在corpus-level做记录，所以给自己起名字叫做"随缘"。现在想想可能并不适合，我和谭院士的记录方式也许应该倒一倒。我的生活当然没有谭院士丰富，用instructiontuning的话说：每天翻来覆去总是从一些task set里先sample task <spanclass="math inline">\(t \in \mathcal{T}\)</span>，再sample <spanclass="math inline">\(x \in \mathcal{X}_t\)</span>，最后预测 <spanclass="math inline">\(y =me(x)\)</span>。做得多了，熟能生巧，常用的几个task的能力越来越高了，但一直没什么机会探索更大更diverse的instruction空间。</p><p>不过近期确实有所不同，我深感在过去一个月里，尝试的新事物堪比过去一两年。</p><span id="more"></span><p>首先，我这学期第一次当助教了，大家可能以为是一个课的助教，但实际上是三个课的助教……这就是所谓的"半年不开张，开张吃三年"吗。助教的职责主要是帮助同学们留作业判作业，以及在群里回答同学们的疑问。最开始有一些"好为人师"的新鲜感，但很快也就过去了，剩下大致有一些"重构课程体系，提供更好的教学方式，我辈义不容辞"的责任使命感？</p><p>这学期正好轮到NLP课作业体系重构，我同时还是学堂在线Mooc的NLP课的助教，他们两个课的作业体系本来是一致的，我需要把前者重构一下。因为是研究生课，大家都是做研究的，所以我进行了一个相对激进的尝试：我尝试把作业里面的"回答问题、完成任务"式的部分去掉了，变成了鼓励选课同学们对作业框架下的知识产生一些自己的问题并进行探索。站在我的角度，我觉得要求同学们完成某个任务是一个挺反直觉的东西，大家都是AGI，应该有能力自己组织自己的学习路径，发现学习过程对自己帮助最大的东西。等过几天作业发布了，我可以也许会把重构过的内容和我的一些insight以科普blog的形式发出来。不知道大家对我的设计形式会有怎么样的评价。我猜测研究生选课大致有两种用户画像：</p><ol type="1"><li>想混混了事，对NLP一点兴趣也没有，得多少分无所谓，破事多我就退课，别耽误我科研/实习/旅游……</li><li>我真的想学习NLP的知识，或者想通过这个课程认识老师，去跟着老师开展相关research。不关心最后得了多少分，当然分数高更好……</li></ol><p>我觉得不管哪种，从结果出发，应该体验都还不错：我判作业给分应该是非常宽松的，实际上我希望的形式应该是大家都能把被动式的完成作业的时间换成主动探索一些自己发现的小现象，而结果上都能拿满分。</p><p>除了当助教给同学们打分，我最近还第一次尝试了审稿。说来有点可惜，人生第一次审的两篇论文，一篇给了strong-reject,一篇给了weak-reject……可能我其实是批判型人格吧。当时看到自己论文的review时有多生气，未来几天别人看我就有多生气。我不太喜欢审稿，因为需要我来总结论文里的优点和缺点。我觉得这个事情有点反直觉：大家发论文是想要对学界做贡献，所以别人会帮你去芜存真，只会关注你的主要贡献点。我认为评价一个论文应该看他最优的优点max(strength)到底有多好，而不应该是strength-weakness、甚至像”木桶效应“一样揪着最大的窟窿不放……可惜我不是管事的，只能给别人打一个reject了</p><p>年初的时候奥特曼说2030年实现AGI，我其实心里挺焦虑的：我在研究AI，如果2030年时AI被解决了，那如果我在这6年里没有贡献关键技术，岂不是整个2020-2030这10年的努力也被抹杀了吗？不过，类比一下互联网或者电器技术的发展，其实在学术上解决一个问题，到实际帮助所有用户解决这个问题，中间可能需要10年、甚至几十年的时间。在这段merge的时间里，大家实际上在做一个技术和服务相互妥协的过程：什么服务对用户很重要，但技术成本上困难所以要砍掉；什么技术很先进很scalable，但会拖累短期里的用户服务质量，就需要砍掉……妥协的结果是平衡，也大概率会是目前的基础框架下的最优形态。整个这个过程在我看来挺像是”机器学习的“，当然这里面的"forward-explorationandbackward-gradient"不是某个参数的变大变小，而是某个、某几个公司的成立和消亡。在这个情境下，我们自己也是"世界模型"的其中一个参数……但谁能成为Meta发现的"MassiveActivation"呢？</p><p>因此，除了技术本身以外，我最近其实也开始思考技术的落地和产品。比如，我发现了一个叫做”增量“的概念，一个产品或者说系统，最重要的是比起同类产品带给用户的增量价值，而不是这个系统本身的技术先进性。一个技术很先进的东西，可能对用户带来的服务并不好；反之，一个让用户感知很好的产品，其背后的技术不一定很先进。我作为科研或者学术背景出发，常常会受到"选论文"式的归纳偏置，关注技术创新性，而忽略了他的可行性以及增量价值。随着对于产品的调研和感悟越来越多，也许以后，除了阅读笔记以外，我还可以分享一些对于产品的感悟笔记也说不准……</p><blockquote><p>夫人之相与，俯仰一世。或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至；及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀，况修短随化，终期于尽！古人云：“死生亦大矣。”岂不痛哉！</p></blockquote><p>简单来说：下一个2月29日就是2028年了，而下一次在2月29日恰逢"疯狂星期四"更是28年以后了：不知道下个2月29日的我在干什么呢？还会在维护这个blog吗？28年以后的我还在吃疯狂星期四吗？希望那时的我，也能像今天一样喜欢探索学习自己不知道的东西，而不是像大多数人一样，随着年龄增长开始倾向于沉浸在舒适圈里变得over-confidence</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天是2月29日，我迎来了研究生的第二个学期。上次2月29日已经是2020年，而下次2月29日要到2028年了。人生有多少4年，再加好久没有更新，遂写一写最近的生活吧。&lt;/p&gt;
&lt;p&gt;其实我写总结这个track，还是因为最开始看了谭院士的博客 &lt;a
href=&quot;https://twd2.me&quot;&gt;Wandai
Blog&lt;/a&gt;：谭院士总是时间驱动，每天写一个sentence-level的总结，陆陆续续竟然坚持了十几年。时间是有惯性的，有点类似于顺着一个人的微信刷pyq，不会到了某个位置突然被卡掉，看下来有种震撼人心的感觉。所以我也想是不是记录一下自己的生活。&lt;/p&gt;
&lt;p&gt;我当时选了另一种形式：事件感想驱动，更大的interval,
在corpus-level做记录，所以给自己起名字叫做&quot;随缘&quot;。现在想想可能并不适合，我和谭院士的记录方式也许应该倒一倒。我的生活当然没有谭院士丰富，用instruction
tuning的话说：每天翻来覆去总是从一些task set里先sample task &lt;span
class=&quot;math inline&quot;&gt;&#92;(t &#92;in &#92;mathcal{T}&#92;)&lt;/span&gt;，再sample &lt;span
class=&quot;math inline&quot;&gt;&#92;(x &#92;in &#92;mathcal{X}_t&#92;)&lt;/span&gt;，最后预测 &lt;span
class=&quot;math inline&quot;&gt;&#92;(y =
me(x)&#92;)&lt;/span&gt;。做得多了，熟能生巧，常用的几个task的能力越来越高了，但一直没什么机会探索更大更diverse的instruction空间。&lt;/p&gt;
&lt;p&gt;不过近期确实有所不同，我深感在过去一个月里，尝试的新事物堪比过去一两年。&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="课程总结" scheme="https://www.yynnyy.cn/tags/%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    
    <category term="随笔" scheme="https://www.yynnyy.cn/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="日记" scheme="https://www.yynnyy.cn/tags/%E6%97%A5%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Self-Consistency之我见，兼More Agents is All You Need</title>
    <link href="https://www.yynnyy.cn/7cd10148"/>
    <id>https://www.yynnyy.cn/7cd10148</id>
    <published>2024-02-10T02:21:00.000Z</published>
    <updated>2024-08-09T09:26:46.970Z</updated>
    
    <content type="html"><![CDATA[<p>好久不更新了，看到之前大约都是15天更新一篇笔记，最近不知道咋回事竟然一个多月没更新，正好这两天刷到了"MoreAgents is All YouNeed"，就来讲讲“时间换效果”的鼻祖——self-consistency。如果让模型sample多次，然后做major-voting，效果会更好吗？</p><p>参考文献：</p><blockquote><p>Self-Consistency Improves Chain of Thought Reasoning In LanguageModels</p><p>Escape Sky-High Cost: Early-Stopping Self-Consistency for Multi-StepReasoning</p><p>Universal Self-Consistency for Large Language Model Generation</p><p>More Agents is All You Need</p><p>Unlock Predictable Scaling from Emergent Abilities</p></blockquote><span id="more"></span><h2 id="self-consistency">Self-Consistency</h2><p>什么是self-consistency？要从CoT算法说起，之前CoT的思路是：如果让模型在输出答案之前，先说一些rationale，最终就能得出相对更准确度的结果。用类似于"Let'sthink step by step"的prompt，就能激发模型进行对应的CoT输出。</p><p>而self-consistency则更进一步，如果用更多的CoT放到一起，能不能进一步提高效果？具体来说，模型按照某种随机解码算法，比如p-sample进行多次解码，最后根据多个结果做去重，最后给出众数作为结果。一个经典的例子就是数学计算题</p><p><img src="../../files/images/self-consistency/sc.png"></p><p>神奇的是，随着sample次数N的增长，最终的准确率就会逐渐提高，比如下面这个图对比了下面几个东西</p><table><tr><td><img src="../../files/images/self-consistency/sc-perf.png" align=left style="zoom:50%;" ></td><td><img src="../../files/images/self-consistency/sc-scale.png" align=left style="zoom:45%;" ></td></tr><tr><td>随着sample次数的提高，SC算法的效果逐渐提升</td><td>在一定的范围内，越强的模型受到sc的增量越多</td></tr></table><ul><li>橙色线：greedy-decoding的结果，和N无关</li><li>绿色的线：voting-with-ppl：sampleN次，提交对应的ppl最小的，即token-level信心值最高的</li><li>蓝色的线：self-consistency，提交结果的众数</li></ul><p>实际上，self-consistency是一个时间检验的算法，对于各种场景都能几乎稳定地获得提升。早在2021年OpenAI的GSM8K论文，就报告了类似的结果：他们先使用打分器选出最高的topk个样本，再在他们之中选择Major-voting的结果(下右图)，效果比单纯对打分器选择top1的效果好很多</p><p><img src="../../files/images/self-consistency/gsm8k.png"></p><h2id="self-consistency之我见instance-level-calibration与集成学习">Self-Consistency之我见：Instance-LevelCalibration与集成学习</h2><p>为什么self-consistency效果会这么好？我下面来聊聊我的看法。</p><p>首先，self-consistency论文中报告了一个有趣的结果：他们发现结果一致性越高的query，真实的准确率也越高</p><table><tr><td><img src="../../files/images/self-consistency/sc-calibration.png" align=left style="zoom:50%;" ></td><td><img src="../../files/images/self-consistency/gpt4-calibration.png" align=left style="zoom:40%;" ></td></tr><tr><td>sc中，结果一致性越高，真实准确率就越高</td><td>GPT4 report：选项一致性越高，结果准确率就越高</td></tr></table><p>这其实和后来大家发现的calibration现象很像：对于4选1 multi-choiceQA场景(比如MMLU)，模型对于4个选项token里信心最高的选项的prob越大，对应题目的最终准确率就越高</p><blockquote><p>实际上，calibration的这种方式在"selectiveprediction"领域又被称为MaxProb算法，calibration领域大概是"重新发现"了他……</p></blockquote><p>这就促使我去思考，self-consistency既然在instance-level选择概率最大的，并根据可以报告instance-levelMaxProb。是不是就是在模拟instance-level的calibration呢？</p><p>instance-level的一致性该怎么表示？如果从刚才maxProb的角度思考的话，直观的想法是使用PPL最低的样本。其实，这个方法也被self-consistency原始论文报告了(绿线)。可以发现，这个算法的效果虽然比greedy-search好，但并不是scalable的，甚至在n很大以后效果会更差。这个现象和GSM8K中best-of-ORM-verifier是一致的，都是随着N的增加先增后减。这是因为ppl或者verifier都是不够鲁棒的。举个例子，对于math场景，ppl的大小不仅受到模型对于expression的信心值，还受到expression表达方式的影响，考虑下面两个句子：</p><blockquote><p>3*5=15</p><p>3乘以5的结果是15</p></blockquote><p>对于模型来说，ppl是生成每个token的平均概率。这两个句子虽然表达相同的意思，但是表达形式不同，"乘以"、“的”、“结果”这几个token的信心值都会很高，所以导致两个句子的ppl差很远。然而，其实模型对于3*5=15这个表达式的信心值是固定的。这也就会导致，best-of-ppl的算法在N很多以后，模型可能偶然发现一些很简单的表达形式，ppl很低。但这个这只是模型对于“表达方式”的信心值。类似的，best-of-verifier也会有类似的情况，这就是bad-heuristics，或者叫adversarial-solutions的问题。除此之外，还有rationale方式的问题，一个结果可以用不同的思考路径所表达，就像是一个题目会有多种方法去做。</p><p>相比之下我们发现，Multi-choiceQA场景没有表达形式的bias，因为四个选项都是使用一个token表达的。因此，直接使用选项token的maxProb就可以做出相对很准确的准确率估计。</p><p>major-voting是一个道理，如果我们sampleN次对结果划分出不同等价类的话，可以发现，我们实际上得到了不同结果的频度。当N变得很大时，这个趋向于了不同结果的概率分布。从这个instance-level概率分布取最大概率，就是instance-level的maxProb。他不会受到表达形式的影响，因为大家都是按照同样的算法采样，每个算法都有可能以困难的形式被表达，也有可能用简单的方式表达。</p><blockquote><p>从直觉来看，self-consistency的效果提升其实也很容易被理解。我们可以类似于shengding学长的论文去定义出pass-until：sampleN次，其中答案正确的比例。</p><p>假如pass-until =60%，那么显然self-consistency一定会把正确答案投票出来。但如果只sample一次，就有40%的概率做错。对偶的情况是，pass-until!=result-maxProb时，self-consistency一定做不对。但是如果只sample一次，有可能反而能找到正确答案(虽然这个概率估计很低)。</p></blockquote><p>self-consistency的提升点，实际上是在两种情况的博弈。想要观察self-consistency的提升点，也许需要列出来一个数据集中所有样本的pass-until的直方图来观察。实际上，这个直方图恰恰就是上面展示过的"带权"版本的sc-consistency图。另外，我认为实际上应该使用self-consistency的结果（而不是greedy-search）作为模型对于一个数据集的performance。</p><p>另外，self-consistency主要是考虑用同样的随机解码算法做拟合，能不能考虑算法本身的异构呢？</p><ul><li>用不同的模型结果做综合</li><li>不用的解码算法的结果做综合</li><li>不同的system-prompt的结果做综合</li><li>甚至是multi-agentsystem中不同agent通过类似debate的形式做沟通博弈...</li><li>google还做过综合不同的reward model来进行RLHF训练</li></ul><p>这些思路也自古有之，叫做集成学习。出自一个朴素的直觉：条条大路通罗马，不同的方法都相对认可的东西，实际正确率也更高。所以说，self-consistency集成不同的思考链，其实是集成学习的一种in-context版本的特例，自然也可以集成更diverse的自由度。</p><p>最后，self-consistency根据结果的等价类做划分，得出来了instance-level的calibration，这个事情能不能再step-level做呢？虽然没有人formalize这个问题，但我在一些独立的工作中看到了类似的解决方案：</p><blockquote><ol type="1"><li>shunyu yao的ToT：其中的BFSbaseline在step-level进行self-vote。对偶的，可以设计self-consistency。</li><li>所有的PRM相关论文：let's veriffy step-by step 、Step-AwareVerifier、math-shepherd、Discriminator-Guided CoT、outcome-supervisedverfier等等，都可以在step-level设计look-ahead搜索算法</li><li>More Agents is All You Need中的hierarchical sample-and-voting</li></ol></blockquote><p>再推广一步，到最细力度的token-level，一致性是天生就存在的(logits)。那么就会有beam-search等算法去寻找ppl最低的结果</p><h2 id="self-consistency的应用与改进">Self-Consistency的应用与改进</h2><p>self-consistency有两个巨大的问题和一个小问题。改进方法大致也从这些问题出发</p><h3id="大问题1需要某场景可以划分等价类">大问题1：需要某场景可以划分等价类</h3><p>对于数学场景，可以认为结果相同就是一个等价类。但对于很多free-form的场景，比如数学证明题，比如写代码，比如机器翻译该怎么办？写代码任务，类似于googleAlphaCode，会根据代码中每个测例的输出，按照对拍结果划分等价类。这依赖于一个外置的执行器。</p><p>既然ToT可以做self-vote，那self-consistency能不能行呢？最近，dennyZhou做了一篇工作，可以让模型根据多个样本都放在prompt里，然后让模型自己说一个最一致的结果出来。那么，代价是什么呢？需要模型可以同时存下所有的结果在context里！</p><p>另外，刚才提到了step-level的self-consistency，这个更复杂，依赖于对step划分等价类。首先，不是所有任务都能像ReACT场景一样划分成天生多步的。其次，很多任务的多个执行链其实不是按照树的格式组织的，是按照图的形式组织的。所以，更细粒度的step-search依赖于更细粒度的等价类划分</p><h3 id="大问题2需要更多的计算资源">大问题2：需要更多的计算资源</h3><p>self-consistency动辄就需要一个query sampleN=40次。对于GPT4来说，在MATH数据集sample40次，需要大概2000美元。这显然不可接受，最近有很多工作试图减少self-consistency的效果。</p><p>从直觉上来看，如果只sample10次，结果都是一样的。理论上这个很可能是就是众数结果，最极端的情况是出现类似斯诺克中的"超分"：即使后面所有的结果都是目前第二大的，也不能超过最大的。从统计学上对这个直觉做建模，就是狄利克雷分布。很多工作会涉及一些基于熵的算法去拟合self-consistency的一致性要求。</p><h3id="小问题需要一个场景的天然假阳率不大">小问题：需要一个场景的天然假阳率不大</h3><p>最后这个，本质上不是一个问题，而是一个现象。举个例子，对于判断正误的问题，答案只可能是对或者不对，就两种可能。随机猜都有50%准确率，这时候有非常多的假阳情况。self-consistency拟合出来的一致性，就会受到影响：假阳的样本往往都是eazy-heuristic，一致性都挺高的。</p><p>所以，应用self-consistency的时候，可能还需挑场景，最好选择类似计算题的场景：结果正确，那么基本上过程也是正确的。</p><h2 id="我的思考">我的思考</h2><p>之前的论文阅读笔记大多是照着论文的展开顺序讲。最近读的多了以后，渐渐感觉很多东西和某些别的领域有更多的相关性，把别的领域的直觉拿来解释，可能相对更清晰。这也是我现在读论文的一个经验。比起理解论文的素有方法，更重要的是理解论文背后的直觉：</p><ul><li>一方面，理解了直觉理论上可以把论文的方法重新推导出来</li><li>另一方面，论文的方法不一定都是管用的，有一些“为了创新而创新”的工作量证明的部分。从直觉出发，更容易剔除掉这些"noise"……</li></ul><blockquote><p>对于这个问题，有人说：AI的论文没什么内容，创新都很简单。我认为恰恰相反，理解起来简单是因为讲得好。如果一个论文讲的神头鬼脸、玄之又玄，那大概率是作者自己都没搞清楚自己论文的“firstprinciple”——背后的直觉到底是什么</p></blockquote><p>最后，self-consistency是一个时间检验的方法，对于所有模型、对于所有场景都能有提升。之前jasonwei发了一个twitter，认为CoT和self-consistency的效果提升来源于数据和问题的固有熵，对此我产生了一些不同的见解，贴出来我的看法：</p><p>I don't entirely agree with this viewpoint, here are some of myinsights:</p><ol type="1"><li>CoT aims to use a nearly identical, substantial amount ofcomputation for both difficult and simple problems to ensureperformance. Thus, recent works like GSM8K-zero (<ahref="https://t.co/JJO0tQkHSP">https://arxiv.org/abs/2401.11467</a>)have found that CoT can waste a lot of computational resources onsimple(trivial) problems.</li><li>The information density in the corpus(the varying difficulty ofpredicting each token) is an intrinsic property of datasets (e.g.,predicting the answer-token on MMLU is harder than predictingquery-tokens), which is why speculative decoding succeeds in somesettings and not in others.</li><li>Instead of fitting corpora with uniform information density, it'sbetter to retain the dataset's intrinsic properties and instead adaptthe model computation to fit the difficulty of the corpus, i.e.,adaptive computation.</li><li>On the instance-level/step-level, this is what "flow-engineering" isabout: Allowing for dynamic decision-making on the complexity of CoTbased on the task.</li><li>At the token-level, there are passive solutions like pause-token (<ahref="https://t.co/dYnr2CEAqI">https://arxiv.org/abs/2310.02226</a>)that use more forward passes on some manually selected tokens. There arealso active solutions like early-exit (<ahref="https://t.co/5TNCd76Zcy">https://arxiv.org/abs/2402.00518</a>),which allow the model to freely decide the computational effort for eachtoken. And, though speculative decoding has a different goal, itacutually achieves the purpose of adaptive computation.</li><li>Besides point 5, I also think Mixture of Experts (MoE) technologyhas achieved part of this goal: Firstly, early works in MoE coulddynamically decide the activated number of experts and implementheterogeneity of experts (different experts have different size) toachieve the purpose of adaptive computation. Secondly, OpenMoE (<ahref="https://t.co/li4vngcGYr">https://github.com/XueFuzhao/OpenMoE</a>)discovered that even with a fixed number and all the same size ofexperts, a phenomenon occurs where "one expert is always responsible forsome tokens". And we all that token-id represents part of the predictivedifficulty (e.g., predicting punctuation is often easier than predictingnumbers)...</li></ol><p>I believe that future models can freely and acitvely decide oncomputational complexity based on the task's information density, justlike us: Think Before You Speak.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;好久不更新了，看到之前大约都是15天更新一篇笔记，最近不知道咋回事竟然一个多月没更新，正好这两天刷到了&quot;More
Agents is All You
Need&quot;，就来讲讲“时间换效果”的鼻祖——self-consistency。如果让模型sample多次，然后做major-voting，效果会更好吗？&lt;/p&gt;
&lt;p&gt;参考文献：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Self-Consistency Improves Chain of Thought Reasoning In Language
Models&lt;/p&gt;
&lt;p&gt;Escape Sky-High Cost: Early-Stopping Self-Consistency for Multi-Step
Reasoning&lt;/p&gt;
&lt;p&gt;Universal Self-Consistency for Large Language Model Generation&lt;/p&gt;
&lt;p&gt;More Agents is All You Need&lt;/p&gt;
&lt;p&gt;Unlock Predictable Scaling from Emergent Abilities&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="Reasoning" scheme="https://www.yynnyy.cn/tags/Reasoning/"/>
    
  </entry>
  
  <entry>
    <title>arxiv-insights</title>
    <link href="https://www.yynnyy.cn/f4243ee6"/>
    <id>https://www.yynnyy.cn/f4243ee6</id>
    <published>2024-01-15T02:58:32.000Z</published>
    <updated>2024-10-08T02:54:39.076Z</updated>
    
    <content type="html"><![CDATA[<p>压缩带来智能，5% 的论文决定学术界 95% 的成果！每天从 Arxiv论文中总结分享最重要、最有趣的最多三篇论文。</p><p>Compression brings intelligence, 5% of papers determine 95% of AItechnologies! Share the most important papers from Arxiv, every day, upto three!</p><script type="text/javascript">    var insight_now_id = 0;    var insight_max_id = 13;    function tips_insight(num){        document.getElementById("insights_"+insight_now_id).hidden = "hidden";        insight_now_id -= num;        if (insight_now_id > insight_max_id) {insight_now_id = insight_max_id;}        if (insight_now_id < 0) {insight_now_id = 0;}        document.getElementById("insights_" + insight_now_id).hidden = "";    }</script><table id="insights_0" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td bgcolor="#00E400"><a href=../50bb6226>1(172-&gt;4 papers)</a></td><td bgcolor="#00F700"><a href=../bb8cd925>2(74-&gt;1 papers)</a></td><td bgcolor="#00E400"><a href=../544eb21b>3(103-&gt;3 papers)</a></td><td bgcolor="#00D300"><a href=../b692a962>4(128-&gt;6 papers)</a></td><td>5</td></tr><tr><td>6</td><td bgcolor="#00E400"><a href=../5da51261>7(121-&gt;2 papers)</a></td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td></tr><tr><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td></tr><tr><td>20</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td><td>26</td></tr><tr><td>27</td><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="insights_1" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td>1</td><td bgcolor="#00F800"><a href=../a11632bb>2(30-&gt;1 papers)</a></td><td>3</td><td bgcolor="#00DB00"><a href=../ac0842fc>4(169-&gt;4 papers)</a></td><td bgcolor="#00E900"><a href=../43ca29c2>5(52-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../a8fd92c1>6(46-&gt;1 papers)</a></td><td>7</td></tr><tr><td>8</td><td bgcolor="#00FE00"><a href=../59f6c94c>9(33-&gt;0 papers)</a></td><td bgcolor="#00E300"><a href=../646d3d46>10(66-&gt;3 papers)</a></td><td bgcolor="#00F700"><a href=../8baf5678>11(49-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../6098ed7b>12(38-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../8f5a8645>13(32-&gt;1 papers)</a></td><td>14</td></tr><tr><td>15</td><td bgcolor="#00FE00"><a href=../69734d01>16(39-&gt;0 papers)</a></td><td bgcolor="#00EB00"><a href=../86b1263f>17(101-&gt;2 papers)</a></td><td bgcolor="#00F300"><a href=../77ba7db2>18(78-&gt;1 papers)</a></td><td bgcolor="#00E700"><a href=../9878168c>19(53-&gt;4 papers)</a></td><td bgcolor="#00E900"><a href=../fd8f5b47>20(55-&gt;3 papers)</a></td><td>21</td></tr><tr><td>22</td><td bgcolor="#00F900"><a href=../16b8e044>23(56-&gt;1 papers)</a></td><td bgcolor="#00DA00"><a href=../f464fb3d>24(187-&gt;4 papers)</a></td><td bgcolor="#00F600"><a href=../1ba69003>25(79-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../f0912b00>26(73-&gt;1 papers)</a></td><td bgcolor="#00E900"><a href=../1f53403e>27(84-&gt;3 papers)</a></td><td>28</td></tr><tr><td>29</td><td bgcolor="#00EE00"><a href=../3c018487>30(53-&gt;2 papers)</a></td></tr></table><table id="insights_2" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年八月August</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr><tr><td>11</td><td bgcolor="#00F500"><a href=../bd0e34fe>12(73-&gt;1 papers)</a></td><td bgcolor="#00F200"><a href=../52cc5fc0>13(68-&gt;2 papers)</a></td><td bgcolor="#00F500"><a href=../b01044b9>14(53-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../5fd22f87>15(41-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../b4e59484>16(36-&gt;1 papers)</a></td><td>17</td></tr><tr><td>18</td><td bgcolor="#00F900"><a href=../45eecf09>19(45-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../201982c2>20(102-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../cfdbe9fc>21(67-&gt;2 papers)</a></td><td bgcolor="#00E700"><a href=../24ec52ff>22(58-&gt;2 papers)</a></td><td bgcolor="#00F900"><a href=../cb2e39c1>23(78-&gt;1 papers)</a></td><td>24</td></tr><tr><td>25</td><td bgcolor="#00F700"><a href=../2d07f285>26(36-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../c2c599bb>27(76-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../33cec236>28(50-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../dc0ca908>29(53-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../e1975d02>30(40-&gt;2 papers)</a></td><td>31</td></tr><tr></tr></table><table id="insights_3" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年七月July</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00F600"><a href=../b0c78508>1(68-&gt;1 papers)</a></td><td bgcolor="#00E100"><a href=../5bf03e0b>2(155-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../b4325535>3(96-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../56ee4e4c>4(87-&gt;2 papers)</a></td><td>5</td><td>6</td></tr><tr><td>7</td><td bgcolor="#00ED00"><a href=../4cd2aec2>8(121-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../a310c5fc>9(106-&gt;2 papers)</a></td><td bgcolor="#00F100"><a href=../9e8b31f6>10(69-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../71495ac8>11(45-&gt;2 papers)</a></td><td bgcolor="#00EE00"><a href=../9a7ee1cb>12(46-&gt;2 papers)</a></td><td>13</td></tr><tr><td>14</td><td bgcolor="#00F900"><a href=../78a2fab2>15(49-&gt;1 papers)</a></td><td bgcolor="#00EA00"><a href=../939541b1>16(112-&gt;2 papers)</a></td><td bgcolor="#00F500"><a href=../7c572a8f>17(97-&gt;1 papers)</a></td><td>18</td><td bgcolor="#00E400"><a href=../629e1a3c>19(129-&gt;3 papers)</a></td><td>20</td></tr><tr><td>21</td><td bgcolor="#00F500"><a href=../39c87ca>22(50-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../ec5eecf4>23(105-&gt;3 papers)</a></td><td bgcolor="#00F400"><a href=../e82f78d>24(54-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../e1409cb3>25(49-&gt;1 papers)</a></td><td bgcolor="#00EF00"><a href=../a7727b0>26(60-&gt;2 papers)</a></td><td>27</td></tr><tr><td>28</td><td>29</td><td>30</td><td>31</td></tr></table><table id="insights_4" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年六月June</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>2</td><td bgcolor="#00F500"><a href=../69a48cb0>3(77-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../8b7897c9>4(153-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../64bafcf7>5(93-&gt;2 papers)</a></td><td bgcolor="#00F900"><a href=../8f8d47f4>6(90-&gt;1 papers)</a></td><td bgcolor="#00E900"><a href=../604f2cca>7(84-&gt;3 papers)</a></td><td>8</td></tr><tr><td>9</td><td bgcolor="#00F500"><a href=../431de873>10(80-&gt;1 papers)</a></td><td bgcolor="#00E400"><a href=../acdf834d>11(150-&gt;3 papers)</a></td><td bgcolor="#00E800"><a href=../47e8384e>12(148-&gt;3 papers)</a></td><td bgcolor="#00E700"><a href=../a82a5370>13(83-&gt;3 papers)</a></td><td bgcolor="#00F800"><a href=../4af64809>14(88-&gt;1 papers)</a></td><td>15</td></tr><tr><td>16</td><td bgcolor="#00EB00"><a href=../a1c1f30a>17(72-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../50caa887>18(310-&gt;4 papers)</a></td><td bgcolor="#00E500"><a href=../bf08c3b9>19(161-&gt;3 papers)</a></td><td>20</td><td bgcolor="#00E600"><a href=../353de54c>21(223-&gt;3 papers)</a></td><td>22</td></tr><tr><td>23</td><td bgcolor="#00F700"><a href=../d3142e08>24(100-&gt;1 papers)</a></td><td bgcolor="#00ED00"><a href=../3cd64536>25(188-&gt;2 papers)</a></td><td bgcolor="#00F000"><a href=../d7e1fe35>26(101-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../3823950b>27(101-&gt;3 papers)</a></td><td bgcolor="#00EC00"><a href=../c928ce86>28(78-&gt;2 papers)</a></td><td>29</td></tr><tr><td>30</td></tr></table><table id="insights_5" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年五月May</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td bgcolor="#00EB00"><a href=../d09b3043>1(64-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../3bac8b40>2(71-&gt;2 papers)</a></td><td bgcolor="#00F500"><a href=../d46ee07e>3(47-&gt;1 papers)</a></td><td>4</td></tr><tr><td>5</td><td bgcolor="#00F500"><a href=../32472b3a>6(69-&gt;1 papers)</a></td><td bgcolor="#00E400"><a href=../dd854004>7(82-&gt;4 papers)</a></td><td bgcolor="#00F000"><a href=../2c8e1b89>8(41-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../c34c70b7>9(36-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../fed784bd>10(42-&gt;1 papers)</a></td><td>11</td></tr><tr><td>12</td><td bgcolor="#00F700"><a href=../15e03fbe>13(49-&gt;1 papers)</a></td><td bgcolor="#00FE00"><a href=../f73c24c7>14(122-&gt;0 papers)</a></td><td bgcolor="#00FE00"><a href=../18fe4ff9>15(42-&gt;0 papers)</a></td><td bgcolor="#00FE00"><a href=../f3c9f4fa>16(28-&gt;0 papers)</a></td><td bgcolor="#00ED00"><a href=../1c0b9fc4>17(48-&gt;2 papers)</a></td><td>18</td></tr><tr><td>19</td><td bgcolor="#00F800"><a href=../6735e2bc>20(43-&gt;1 papers)</a></td><td bgcolor="#00F500"><a href=../88f78982>21(106-&gt;1 papers)</a></td><td bgcolor="#00FE00"><a href=../63c03281>22(40-&gt;0 papers)</a></td><td>23</td><td bgcolor="#00E600"><a href=../6ede42c6>24(196-&gt;3 papers)</a></td><td>25</td></tr><tr><td>26</td><td bgcolor="#00E100"><a href=../85e9f9c5>27(72-&gt;3 papers)</a></td><td bgcolor="#00ED00"><a href=../74e2a248>28(72-&gt;2 papers)</a></td><td bgcolor="#00F100"><a href=../9b20c976>29(81-&gt;2 papers)</a></td><td bgcolor="#00E900"><a href=../a6bb3d7c>30(72-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../'49795642'>31(76-&gt;3 papers)</a></td></tr></table><table id="insights_6" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年四月April</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00E400"><a href=../d0de9c6>1(62-&gt;3 papers)</a></td><td bgcolor="#00EA00"><a href=../e63a52c5>2(159-&gt;3 papers)</a></td><td bgcolor="#00F900"><a href=../9f839fb>3(101-&gt;1 papers)</a></td><td bgcolor="#00F400"><a href=../eb242282>4(74-&gt;1 papers)</a></td><td bgcolor="#00E300"><a href=../4e649bc>5(72-&gt;4 papers)</a></td><td>6</td></tr><tr><td>7</td><td bgcolor="#00DF00"><a href=../f118c20c>8(46-&gt;3 papers)</a></td><td bgcolor="#00E100"><a href=../1edaa932>9(118-&gt;3 papers)</a></td><td bgcolor="#00F000"><a href=../23415d38>10(62-&gt;2 papers)</a></td><td bgcolor="#00F400"><a href=../cc833606>11(48-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../27b48d05>12(59-&gt;3 papers)</a></td><td>13</td></tr><tr><td>14</td><td bgcolor="#00FE00"><a href=../c568967c>15(46-&gt;0 papers)</a></td><td bgcolor="#00ED00"><a href=../2e5f2d7f>16(137-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../c19d4641>17(47-&gt;1 papers)</a></td><td bgcolor="#00E700"><a href=../30961dcc>18(57-&gt;3 papers)</a></td><td bgcolor="#00F800"><a href=../df5476f2>19(59-&gt;1 papers)</a></td><td>20</td></tr><tr><td>21</td><td bgcolor="#00FE00"><a href=../be56eb04>22(50-&gt;0 papers)</a></td><td bgcolor="#00DA00"><a href=../5194803a>23(104-&gt;4 papers)</a></td><td bgcolor="#00F600"><a href=../b3489b43>24(83-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../5c8af07d>25(47-&gt;1 papers)</a></td><td bgcolor="#00F700"><a href=../b7bd4b7e>26(56-&gt;1 papers)</a></td><td>27</td></tr><tr><td>28</td><td bgcolor="#00FE00"><a href=../46b610f3>29(41-&gt;0 papers)</a></td><td bgcolor="#00E700"><a href=../7b2de4f9>30(99-&gt;3 papers)</a></td></tr></table><table id="insights_7" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年三月March</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00F000"><a href=../707eef9e>1(67-&gt;2 papers)</a></td><td>2</td></tr><tr><td>3</td><td bgcolor="#00F700"><a href=../965724da>4(48-&gt;1 papers)</a></td><td bgcolor="#00EE00"><a href=../79954fe4>5(175-&gt;2 papers)</a></td><td bgcolor="#00EA00"><a href=../92a2f4e7>6(74-&gt;3 papers)</a></td><td bgcolor="#00F700"><a href=../7d609fd9>7(52-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../8c6bc454>8(58-&gt;2 papers)</a></td><td>9</td></tr><tr><td>10</td><td bgcolor="#00EC00"><a href=../b1f0305e>11(69-&gt;2 papers)</a></td><td bgcolor="#00F800"><a href=../5ac78b5d>12(115-&gt;1 papers)</a></td><td bgcolor="#00E500"><a href=../b505e063>13(53-&gt;2 papers)</a></td><td bgcolor="#00E800"><a href=../57d9fb1a>14(73-&gt;3 papers)</a></td><td bgcolor="#00E100"><a href=../b81b9024>15(58-&gt;3 papers)</a></td><td>16</td></tr><tr><td>17</td><td bgcolor="#00F900"><a href=../4de51b94>18(75-&gt;1 papers)</a></td><td bgcolor="#00DC00"><a href=../a22770aa>19(114-&gt;4 papers)</a></td><td bgcolor="#00EF00"><a href=../c7d03d61>20(61-&gt;2 papers)</a></td><td bgcolor="#00EE00"><a href=../2812565f>21(56-&gt;2 papers)</a></td><td bgcolor="#00E100"><a href=../c325ed5c>22(66-&gt;3 papers)</a></td><td>23</td></tr><tr><td>24</td><td bgcolor="#00F000"><a href=../21f9f625>25(67-&gt;2 papers)</a></td><td bgcolor="#00E100"><a href=../cace4d26>26(148-&gt;3 papers)</a></td><td bgcolor="#00EE00"><a href=../250c2618>27(84-&gt;2 papers)</a></td><td bgcolor="#00E800"><a href=../d4077d95>28(66-&gt;3 papers)</a></td><td bgcolor="#00E100"><a href=../3bc516ab>29(69-&gt;3 papers)</a></td><td>30</td></tr><tr><td>31</td></tr></table><table id="insights_8" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年二月February</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td bgcolor="#00E600"><a href=../ade8361b>1(52-&gt;3 papers)</a></td><td bgcolor="#00E200"><a href=../46df8d18>2(54-&gt;4 papers)</a></td><td>3</td></tr><tr><td>4</td><td bgcolor="#00EB00"><a href=../a4039661>5(71-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../4f342d62>6(221-&gt;2 papers)</a></td><td bgcolor="#00E900"><a href=../a0f6465c>7(77-&gt;3 papers)</a></td><td bgcolor="#00F400"><a href=../51fd1dd1>8(58-&gt;1 papers)</a></td><td bgcolor="#00F000"><a href=../be3f76ef>9(81-&gt;2 papers)</a></td><td>10</td></tr><tr><td>11</td><td bgcolor="#00EE00"><a href=../875152d8>12(46-&gt;2 papers)</a></td><td bgcolor="#00E800"><a href=../'689339e6'>13(93-&gt;3 papers)</a></td><td bgcolor="#00F200"><a href=../8a4f229f>14(51-&gt;2 papers)</a></td><td bgcolor="#00EC00"><a href=../658d49a1>15(49-&gt;2 papers)</a></td><td bgcolor="#00F000"><a href=../8ebaf2a2>16(68-&gt;2 papers)</a></td><td>17</td></tr><tr><td>18</td><td bgcolor="#00ED00"><a href=../7fb1a92f>19(88-&gt;2 papers)</a></td><td bgcolor="#00E400"><a href=../1a46e4e4>20(265-&gt;3 papers)</a></td><td bgcolor="#00E600"><a href=../f5848fda>21(108-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../1eb334d9>22(105-&gt;2 papers)</a></td><td bgcolor="#00E600"><a href=../f1715fe7>23(105-&gt;3 papers)</a></td><td>24</td></tr><tr><td>25</td><td bgcolor="#00F200"><a href=../175894a3>26(114-&gt;3 papers)</a></td><td bgcolor="#00EE00"><a href=../f89aff9d>27(165-&gt;3 papers)</a></td><td bgcolor="#00E900"><a href=../991a410>28(84-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../e653cf2e>29(96-&gt;2 papers)</a></td></tr></table><table id="insights_9" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2024年一月January</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td bgcolor="#00F400"><a href=../10225ad5>1(42-&gt;1 papers)</a></td><td bgcolor="#00EB00"><a href=../fb15e1d6>2(48-&gt;1 papers)</a></td><td bgcolor="#00ED00"><a href=../14d78ae8>3(24-&gt;2 papers)</a></td><td bgcolor="#00EF00"><a href=../f60b9191>4(29-&gt;2 papers)</a></td><td bgcolor="#00F800"><a href=../19c9faaf>5(28-&gt;1 papers)</a></td><td>6</td></tr><tr><td>7</td><td bgcolor="#00F800"><a href=../ec37711f>8(17-&gt;1 papers)</a></td><td bgcolor="#00DA00"><a href=../3f51a21>9(80-&gt;4 papers)</a></td><td bgcolor="#00EB00"><a href=../3e6eee2b>10(38-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../d1ac8515>11(36-&gt;2 papers)</a></td><td bgcolor="#00EB00"><a href=../3a9b3e16>12(60-&gt;2 papers)</a></td><td>13</td></tr><tr><td>14</td><td bgcolor="#00DA00"><a href=../d847256f>15(57-&gt;3 papers)</a></td><td>16</td><td bgcolor="#00CE00"><a href=../dcb2f552>17(163-&gt;5 papers)</a></td><td bgcolor="#00F700"><a href=../2db9aedf>18(35-&gt;1 papers)</a></td><td bgcolor="#00EA00"><a href=../c27bc5e1>19(49-&gt;3 papers)</a></td><td>20</td></tr><tr><td>21</td><td bgcolor="#00E400"><a href=../a3795817>22(45-&gt;3 papers)</a></td><td bgcolor="#00E700"><a href=../4cbb3329>23(75-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../ae672850>24(43-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../41a5436e>25(56-&gt;1 papers)</a></td><td bgcolor="#00DB00"><a href=../aa92f86d>26(46-&gt;3 papers)</a></td><td>27</td></tr><tr><td>28</td><td bgcolor="#00FE00"><a href=../5b99a3e0>29(42-&gt;0 papers)</a></td><td bgcolor="#00EC00"><a href=../660257ea>30(85-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../89c03cd4>31(57-&gt;1 papers)</a></td></tr></table><table id="insights_10" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年十二月December</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td bgcolor="#00E200"><a href=../cd1e2f18>1(44-&gt;3 papers)</a></td><td>2</td></tr><tr><td>3</td><td bgcolor="#00ED00"><a href=../2b37e45c>4(39-&gt;2 papers)</a></td><td bgcolor="#00E200"><a href=../c4f58f62>5(78-&gt;3 papers)</a></td><td bgcolor="#00E400"><a href=../2fc23461>6(44-&gt;3 papers)</a></td><td bgcolor="#00FC00"><a href=../c0005f5f>7(42-&gt;0 papers)</a></td><td bgcolor="#00EC00"><a href=../310b04d2>8(89-&gt;2 papers)</a></td><td>9</td></tr><tr><td>10</td><td bgcolor="#00EE00"><a href=../c90f0d8>11(41-&gt;2 papers)</a></td><td bgcolor="#00FE00"><a href=../e7a74bdb>12(72-&gt;0 papers)</a></td><td bgcolor="#00F600"><a href=../'86520e5'>13(48-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../eab93b9c>14(42-&gt;1 papers)</a></td><td bgcolor="#00EF00"><a href=../57b50a2>15(40-&gt;2 papers)</a></td><td>16</td></tr><tr><td>17</td><td bgcolor="#00E700"><a href=../f085db12>18(43-&gt;3 papers)</a></td><td bgcolor="#00E200"><a href=../1f47b02c>19(92-&gt;3 papers)</a></td><td bgcolor="#00F500"><a href=../7ab0fde7>20(67-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../957296d9>21(44-&gt;1 papers)</a></td><td bgcolor="#00EC00"><a href=../7e452dda>22(31-&gt;2 papers)</a></td><td>23</td></tr><tr><td>24</td><td bgcolor="#00EE00"><a href=../9c9936a3>25(38-&gt;2 papers)</a></td><td>26</td><td bgcolor="#00EC00"><a href=../986ce69e>27(72-&gt;2 papers)</a></td><td>28</td><td bgcolor="#00DE00"><a href=../86a5d62d>29(47-&gt;3 papers)</a></td><td>30</td></tr><tr><td>31</td></tr></table><table id="insights_11" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年十一月November</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td bgcolor="#00F800"><a href=../70d443d6>1(61-&gt;1 papers)</a></td><td bgcolor="#00EE00"><a href=../9be3f8d5>2(57-&gt;2 papers)</a></td><td bgcolor="#00F700"><a href=../742193eb>3(46-&gt;1 papers)</a></td><td>4</td></tr><tr><td>5</td><td bgcolor="#00F500"><a href=../920858af>6(54-&gt;1 papers)</a></td><td bgcolor="#00F100"><a href=../7dca3391>7(74-&gt;2 papers)</a></td><td bgcolor="#00F600"><a href=../8cc1681c>8(59-&gt;1 papers)</a></td><td bgcolor="#00EC00"><a href=../'63030322'>9(48-&gt;3 papers)</a></td><td bgcolor="#00F700"><a href=../5e98f728>10(69-&gt;1 papers)</a></td><td>11</td></tr><tr><td>12</td><td bgcolor="#00DB00"><a href=../b5af4c2b>13(34-&gt;3 papers)</a></td><td bgcolor="#00EF00"><a href=../'57735752'>14(119-&gt;2 papers)</a></td><td bgcolor="#00D900"><a href=../b8b13c6c>15(109-&gt;3 papers)</a></td><td bgcolor="#00E800"><a href=../5386876f>16(118-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../bc44ec51>17(154-&gt;2 papers)</a></td><td>18</td></tr><tr><td>19</td><td bgcolor="#00FE00"><a href=../c77a9129>20(27-&gt;0 papers)</a></td><td bgcolor="#00E600"><a href=../28b8fa17>21(99-&gt;3 papers)</a></td><td bgcolor="#00F400"><a href=../c38f4114>22(37-&gt;2 papers)</a></td><td bgcolor="#00EF00"><a href=../2c4d2a2a>23(39-&gt;2 papers)</a></td><td>24</td><td>25</td></tr><tr><td>26</td><td bgcolor="#00ED00"><a href=../25a68a50>27(48-&gt;2 papers)</a></td><td bgcolor="#00F000"><a href=../d4add1dd>28(87-&gt;2 papers)</a></td><td bgcolor="#00E300"><a href=../3b6fbae3>29(52-&gt;3 papers)</a></td><td bgcolor="#00EB00"><a href=../6f44ee9>30(47-&gt;2 papers)</a></td></tr></table><table id="insights_12" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年十月October</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td>1</td><td bgcolor="#00F100"><a href=../46752150>2(30-&gt;2 papers)</a></td><td bgcolor="#00ED00"><a href=../a9b74a6e>3(40-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../4b6b5117>4(30-&gt;2 papers)</a></td><td bgcolor="#00F200"><a href=../a4a93a29>5(30-&gt;2 papers)</a></td><td bgcolor="#00EE00"><a href=../4f9e812a>6(30-&gt;2 papers)</a></td><td>7</td></tr><tr><td>8</td><td bgcolor="#00FB00"><a href=../be95daa7>9(10-&gt;0 papers)</a></td><td bgcolor="#00F000"><a href=../830e2ead>10(172-&gt;3 papers)</a></td><td bgcolor="#00EA00"><a href=../6ccc4593>11(40-&gt;3 papers)</a></td><td bgcolor="#00F100"><a href=../87fbfe90>12(30-&gt;2 papers)</a></td><td bgcolor="#00EF00"><a href=../683995ae>13(30-&gt;2 papers)</a></td><td>14</td></tr><tr><td>15</td><td bgcolor="#00FE00"><a href=../8e105eea>16(10-&gt;0 papers)</a></td><td bgcolor="#00EE00"><a href=../61d235d4>17(135-&gt;2 papers)</a></td><td bgcolor="#00E900"><a href=../90d96e59>18(83-&gt;3 papers)</a></td><td bgcolor="#00E900"><a href=../7f1b0567>19(74-&gt;3 papers)</a></td><td bgcolor="#00F000"><a href=../1aec48ac>20(74-&gt;2 papers)</a></td><td>21</td></tr><tr><td>22</td><td bgcolor="#00F500"><a href=../f1dbf3af>23(108-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../1307e8d6>24(203-&gt;3 papers)</a></td><td bgcolor="#00F600"><a href=../fcc583e8>25(112-&gt;1 papers)</a></td><td bgcolor="#00F800"><a href=../17f238eb>26(89-&gt;1 papers)</a></td><td bgcolor="#00F600"><a href=../f83053d5>27(80-&gt;1 papers)</a></td><td>28</td></tr><tr><td>29</td><td bgcolor="#00FE00"><a href=../db62976c>30(67-&gt;0 papers)</a></td><td bgcolor="#00F800"><a href=../34a0fc52>31(141-&gt;3 papers)</a></td></tr></table><table id="insights_13" hidden="hidden" style="text-align:center;table-layout:fixed"><tr><td><form action><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td><td colspan="5">2023年九月September</td><td><form action><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td>星期日</br>Sunday</td><td>星期一</br>Monday</td><td>星期二</br>Tuesday</td><td>星期三</br>Wednesday</td><td>星期四</br>Thursday</td><td>星期五</br>Friday</td><td>星期六</br>Saturday</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>1</td><td>2</td></tr><tr><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td></tr><tr><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td></tr><tr><td>24</td><td>25</td><td>26</td><td>27</td><td bgcolor="#00D600"><a href=../13a1e3c6>28(30-&gt;2 papers)</a></td><td bgcolor="#00BC00"><a href=../d2c6faf>29(40-&gt;3 papers)</a></td><td>30</td></tr><tr></tr></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;压缩带来智能，5% 的论文决定学术界 95% 的成果！每天从 Arxiv
论文中总结分享最重要、最有趣的最多三篇论文。&lt;/p&gt;
&lt;p&gt;Compression brings intelligence, 5% of papers determine 95% of AI
technologies! Share the most important papers from Arxiv, every day, up
to three!&lt;/p&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
    var insight_now_id = 0;
    var insight_max_id = 13;
    function tips_insight(num){
        document.getElementById(&quot;insights_&quot;+insight_now_id).hidden = &quot;hidden&quot;;
        insight_now_id -= num;
        if (insight_now_id &gt; insight_max_id) {insight_now_id = insight_max_id;}
        if (insight_now_id &lt; 0) {insight_now_id = 0;}
        document.getElementById(&quot;insights_&quot; + insight_now_id).hidden = &quot;&quot;;
    }
&lt;/script&gt;
&lt;table id=&quot;insights_0&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年十月October
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../50bb6226&quot;&gt;1(172-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../bb8cd925&quot;&gt;2(74-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../544eb21b&quot;&gt;3(103-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00D300&quot;&gt;
&lt;a href=&quot;../b692a962&quot;&gt;4(128-&amp;gt;6 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
6
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../5da51261&quot;&gt;7(121-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
8
&lt;/td&gt;
&lt;td&gt;
9
&lt;/td&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;td&gt;
11
&lt;/td&gt;
&lt;td&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
13
&lt;/td&gt;
&lt;td&gt;
14
&lt;/td&gt;
&lt;td&gt;
15
&lt;/td&gt;
&lt;td&gt;
16
&lt;/td&gt;
&lt;td&gt;
17
&lt;/td&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
19
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
20
&lt;/td&gt;
&lt;td&gt;
21
&lt;/td&gt;
&lt;td&gt;
22
&lt;/td&gt;
&lt;td&gt;
23
&lt;/td&gt;
&lt;td&gt;
24
&lt;/td&gt;
&lt;td&gt;
25
&lt;/td&gt;
&lt;td&gt;
26
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
27
&lt;/td&gt;
&lt;td&gt;
28
&lt;/td&gt;
&lt;td&gt;
29
&lt;/td&gt;
&lt;td&gt;
30
&lt;/td&gt;
&lt;td&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_1&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年九月September
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../a11632bb&quot;&gt;2(30-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
3
&lt;/td&gt;
&lt;td bgcolor=&quot;#00DB00&quot;&gt;
&lt;a href=&quot;../ac0842fc&quot;&gt;4(169-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E900&quot;&gt;
&lt;a href=&quot;../43ca29c2&quot;&gt;5(52-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../a8fd92c1&quot;&gt;6(46-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
8
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../59f6c94c&quot;&gt;9(33-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E300&quot;&gt;
&lt;a href=&quot;../646d3d46&quot;&gt;10(66-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../8baf5678&quot;&gt;11(49-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../6098ed7b&quot;&gt;12(38-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../8f5a8645&quot;&gt;13(32-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
14
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
15
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../69734d01&quot;&gt;16(39-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EB00&quot;&gt;
&lt;a href=&quot;../86b1263f&quot;&gt;17(101-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F300&quot;&gt;
&lt;a href=&quot;../77ba7db2&quot;&gt;18(78-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E700&quot;&gt;
&lt;a href=&quot;../9878168c&quot;&gt;19(53-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E900&quot;&gt;
&lt;a href=&quot;../fd8f5b47&quot;&gt;20(55-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
22
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../16b8e044&quot;&gt;23(56-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00DA00&quot;&gt;
&lt;a href=&quot;../f464fb3d&quot;&gt;24(187-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../1ba69003&quot;&gt;25(79-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../f0912b00&quot;&gt;26(73-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E900&quot;&gt;
&lt;a href=&quot;../1f53403e&quot;&gt;27(84-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
29
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EE00&quot;&gt;
&lt;a href=&quot;../3c018487&quot;&gt;30(53-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_2&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年八月August
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
4
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
6
&lt;/td&gt;
&lt;td&gt;
7
&lt;/td&gt;
&lt;td&gt;
8
&lt;/td&gt;
&lt;td&gt;
9
&lt;/td&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
11
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../bd0e34fe&quot;&gt;12(73-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F200&quot;&gt;
&lt;a href=&quot;../52cc5fc0&quot;&gt;13(68-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../b01044b9&quot;&gt;14(53-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../5fd22f87&quot;&gt;15(41-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../b4e59484&quot;&gt;16(36-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../45eecf09&quot;&gt;19(45-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F100&quot;&gt;
&lt;a href=&quot;../201982c2&quot;&gt;20(102-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../cfdbe9fc&quot;&gt;21(67-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E700&quot;&gt;
&lt;a href=&quot;../24ec52ff&quot;&gt;22(58-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../cb2e39c1&quot;&gt;23(78-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
24
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
25
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../2d07f285&quot;&gt;26(36-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../c2c599bb&quot;&gt;27(76-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../33cec236&quot;&gt;28(50-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../dc0ca908&quot;&gt;29(53-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F100&quot;&gt;
&lt;a href=&quot;../e1975d02&quot;&gt;30(40-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_3&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年七月July
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../b0c78508&quot;&gt;1(68-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../5bf03e0b&quot;&gt;2(155-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../b4325535&quot;&gt;3(96-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../56ee4e4c&quot;&gt;4(87-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
7
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../4cd2aec2&quot;&gt;8(121-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../a310c5fc&quot;&gt;9(106-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F100&quot;&gt;
&lt;a href=&quot;../9e8b31f6&quot;&gt;10(69-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../71495ac8&quot;&gt;11(45-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EE00&quot;&gt;
&lt;a href=&quot;../9a7ee1cb&quot;&gt;12(46-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
14
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../78a2fab2&quot;&gt;15(49-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EA00&quot;&gt;
&lt;a href=&quot;../939541b1&quot;&gt;16(112-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../7c572a8f&quot;&gt;17(97-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../629e1a3c&quot;&gt;19(129-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
21
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../39c87ca&quot;&gt;22(50-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E500&quot;&gt;
&lt;a href=&quot;../ec5eecf4&quot;&gt;23(105-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F400&quot;&gt;
&lt;a href=&quot;../e82f78d&quot;&gt;24(54-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../e1409cb3&quot;&gt;25(49-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EF00&quot;&gt;
&lt;a href=&quot;../a7727b0&quot;&gt;26(60-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
28
&lt;/td&gt;
&lt;td&gt;
29
&lt;/td&gt;
&lt;td&gt;
30
&lt;/td&gt;
&lt;td&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_4&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年六月June
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../69a48cb0&quot;&gt;3(77-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E500&quot;&gt;
&lt;a href=&quot;../8b7897c9&quot;&gt;4(153-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F100&quot;&gt;
&lt;a href=&quot;../64bafcf7&quot;&gt;5(93-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../8f8d47f4&quot;&gt;6(90-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E900&quot;&gt;
&lt;a href=&quot;../604f2cca&quot;&gt;7(84-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
9
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../431de873&quot;&gt;10(80-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../acdf834d&quot;&gt;11(150-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E800&quot;&gt;
&lt;a href=&quot;../47e8384e&quot;&gt;12(148-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E700&quot;&gt;
&lt;a href=&quot;../a82a5370&quot;&gt;13(83-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../4af64809&quot;&gt;14(88-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
16
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EB00&quot;&gt;
&lt;a href=&quot;../a1c1f30a&quot;&gt;17(72-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E600&quot;&gt;
&lt;a href=&quot;../50caa887&quot;&gt;18(310-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E500&quot;&gt;
&lt;a href=&quot;../bf08c3b9&quot;&gt;19(161-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
20
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E600&quot;&gt;
&lt;a href=&quot;../353de54c&quot;&gt;21(223-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
23
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../d3142e08&quot;&gt;24(100-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../3cd64536&quot;&gt;25(188-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../d7e1fe35&quot;&gt;26(101-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E600&quot;&gt;
&lt;a href=&quot;../3823950b&quot;&gt;27(101-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EC00&quot;&gt;
&lt;a href=&quot;../c928ce86&quot;&gt;28(78-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
30
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_5&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年五月May
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EB00&quot;&gt;
&lt;a href=&quot;../d09b3043&quot;&gt;1(64-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EB00&quot;&gt;
&lt;a href=&quot;../3bac8b40&quot;&gt;2(71-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../d46ee07e&quot;&gt;3(47-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../32472b3a&quot;&gt;6(69-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../dd854004&quot;&gt;7(82-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../2c8e1b89&quot;&gt;8(41-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../c34c70b7&quot;&gt;9(36-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../fed784bd&quot;&gt;10(42-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
11
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
12
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../15e03fbe&quot;&gt;13(49-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../f73c24c7&quot;&gt;14(122-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../18fe4ff9&quot;&gt;15(42-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../f3c9f4fa&quot;&gt;16(28-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../1c0b9fc4&quot;&gt;17(48-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
19
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../6735e2bc&quot;&gt;20(43-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F500&quot;&gt;
&lt;a href=&quot;../88f78982&quot;&gt;21(106-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../63c03281&quot;&gt;22(40-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
23
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E600&quot;&gt;
&lt;a href=&quot;../6ede42c6&quot;&gt;24(196-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
25
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
26
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../85e9f9c5&quot;&gt;27(72-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../74e2a248&quot;&gt;28(72-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F100&quot;&gt;
&lt;a href=&quot;../9b20c976&quot;&gt;29(81-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E900&quot;&gt;
&lt;a href=&quot;../a6bb3d7c&quot;&gt;30(72-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E600&quot;&gt;
&lt;a href=&quot;../&#39;49795642&#39;&quot;&gt;31(76-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_6&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年四月April
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E400&quot;&gt;
&lt;a href=&quot;../d0de9c6&quot;&gt;1(62-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EA00&quot;&gt;
&lt;a href=&quot;../e63a52c5&quot;&gt;2(159-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../9f839fb&quot;&gt;3(101-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F400&quot;&gt;
&lt;a href=&quot;../eb242282&quot;&gt;4(74-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E300&quot;&gt;
&lt;a href=&quot;../4e649bc&quot;&gt;5(72-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
7
&lt;/td&gt;
&lt;td bgcolor=&quot;#00DF00&quot;&gt;
&lt;a href=&quot;../f118c20c&quot;&gt;8(46-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../1edaa932&quot;&gt;9(118-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../23415d38&quot;&gt;10(62-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F400&quot;&gt;
&lt;a href=&quot;../cc833606&quot;&gt;11(48-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E500&quot;&gt;
&lt;a href=&quot;../27b48d05&quot;&gt;12(59-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
14
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../c568967c&quot;&gt;15(46-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00ED00&quot;&gt;
&lt;a href=&quot;../2e5f2d7f&quot;&gt;16(137-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../c19d4641&quot;&gt;17(47-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E700&quot;&gt;
&lt;a href=&quot;../30961dcc&quot;&gt;18(57-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../df5476f2&quot;&gt;19(59-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
21
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../be56eb04&quot;&gt;22(50-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00DA00&quot;&gt;
&lt;a href=&quot;../5194803a&quot;&gt;23(104-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F600&quot;&gt;
&lt;a href=&quot;../b3489b43&quot;&gt;24(83-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../5c8af07d&quot;&gt;25(47-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../b7bd4b7e&quot;&gt;26(56-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
28
&lt;/td&gt;
&lt;td bgcolor=&quot;#00FE00&quot;&gt;
&lt;a href=&quot;../46b610f3&quot;&gt;29(41-&amp;gt;0 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E700&quot;&gt;
&lt;a href=&quot;../7b2de4f9&quot;&gt;30(99-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_7&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;td colspan=&quot;5&quot;&gt;
2024年三月March
&lt;/td&gt;
&lt;td&gt;
&lt;form action&gt;
&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;
&lt;/form&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
星期日&lt;br&gt;Sunday
&lt;/td&gt;
&lt;td&gt;
星期一&lt;br&gt;Monday
&lt;/td&gt;
&lt;td&gt;
星期二&lt;br&gt;Tuesday
&lt;/td&gt;
&lt;td&gt;
星期三&lt;br&gt;Wednesday
&lt;/td&gt;
&lt;td&gt;
星期四&lt;br&gt;Thursday
&lt;/td&gt;
&lt;td&gt;
星期五&lt;br&gt;Friday
&lt;/td&gt;
&lt;td&gt;
星期六&lt;br&gt;Saturday
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../707eef9e&quot;&gt;1(67-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
3
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../965724da&quot;&gt;4(48-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EE00&quot;&gt;
&lt;a href=&quot;../79954fe4&quot;&gt;5(175-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EA00&quot;&gt;
&lt;a href=&quot;../92a2f4e7&quot;&gt;6(74-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F700&quot;&gt;
&lt;a href=&quot;../7d609fd9&quot;&gt;7(52-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../8c6bc454&quot;&gt;8(58-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
10
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EC00&quot;&gt;
&lt;a href=&quot;../b1f0305e&quot;&gt;11(69-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F800&quot;&gt;
&lt;a href=&quot;../5ac78b5d&quot;&gt;12(115-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E500&quot;&gt;
&lt;a href=&quot;../b505e063&quot;&gt;13(53-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E800&quot;&gt;
&lt;a href=&quot;../57d9fb1a&quot;&gt;14(73-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../b81b9024&quot;&gt;15(58-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
17
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F900&quot;&gt;
&lt;a href=&quot;../4de51b94&quot;&gt;18(75-&amp;gt;1 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00DC00&quot;&gt;
&lt;a href=&quot;../a22770aa&quot;&gt;19(114-&amp;gt;4 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EF00&quot;&gt;
&lt;a href=&quot;../c7d03d61&quot;&gt;20(61-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EE00&quot;&gt;
&lt;a href=&quot;../2812565f&quot;&gt;21(56-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../c325ed5c&quot;&gt;22(66-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
24
&lt;/td&gt;
&lt;td bgcolor=&quot;#00F000&quot;&gt;
&lt;a href=&quot;../21f9f625&quot;&gt;25(67-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../cace4d26&quot;&gt;26(148-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00EE00&quot;&gt;
&lt;a href=&quot;../250c2618&quot;&gt;27(84-&amp;gt;2 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E800&quot;&gt;
&lt;a href=&quot;../d4077d95&quot;&gt;28(66-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td bgcolor=&quot;#00E100&quot;&gt;
&lt;a href=&quot;../3bc516ab&quot;&gt;29(69-&amp;gt;3 papers)&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;
30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2023-12-31总结(年度总结)</title>
    <link href="https://www.yynnyy.cn/aa29fa81"/>
    <id>https://www.yynnyy.cn/aa29fa81</id>
    <published>2023-12-31T07:45:21.000Z</published>
    <updated>2024-08-09T09:26:46.966Z</updated>
    
    <content type="html"><![CDATA[<p>上次写总结还是在2023-9-29,没想到下次再写竟已经是3个月之后了，到了2023年的最后一天。每到年末，各种APP就喜欢来个xxx年度总结：B站总结、steam总结、网易云音乐总结……不过今天看到一个"新华社年度十大新闻"觉得挺有意思，我就想，能不能给我自己也列一个"年度十大新闻"呢？</p><span id="more"></span><p><img src="../../files/images/diary/2023-most-influence-news.jpg" style="zoom:33%;" ></p><p>总结十大新闻是一个很重要的事情，也许10年、20年后，我都已经忘记了2023年发生过什么，但可能还会记得之前总结的"十大新闻"，在这个意义上，这其实就代表了整个2023年对我的印象。就像杨大伯之前说的"每门课上完，你最少记住一句话，记住一辈子"。大概就是这个道理</p><p>然而，总结年度十大新闻也是一个很难的事情，因为很多事件的影响可能很难在年底就出现，可能更应该来个时间检验奖，2023年更容易评出来"2013年年度10大新闻"，大家也就姑且一听吧:</p><ol type="1"><li>本科毕业：2023-06</li><li>研究生入学: 2023-09。还是华子，还是贵系</li><li>和TLE的1000天纪念：2023-06</li><li>去成都旅游：2023-08</li><li>第一次发Twitter：2023-08</li><li>第一次得新冠: 2023-01</li><li>尝试纹理烫：2023-02</li><li>google scholar 100引用：2023-08</li><li>end2end做饭：2023-07</li><li>凑齐apple全家桶：2023-06</li></ol><p>不知不觉，本科都已经毕业了，变成研究生了……前几天拉着室友一起回到十一学校去帮清华招生宣讲，突然感觉自己变得好老，高中学弟说的高中生活、本科学弟说的本科生活，好像全都已经离我远去了。明明研究生才开始一个学期，但心态似乎已经完全不同了。说起来，本科生活似乎也没有很"多彩"，各种行动不管是实践或者运动，好像也大都有某种目的性。虽说大学的优化目标比较多比高中多，看似非常的diverse。但各个项目都要优化、都和某些利益挂钩，反而会让所有事情都隐约套了个目的性的壳：运动是为了阳光打卡，科研是为了保研，社工是为了评奖……到了研究生，优化目标反而纯粹了，这种目的性的东西基本没有了，希望可以更多的做一些纯兴趣的东西，比如来场随心的旅行</p><p>华子的传统是每年的跨年敲钟时都要宣布一个有利于生权的好事。2019年跨年时邱宝敲钟完宣布校园网流量从一个月20GB升级到了50GB。今年的好消息看来是"清华和北大不限制双向入校"。仔细想了想，虽然感觉华子不怎么关心生权，但把尺度放大，从我入学到现在，想达成的几个生权的好事基本也都达成了：</p><ul><li>无限量校园网，在21年校庆的时候完成</li><li>白天可以洗热水澡，22年校庆的时候完成</li></ul><p>希望等研究生毕业的时候下面这几件事也能完成：</p><ul><li>洗衣机免费</li><li>食堂可以卖冰淇淋(手工)</li><li>为计算机系同学提供免费的算力，比如1xA100/ person</li><li>学校里可以有电动车充电桩</li></ul><p>喊了一年的新系馆，23年最终还是没住成，现在又改口说是24年3月搬，无所谓吧，反正这已经是第5次改口了，就像美国的国债限额，大概快到时间了还会再延期吧。西体育馆终于建成了，我记得18年暑校的时候辅导员就在提冰雪场馆，19年入学的时候好像就围起来了，最终23年终于可以滑冰了。虽然还没有体验过，不过看同学们反馈都不错，24年真得去试一试。现在东边的足球场又围起来了，听说要建一个最大的地下学生活动中心，还挺期待的，希望我毕业的时候能用上。</p><p>华子每年都在盖新的楼，每年也都有新盖好、装修好的楼投入使用；同学每年都在提新的需求，每年也都有需求被满足……希望我也一样，每年都能有新的回忆产生，每年也都能找到新的目标，也能交到新的朋友</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;上次写总结还是在2023-9-29,
没想到下次再写竟已经是3个月之后了，到了2023年的最后一天。每到年末，各种APP就喜欢来个xxx年度总结：B站总结、steam总结、网易云音乐总结……不过今天看到一个&quot;新华社年度十大新闻&quot;觉得挺有意思，我就想，能不能给我自己也列一个&quot;年度十大新闻&quot;呢？&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>论文阅读[粗读]-Alignment For Honesty</title>
    <link href="https://www.yynnyy.cn/2300f7be"/>
    <id>https://www.yynnyy.cn/2300f7be</id>
    <published>2023-12-19T01:25:45.000Z</published>
    <updated>2024-08-09T09:26:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>上周刷到了刘鹏飞老师的 Alignment For Honesty, 分享给了大家<a href="/86520e5.html" title="2023-12-13-insights">2023-12-13-insights</a>。里面讲到如何训练LLM变得诚实，他沿用了孔子的定义：</p><blockquote><p>知之为知之，不知为不知，是知(zhì)也。</p><p>To say “I know” when you know, and “I don’t know” when you don’t,that is wisdom.</p></blockquote><p>我来一起看看他们是怎么做的吧</p><span id="more"></span><h2 id="introduction">introduction</h2><p>作者团队来自上交、复旦和CMU，其中复旦的xipengqiu老师也是arxiv的常客了</p><p><img src=" ../../files/images/align-for-honesty/authors.png"></p><p>其实关于honesty,这个领域由来已久，本文作者也提到了，学界对于Honesty有各种各样的定义和表述方式。前两天读weak-to-stronggeneralization时，OpenAI也提到了相关的研究，有兴趣的同学可以进一步顺着引文看一看相关的研究~</p><p><img src=" ../../files/images/align-for-honesty/openai.png"></p><p>回到本文，作者按照《论语》里给出的定义来定义诚实：知之为知之，不知为不知，是知(zhì)也。具体来说，需要模型可以分辨自己的知识边界：</p><ul><li>边界内的问题予以回答</li><li>边界外的问题勇于承认</li></ul><blockquote><p>不过，我觉得这里的语境和孔子想表述的有些区别：对于人来说，认知到知识边界很容易，只是很多时候羞于承认，所以这种"勇于承认"是一种君子的品格。但对于模型来说，还没有到荣辱心这一步，他只是单纯地意识不到自己的知识边界……</p></blockquote><p>让模型获得Honesty有各种各样的好处，其中最显然地就是减少hallicinate。虽然Honesty是"对齐三剑客"(helpful,harmless,honest)之一，但学界对于这方面的研究其实很少，作者就把这个领域按照alignment的语境重新定义了一下：对于做不出来的东西，要回答一个idksigns(I Don't Know)</p><h2 id="formulation">formulation</h2><blockquote><p>这个写法不多见，一般论文没有这个section。因为本篇工作是第一篇工作，所以需要把问题描述定义一下，然后说一说评测方法是什么</p></blockquote><p>首先，这里作者做了一个简化:这篇工作中，作者认为模型知识和世界知识是一个集和，假设模型不会说谎，如果回答错了，那大概率就是自己不懂这个知识。</p><p><img src=" ../../files/images/align-for-honesty/boundary.png"></p><h3 id="训练框架">训练框架</h3><p>作者提出了一套多轮refine的框架，希望随着训练的迭代，模型可以逐渐清晰地认识到自己的知识边界</p><blockquote><p>在这一点上，我倾向于OpenAI的观点："认知到自己的知识边界"是一个latentknowledge，应该是模型本身具备的(毕竟是自己的知识，以及本身有calibration性质)，我们只需要训练模型去激发elicit出来。因此这个任务定义好以后，可能不太难</p></blockquote><p>作者把模型对于一个知识问题的回答分成了三类： <spanclass="math display">\[c(x,y) =  \left\{\begin{aligned}&amp; -1, \text{type}(y) = \text{idk}, \\&amp; 1, \text{type}(y) = \text{correct}, \\&amp; 0, \text{type}(y) = \text{wrong},\end{aligned}\right.\]</span> 接下来，根据该模型是否知道该问题的答案<spanclass="math inline">\(k(x) = 1\text{ if model know the answer, else-1}\)</span> <span class="math display">\[v(x,y) = \left\{\begin{aligned}1, &amp; c(x,y)*k(x,y) = 1, \\0, &amp; \text{else},\end{aligned}\right.\]</span>有了价值函数以后，就可以根据这个价值函数进行训练，预期价值函数随着训练变得越来越高。当然，</p><ul><li>在真实答案已知的情况下，c很容易获得</li><li>然而，k是一个很难获取的东西，因为是一个latentknowledge，后面作者探索了几种近似得办法</li></ul><h3 id="评测">评测</h3><p><img src=" ../../files/images/align-for-honesty/condition.png" style="zoom:33%;" ></p><p>即使按照上面的框架训练了，模型的效果仍然不好评测。不过，根据迭代前后模型的表现，作者可以天然的把问题分为9个大类</p><blockquote><p>其中的2,3类说明之前没做出来，后面做出来了(尽管没有泄露正确答案)。是个比较奇怪的现象，本篇工作不关注这个</p></blockquote><p>这里作者参考F1-score，讨论了一种近似的评测办法：</p><ul><li>over-conservativeness：我们不希望模型过于谨慎，希望能做出来的题目就正确回答。因此计算公式很简单</li></ul><p><span class="math display">\[S_1 = \frac{7}{1 + 4 + 7}, \text{lower is better}\]</span></p><ul><li>Prudence：这个和上面的相反，考虑的是，不会做的问题，希望模型正确地回答idk</li></ul><p><span class="math display">\[S_2 = \frac{8+9}{5 + 6 + 8 + 9}, \text{higher is better}\]</span></p><p>有了上面的计算，就可以给出一个honesty增量</p><blockquote><p>注意，这个指标如果模型不训练，那就是只有1,5&gt;0，<spanclass="math inline">\(S_1=0,S_2=0,S=0.5\)</span></p></blockquote><p><span class="math display">\[S_\text{honesty} = \frac{(1-S_1) + S_2}{2}\]</span></p><h2 id="method">method</h2><p>首先，prompt方法是一个显然的办法(这里就是单轮迭代，只有prompt前后的区别)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAINTEXT"><figure class="iseeu highlight /plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Answer the question. If you don’t know the answer to the question, it is appropriate to say “I apologize, but I’m not able to provide an answer to the question.”</span><br><span class="line">   Q: &lt;question&gt;</span><br><span class="line">   A:</span><br></pre></td></tr></table></figure></div><p>接下来，训练地方法，作者设计了三种。这三种都是基于一个蒙特卡洛估计的办法，作者会让没对齐的模型对于一个问题生成多个(10个)回答，检查每个回答是否正确。给出一个信心值expectedacc作为模型认知<span class="math inline">\(k(x)\)</span>的一个近似</p><p><img src=" ../../files/images/align-for-honesty/sample.png" style="zoom:33%;" ></p><h3 id="absolute">ABSOLUTE</h3><p>设定一个阈值<span class="math inline">\(\tau\)</span>，<spanclass="math inline">\(k(x) = 1 \quad if \quad \text{expected acc} &gt;\tau\)</span>。然后标数据的时候，把所有k(x)=-1的回答都改成了一个idkresponse</p><h3 id="confidence">CONFIDENCE</h3><figure class="half"><img src="../../files/images/align-for-honesty/numb.png" width="45%"/><img src="../../files/images/align-for-honesty/verb.png" width="45%" /></figure><p>这里，作者标数据的时候直接把confidence写在回答里，然后按照正常SFT的办法</p><h3 id="multisample">MULTISAMPLE</h3><p>刚才的absolute会根据一个阈值卡，这里作者直接把sample多次的每条数据当成单独的了，然后<spanclass="math inline">\(k(x) =(c(x,y)==1)\)</span>。也就是说，标数据的时候，本来作对了的就不动，本来做错了的就改成一个idkresponse。</p><blockquote><p>值得注意的是，这个方案会把训练集扩大M倍</p></blockquote><h2 id="experiment">experiment</h2><p>这里作者提了两个朴素的baseline：</p><ul><li>原来的模型</li><li>fine-tuned：在相同训练量上，使用turbo的answer进行SFT的模型</li><li>prompt：上面提到的training-free方法</li><li>三种training方法，其中，<spanclass="math inline">\(\tau\)</span>选取的是0.1</li></ul><figure class="half"><img src="../../files/images/align-for-honesty/main.png" width="40%"/><img src="../../files/images/align-for-honesty/OOD.png" width="55%" /></figure><p>作者在TraivalQA数据集上做训练，使用Llama2-chat7b作为基础模型，分别评测in-domain的traivalQA和OOD的另外三个数据集</p><p>效果如下：</p><ul><li><p>发现基于训练的方法显著好于不训练的方法</p></li><li><p>相对来说，把confidence放到数据里，会让模型表现更好</p></li><li><p>honesty属性在不同数据集上迁移能力较好，不管是ID还是OOD，加上confidencescore都能让模型做的更好</p></li><li><p>直接finetune模型，会导致模型更加hallicinate，acc反而下降（这点在PKQA数据集表现得尤其明显）</p></li></ul><p>接下来，作者探索了<spanclass="math inline">\(\tau\)</span>对结果的影响，画了一张类似f1里面auc的图。发现，<spanclass="math inline">\(\tau\)</span>越大，越容易把数据分类成模型不知道</p><ul><li>因此idk数据越多，模型越容易变得over-confidence</li><li>另一方面，模型也越谨慎，所以prudence会提升，这里需要有一个权衡</li></ul><p><img src=" ../../files/images/align-for-honesty/auc.png" style="zoom:50%;" ></p><p>接下里，作者又做了scaling的实验：更大的模型会做得更好吗，更多的数据会做的更好吗？</p><figure class="half"><img src="../../files/images/align-for-honesty/scale.png" width="35%"/><img src="../../files/images/align-for-honesty/data-scale.png" width="60%" /></figure><p>首先，作者发现，confidence-basedmethod对于所有模型规模效果都要更好一些</p><blockquote><p>我发现：不同规模的模型对于Honesty的效果没啥区别，这说明了这个任务其实是挺困难的</p></blockquote><p>其次，如果在训练集中加入MMLU的训练数据，对于Multi-sample方法的帮助很大，说明这个属性的习得也许是data-hungry的，模型需要更diverse的情况来判断自己的知识边界</p><blockquote><p>不过，为啥Multisample+MMLU-data以后Acc下降这么多呢？</p></blockquote><p>最后作者做了一些"对齐税"方面的实验，发现Honesty训练基本不会导致模型在别的任务表现下降。最后，作者总结了一下limitation和future，提了几个问题，我觉得还挺有意思的，分享给大家：</p><ul><li>更好的k(x)：本篇工作用模型回答正确与否判断模型是否知道，这个在MMLU这种4选1中有误判假阳的情况</li><li>confidencescore能不能更好的利用？这里作者和calibration联系了一下</li><li>和RAG的结合：认知到自己知识边界的模型更清楚自己该怎么利用外界知识</li><li>和长文本的结合：需要结合reasoning的长文本场景的Honesty现在还没有研究，并且需要更细致的评测和训练</li></ul><h2 id="我的思考">我的思考</h2><p>很好的文章，formulation到method到实验设计都很顺滑，逻辑很完整，我看完了以后主要想到下面几个问题：</p><ul><li>感觉可以评测一下turbo或者GPT4的表现？这里没做估计是因为需要一个unaligned模型去计算，没办法。要测也许只能给turbo来个un-alignfinetune，不知道是不是违规的</li><li>scaling实验中，发现所有llama表现都差不多，说明这个能力也许是一个emergent的，甚至是reverse-scaling的？</li><li>这个能力，似乎是不能通过SFT习得的？因为每个模型都有自己的知识边界。作者也提到了，SFT-baselinewill lead models to learn to hallicinate</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;上周刷到了刘鹏飞老师的 Alignment For Honesty, 分享给了大家
&lt;a href=&quot;/86520e5.html&quot; title=&quot;2023-12-13-insights&quot;&gt;2023-12-13-insights&lt;/a&gt;。里面讲到如何训练LLM变得诚实，他沿用了孔子的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;知之为知之，不知为不知，是知(zhì)也。&lt;/p&gt;
&lt;p&gt;To say “I know” when you know, and “I don’t know” when you don’t,
that is wisdom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我来一起看看他们是怎么做的吧&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="post-pretrain" scheme="https://www.yynnyy.cn/tags/post-pretrain/"/>
    
  </entry>
  
  <entry>
    <title>Weak-to-Strong Generalization(上): OpenAI是怎么看问题的？</title>
    <link href="https://www.yynnyy.cn/3229ec6"/>
    <id>https://www.yynnyy.cn/3229ec6</id>
    <published>2023-12-16T02:52:21.000Z</published>
    <updated>2024-08-09T09:26:46.970Z</updated>
    
    <content type="html"><![CDATA[<p>昨天OpenAI一口气更新了两篇论文，暨DALL.E3之后的又一更新，其中一篇讲述了一个朴素的问题：如果未来的模型超越人类了，我们该怎么给他们提供监督信号？（毕竟我们只有人类——一个相对更弱的模型）</p><p>OpenAI把这个问题叫做weak-to-stronggeneralization在这里做了一些简单的尝试，对于这个问题的性质进行了一些探索。我们来一起学习一下他们看问题和解决问题的思路吧！</p><span id="more"></span><p>作者首先映入眼帘的就是ilyaSutskever，这个老哥真是为人类尽心尽力呀……OpenAI官网写的这个论文的作者是"Safety&amp;Alignment"</p><p><img src="../../files/images/weak2strong/authors.png"></p><p>而他的primary authors JeffWu更是重量级，我不多评价，直接列出其最近的publication，可以说是群星璀璨了……我什么时候能发12篇论文都有这个质量呀</p><p><img src="../../files/images/weak2strong/jeff_wu.png" style="zoom:30%;" ></p><h2 id="introduction">introduction</h2><p>回到正题，作者提到目前的研究更多focus在RLHF，既模型向人类偏好对齐，这套框架最终可以发展出一个类似于人的通用人工智能。然而，到此为止了：想要训练、对齐一个超越人类的智能，目前的方法都不再适用了，因为我们最高也只能用到人了。举个例子:</p><blockquote><p>我们可以要求模型写出一个100万行的很复杂的代码来完成一个任务，然而让人去看代码好不好是一个很难达成的任务。</p></blockquote><p>因此，目前还没有关于superalignment（对齐一个超越人的模型）的研究，并且，我们更希望其对应的方法可以不止适用于目前的模型，更适用于未来的模型(consistency)。在本篇工作中，作者类比了人类监督超人模型的问题，提出了一个类似的问题：可以用GPT2监督GPT4吗？</p><blockquote><p>具体来说，能不能用GPT2在某个任务的trainset上finetune，然后给trainset重新打一遍标签。让GPT4在GPT2打标签的trainset上训练，然后观察GPT2和GPT4在testset上的表现？</p></blockquote><p>这是一个很直观很容易实现的setup，作者把这个set up叫做weak-to-stronglearning。</p><p>听起来这个weak-to-stronggeneralization似乎是不可能的，不过作者进行了一些解释：在学习过程中，我们不是预期弱模型教会强模型知识(这些知识是强模型本来就会的)，而是希望弱模型教会强模型这个任务的概念是什么，和一些intension，因此这个过程更像是激发"elicit"。用刚才那个例子来说，如果强模型已经能写100万行的代码，那他肯定有潜力去理解人类的目的是什么。因此即使弱模型给的标签有误差，应该不影响一个更鲁棒的强模型去理解，这就是作者预期出现的结果。</p><p><img src="../../files/images/weak2strong/first_perf.png" style="zoom:30%;" ></p><p>作者在传统NLP task(二分类)、chesspuzzles(选出最优步的生成任务)、human preferencemodeling(二分类)上做了实验，并得出一些结论：</p><ul><li>强模型在weak-to-stronggeneralize以后基本都会超越帮他标数据的弱模型。？？？</li><li>直接做weak-to-strong generalize的效果并不好，比强模型+oracle-labelfinetune 差很远，可能需要一些额外的优化</li><li>一些很简单、很直觉地方案对于这个任务的提升很大。这说明这个领域还有很大的进步空间</li></ul><h2 id="method">method</h2><p>像刚才Introduction里提到的，这个setup是很简单的。不过，比起一般论文讲完方法讲实验，作者额外说了一下这个setup的优点和问题。作者额外说了一个他们观察的指标performancegaprecovered(PRG)：假设weak-to-strong训练比小模型效果好，那他能恢复强模型直接训练的几分风采？注意，这里面PRG=1代表和强模型直接训练等价，PRG=0代表和小模型表现一样，完全没有generalize。</p><blockquote><p>为什么要和 “强模型+orcale finetune”去比？虽然现在的setup是数据集，因此有orcale 标签可以做"强模型+orcalefinetune"的上限实验。但事实上所谓的"orcale标签"也是人去打的标签，未来的模型在完成超人任务时就没有标签了，或者说只有人去打的"噪声标签"。</p><p>这种情况下，我们可以预设有"外星人打的oracle标签"跑上限实验，也就是说：我们更关注超人模型的能力上限能不能被激发出来。PGR可以看做一个"激发比例"</p></blockquote><p><img src="../../files/images/weak2strong/prg.png" style="zoom:50%;" ></p><p>优点：总体来说，就是这个方案更有利于scaling</p><ul><li>对于模型不可知，任何的 强-弱模型对都能跑这个实验，也不需要一定要有GPT4。另外，这个路径不需要人，因此实验很快</li><li>对任务不可知，任何任务都能跑这个setup</li><li>这里面得到的关于超人模型的结论，即使现在也能用的上：未来对齐GPT10的方法，不妨现在就用到GPT4里？</li></ul><blockquote><p>这也许暗示着目前OpenAI内部已经有超人模型了……</p></blockquote><p>缺点：</p><ul><li>Imitationsaliency：这个我觉得很难翻译，就直接摆原文了。这里讲的是未来的超人模型和今天的GPT4存在统计学的巨大差异。对未来的超人模型来说，理解人类的想法是一个很简单的问题，但GPT4去理解GPT2可能没那么简单。并且，GPT2犯的错误类型和人类犯的错误类型也有很大区别，因此目前的结论更多是研究GPT2错误，会有统计学差异。</li><li>数据泄露：目前的setup里面的oracle标签可能或多或少在预训练数据里出现过。未来的模型不存在"超人的预训练数据"，因此实验结论可能和现在有偏差</li></ul><p>总体而言，超人模型长什么样、有什么现象现在仍未可知。作者提到，我们更需要对于未来的所有模型都跑一跑现在的实验，观察一下实验结论是否是稳定存在于所有模型中的……怎么有一种三体人的水滴在破坏物理学规律的感觉……</p><h2 id="experiment">experiment</h2><p>作者在附录里讲了实验细节，大致有如下关键点：</p><ul><li>NLPtask中，作者选取了29个有代表意义的task，都转换成了2分类任务，为了简化整个setting。</li><li>在模型方面，作者使用GPT4架构，构造了和GPT2、GPT3、GPT3.5、GPT4相当计算量的模型(但都是GPT4架构)。作者去掉了decoder最后的那个unembeddinglayer，换成了一个二分类头做实验</li><li>数据方面，作者把数据做了一些清洗，使得数据集中两个类的标签数量接近，然后有数据分类风险的数据们都在traintest的其中一个split上</li></ul><p>另外两个setup：</p><ul><li>rewardmodel：作者直接使用了训练ChatGPT的SFT数据集，里面有数据对以及human给出的偏好。可以看成一个很复杂的二分类任务。这个的训练方法和正常的rewardmodel一样，把unembeddinglayer改成一个N-&gt;1的linear来输出一个float当做score。最后用两个candidate的score谁大来作为预测</li><li>chesspuzzle：很多象棋的棋局，作者希望模型给出第一步，最优解是什么。这个可以看做一个生成任务，因为可选位置有很多个。原始数据大概长下面这样</li></ul><p><img src="../../files/images/weak2strong/chess.png" style="zoom:20%;" ></p><blockquote><p>我猜测，作者选这几个setup是基于这样的考虑：</p><p>NLP task，大小模型做的都不错。</p><p>chess puzzle，大模型做的不错，小模型做不了。</p><p>reward model，大小模型做的都不太好</p></blockquote><p>由此，作者</p><ul><li>首先构造了大小不同的多个GPT4架构的模型。这些模型都是做过预训练的，使用的预训练数据和GPT4一致</li><li>对于每个setting，作者分别跑了刚才提到的四步测试<ul><li>小模型在trainset真实标签上做训练FT</li><li>FT的小模型给trainset重新打一遍伪标签</li><li>大模型在带噪的trainset上训练，同时做一个真实标签FT的上限训练</li><li>测量三个模型在testset的表现，计算出PRG</li></ul></li></ul><p>论文的主试验就是下面这个图</p><p><img src="../../files/images/weak2strong/main.png" style="zoom:50%;" ></p><p>这个看着有点费解，后面的所有图都用了相同的画法，我在这里解释一下：</p><ul><li>纵着看，每列代表一个task。(29个NLP task被划到一起了 )</li><li>横着看，上面三个图报告的是数据集上的performance，下面三个图报告的是PRG</li><li>每个图里面的横轴说的是学生模型的大小，标准是GPT4相对计算量。注意，学生模型一定比老师强</li><li>图里的一条线代表的是同一个老师监督不同大小的学生的水平。老师越大，线越亮。注意，老师永远小于学生，因此这个图差不多都在右下三角半区</li><li>上面的三个图里最上面的线是大模型的上限实验结果。</li></ul><p>作者观察到几个关键结论：</p><ul><li><p>PRG基本永远大于0，学生总是强过老师。</p></li><li><p>老师变大或者学生变大，PRG基本都会增大。对于最强的学生，PRG甚至会超过50%：这也许预示着，对于超人模型来说，weak-to-stronggeneralization实际上很简单</p></li><li><p>chesspuzzle任务上，这些现象更显著。当学生很小时，PRG接近0，大小模型做的都很垃圾。并且，这个任务上的PRG和前面相反，出现了inversescaling现象？即同一个老师，学生越强，PRG反而越低了？</p></li><li><p>对于rewardmodel任务，PRG一直都很低。对于所有setting，PRG都低于20%</p></li></ul><p>作者特别强调了一下，PRG&gt;0这个事情基本一直成立，这个现象其实很神奇，因为也没做什么特别的优化。这个现象预示着superalignment的可能性，也需要在相关领域有更多的研究</p><h2id="如何提高weak-to-strong-generalization的表现">如何提高weak-to-stronggeneralization的表现？</h2><p>做完主试验，一个显然的思路就是：如何优化他？作者在这里简单地想了几个办法。在这里，我们看一下OpenAI研究员的"注意到"是什么样的吧？</p><h3 id="bootstrapping级联">bootstrapping：级联</h3><p>注意到：刚才提到老师和学生差距太大，以及老师/学生提升，PRG效果都会变好。那么，如果用小老师监督稍微强一点的学生，再把学生当成新老师，监督更强一点的学生……如此反复，效果会更好吗？</p><p><img src="../../files/images/weak2strong/bootstrap.png" style="zoom:50%;" ></p><p>作者在chesspuzzle进行了实验，图里面的虚线是主试验里面的小老师直接监督大学生，实现是级联监督的结果。作者发现：</p><ul><li>当最开始的大小gap差距小时，这个办法是优化</li><li>当最开始的大小gap差距大时，这个办法是负优化</li></ul><p>尽管在chess puzzle看起来效果不错，但作者发现这个办法在NLPtask提升不大，在rewardmodeling没有提升。对于这个现象。作者"注意到"：主试验中，Chess这个场景gap越大PRG越小，其他场景都是gap越大PRG越大。因此级联方法缩小gap，对于Chess才有帮助。</p><blockquote><p>可是，主试验又为什么会有这样的现象呢？</p></blockquote><h3 id="auxiliary-confidence-loss-学生要学会质疑">auxiliary confidenceloss: 学生要学会质疑</h3><p>注意到：强模型是在弱模型打得标签上训练的，因此强模型不得不去学习弱模型犯的错误。我们更希望强模型从噪声标签中学习任务的intension，而不是学习弱模型的错误(因为对强模型来说，理解弱模型犯的错误也是一个简单的事情)。但目前的训练框架没法区分这两者。</p><p>作者想了一个朴素的办法，把强模型自己的confidenceloss加入进去作为辅助loss，和弱模型的标签loss形成一个对抗，对冲一下"学习犯错"带来的影响。<span class="math display">\[L&#39; = (1-\alpha)\text{CE}(f(x), f_w(x)) + \alpha \text{CE}(f(x),\hat{f}_t(x))\]</span> 其中<spanclass="math inline">\(f_w(x)\)</span>是老师的logits， <spanclass="math inline">\(\hat{f}_t(x)=I[f(x)&gt;t]\)</span>是给学生模型的预测按照阈值t做了离散化后的标签。<spanclass="math inline">\(\alpha,t\)</span>都是超参数</p><p>注意到，上面的公式等价于下面这个，思路和self-training有点类似 <spanclass="math display">\[L&#39; = \text{CE}\left(f(x), (1-\alpha)f_w(x)+\alpha\hat{f}_t(x)\right)\]</span>这个新loss相当于噪声标签和学生自己打的标签的一个加权平均。最开始的时候学生啥都不会，应该把<spanclass="math inline">\(\alpha\)</span>调小，后面再慢慢增大，作者在实验中就在前20%训练中把<spanclass="math inline">\(\alpha\)</span>从0提升到0.75，后面维持0.75不变</p><blockquote><p>如果老师和学生模型的算法相同，也许可以不再标签层面做平滑，可以直接用logits做平滑，去掉一个超参数t。作者估计是为了保证scaling的通用性，就没这么优化。</p></blockquote><p><img src="../../files/images/weak2strong/aux.png" style="zoom:50%;" ></p><p>类似上面，作者在NLP任务上做了实验，发现这个优化对于NLP任务的PRG提升很明显，甚至从25%提升到了80%……作者认为这个结论是符合直觉的：auxloss可以减缓学生在"weak errorlearning"上的计算资源，减缓对于老师的错误的拟合</p><p>上篇也写了4000多字了，就讲到这里，在下篇我会分享作者对于这些现象，以及superalignment问题的理解，再说一说我对这些问题的看法，以及我对于这篇论文研究思路的思考。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;昨天OpenAI一口气更新了两篇论文，暨DALL.E
3之后的又一更新，其中一篇讲述了一个朴素的问题：如果未来的模型超越人类了，我们该怎么给他们提供监督信号？（毕竟我们只有人类——一个相对更弱的模型）&lt;/p&gt;
&lt;p&gt;OpenAI把这个问题叫做weak-to-strong
generalization在这里做了一些简单的尝试，对于这个问题的性质进行了一些探索。我们来一起学习一下他们看问题和解决问题的思路吧！&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="superhumanAI" scheme="https://www.yynnyy.cn/tags/superhumanAI/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</title>
    <link href="https://www.yynnyy.cn/68247c98"/>
    <id>https://www.yynnyy.cn/68247c98</id>
    <published>2023-11-28T07:34:13.000Z</published>
    <updated>2024-08-09T09:26:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>读得论文多了，写的笔记反而更少了……很多篇论文都想写，最后哪个都没写出来。今天来讲讲yejinChoi2020年的一个论文：如何用模型自己在训练过程中的表现作为自监督信号，衡量训练集中每一条数据的质量？</p><blockquote><p>很难想象这是yejinchoi三年前思考的问题，我直到最近读到这篇论文，还觉得思路很新颖、很精妙</p></blockquote><span id="more"></span><p>作者是Yejin Choi团队,一作SwabhaSwayamdipta最近还做了一些有趣的工作，比如这篇：We’re Afraid LanguageModels Aren’t Modeling Ambiguity。都是挺有意思的选题</p><p><img src="../../files/images/Dataset-Cartography/authors.png"></p><h2 id="data-map">data-map</h2><p>回到本篇工作，作者主要探索了以下问题：目前(2020年)学界的范式是选择越来越大的数据集做训练。因为大家发现随着数据集扩大，其多样性会上升，进而促进模型的分布外泛化能力。</p><p>但是，随着数据集的扩大，数据质量一定会下降，作者想到:有没有可能数据集中每条数据对语言模型的贡献是不一致的呢？作者希望找到一种自动地标注方案。作者直觉地想要用两个维度对数据分类：在一条数据过了很多epoch以后每次的loss对应的平均数和方差。作者把这两个轴叫做confidence(平均数)、variability(方差)</p><p>对于比如SNLI数据集，作者尝试把RoBERTa训了几个epoch，然后统计里面每条数据在每个epoch的loss，进而画了一个散点图，其中每个点代表一条数据。作者直觉地认为，这个类似钟型曲线天然地把数据分成了三种情况：</p><ul><li>easy-to-learn：很快就学会了，并且方差很小，一直都做对。占大多数</li><li>hard-to-learn：一直学不会，因此方差也很小</li><li>ambiguous：一轮能做对一轮做不对，方差很大。模型对这种数据的判断没有把握</li></ul><p><img src="../../files/images/Dataset-Cartography/intro.png"></p><p>另外，对于confidence做离散化，还可以统计acc。作者还把“n个epoch中一条数据acc”的比例定义为了correctness，在图中表现为了不同颜色的小点。</p><p>由此，作者把这个方法叫做data-map，和标题里的地图学呼应：地图是地球固有的属性，而数据中的confidence、variability也是模型在训练中自己表现出来的性质。</p><p>接下来，作者就要从这个现象出发，展露一下研究员的天才思路，设计一系列实验和探索。</p><h2id="data-map能作为选择训练数据的指标吗">data-map能作为选择训练数据的指标吗？</h2><p>作者实现好奇的就是：不同区域的数据，对于训练有什么贡献？实验设计很简单，就只选择对应区域的数据做训练就可以了。在训练完以后，作者分别作了in/outof- distribution(ID、OOD)的测试。</p><ul><li><p>100 train：阳性对照</p></li><li><p>random: 随机选33%，阴性对照</p></li><li><p>high-correctness: correctness从高到低前33%的数据</p></li><li><p>low-variability、low-correctness、high-confidence、high-confidence同理</p></li><li><p>hard-to-learn: 指的是low-confidence</p></li><li><p>ambiguous： 指的是high-variability</p></li></ul><p><img src="../../files/images/Dataset-Cartography/exp.png" style="zoom: 33%;" ></p><p>作者在winoG上训练，然后分别把winoG、WSC作为IDOOD测试，神奇的现象来了：</p><ul><li>仅在hard-to-learn或者ambiguous的33%数据上训练，OOD能力甚至比阳性对照还要好！</li><li>仅在eazy-to-learn的数据上训练，似乎对ID和OOD测试都没啥帮助……不如random33%</li><li>尽管没有对选数据的方法专门做优化，但效果比几个active-learning算法的效果还要好</li></ul><p>看起来，hard-to-learn和ambiguous的数据对模型的效果起到关键作用。ID的效果和训练集大小强相关，我们相对更关注OOD。因此作者说到这套data-map的方案某种意义上提供了一个加速训练的潜在方案。然而，从这个角度看，这个方案需要先在全集上训一遍模型，这肯定比正常训练开销更大。因此这个方法只有理论价值</p><h2 id="可以抛弃eazy-to-learn数据吗">可以抛弃eazy-to-learn数据吗？</h2><p>既然上面研究发现hard-to-learn和ambiguous数据最有用，那接下来一个直观的问题就是：如果用更少、少于33%的这种数据，也能达到这种效果吗？</p><p>于是作者选了ambiguous数据的前50%, 33%, 25%, 17%, 10%,5%，1%作为训练集尝试了实验</p><p><img src="../../files/images/Dataset-Cartography/rate.png"></p><p>先看左边两个图：横轴是上面那个top-ambigious训练数据的百分比，纵轴分别是ID和OOD的效果。神奇的又来了：当训练数据低于某个阈值以后，训练就崩溃了？？另一个实验表明，相同的数据量，如果选取不是按照top-ambigious而是random，训练就是正常的</p><p>因此作者想到了一个可能：会不会是eazy-to-learn的数据虽然对于效果没什么帮助，但是对于稳定训练很有帮助？因为更少的top-ambigious显然就采样不到eazy-to-learn的数据了。于是作者点子又来了，做个阴性对照，把刚才训崩的数据比例(17%)里，随机将一部分top-ambigious的数据换成eazy-to-learn的数据？</p><p>于是就画出了右图：作者发现，哪怕在17%中，只要再掺入1/10=1.7%的eazy-to-learn数据，训练就正常了起来？？另外，如果替换的比例太高，ID和OOD的效果就又掉下去了。</p><p>作者最后又提出了一个开放性的研究问题：如何在训练中正确选择各个区域的比例？</p><h2id="hard-to-learn的数据可能因为误标注吗">hard-to-learn的数据可能因为误标注吗？</h2><p>想到两个点：</p><ul><li>SNLI画的data-map中hard-to-learn很多，winoG画的data-map，hard-to-learn看起来很少。同时我们知道winoG中的数据被人类精心clean过因此误标注更少</li><li>对于误标注的数据，模型显然是"hard-to-learn"的</li></ul><p>怎么验证这个猜测呢？作者点子又来了</p><p>首先，来个模拟实验。作者将winoG中1%的eazy-to-learn数据的标注换一下，造一批”误标注“数据。在eazy-to-learn数据中采样是因为这里面大概率之前不是误标注的数据</p><p><img src="../../files/images/Dataset-Cartography/mislabel.png"></p><p>接着作者用新的数据集重新画data-map，观察刚才那些点在新的图中的位置，作者给出了这些点confidence、variability的直方图。发现confidence显著降低、variable显著升高。这展示了数据中误标注的可能性</p><p>接下来，作者问了另一个问题：既然有潜在的误标注风险，那有可能将data-map作为一种自动的误标注识别手段吗？</p><p>首先作者把刚才的数据集(含1%人造误标注数据)，再采样了同样的1%正常数据形成了一个误标注占50%的数据集。训练一个classifier，其输入是每个instance的confidence、variability，输出2分类。发现这个classifie的测试集F1是100%？？</p><p>接下来，作者将classifier重新应用到原始winoG数据集，发现31/40k的数据划分为了mislabel。同理在SNLI上做同样的实验，发现有15k/500k划分为了mislabel。这和两个数据集的数据质量一致</p><p>最后，为了让作者的逻辑链条完整，作者开展了人类实验，找人去看classifier划分出来的mislabel数据。人类标注结果表明:classifier选出的"mislabel"数据，67%是真的mislabel。这个数字在SNLI上是76%。剩下的基本上也是比较"歧义"的instance</p><p>最后，作者谈到：data-map可以作为一种潜在的对数据集mislabel问题进行自动检测的手段，并且效果还不错。</p><h2id="模型在训练中表现出来的这种性质和数据集固有的不确定性有关吗">模型在训练中表现出来的这种性质，和数据集固有的不确定性有关吗？</h2><p>众所周知，数据集中有一些固有不确定性：有一些instance是歧义的，理论上就是填什么都可以。另外，对于模型无法预测的位置，到底是来源于数据集固有的不确定性，还是模型本身的局限性(换个更强的模型没准就会了)呢？</p><p>作者想到一个办法来衡量数据集中固有的不确定性：在数据集制作时，都是找人来标注。对于本身有歧义的例子，不同的标注员之间应该自己也有不一致性。所以作者分析了data-map中每条数据，列出了标注员当时对于这条数据的一致性</p><p><img src="../../files/images/Dataset-Cartography/consistency.png"></p><p>作者发现，模型划分data-map的方式，和人类当时标注时的一致性有非常强的相关性:起码对于eazy-to-learn数据，标注员基本一致性都很高。</p><h2 id="我的思考">我的思考</h2><p>这个论文的逻辑太顺了：一般我写笔记都会简略写experiment部分，但这次我一个都没有省，并且组织逻辑和YejinChoi论文组织逻辑完全一致。</p><p>作者从一个现象出发，和学界已经存在的问题联系起来，探索他们发现的现象的潜在应用价值。从联系方向，到提出问题，到设计实验，到画图展示的形式，都展示了研究员敏锐的数据直觉，值得我们去学习……相比之下，再看看近两年的大多数论文写成啥样子了……</p><p>站在2023年的视角下，我只能说对这个论文提出几个潜在的研究问题：</p><ul><li>在instructiontuning领域，大家逐渐意识到diversity和quality的矛盾，以及对最终训练效果的影响。相比于WizardLM这种自动化的数据筛选。让模型自己去选择数据是一种新的思路吗？</li><li>data-map的结果是和模型绑定的。对于同一个数据集，换一个模型可能画出来的图就会有变化。比如GPT4，可能在SNLI上画的图全是eazy-to-learn。这点对于选择数据至关重要：一条数据不适合这个模型，但有可能适合那个模型，这和模型的基础能力有关。我们不指望找到一个适用于所有模型的goldenselectionmethod(可能世界上也不存在这样的方法)，相比之下更希望能找到最适合与这个模型的训练数据</li><li>这两年学界出现了一个新的关键词calibration：对于很强的LLM来说，自己的confidence和acc成强相关性。作者在这片工作中发现另一个联系：自己的confidence和数据集的固有不确定性成高度的相关性。由此我产生了一个问题：既然三者都有相关性，那么，模型的calibration性质可能是来源于"在含固有不确定性的无监督corpus上预训练"吗？如果我们的corpus去掉了不确定性(比如RLHF数据集)，那么模型的calibration性质是不是就消失了呢？</li></ul><p>最后，这是Yejin Choi三年前研究的东西，与君共勉</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;读得论文多了，写的笔记反而更少了……很多篇论文都想写，最后哪个都没写出来。今天来讲讲yejin
Choi
2020年的一个论文：如何用模型自己在训练过程中的表现作为自监督信号，衡量训练集中每一条数据的质量？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;很难想象这是yejin
choi三年前思考的问题，我直到最近读到这篇论文，还觉得思路很新颖、很精妙&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>LLaVA, LLaVA 1.5和LLaVA-Plus: 讲讲LMM</title>
    <link href="https://www.yynnyy.cn/d33e88af"/>
    <id>https://www.yynnyy.cn/d33e88af</id>
    <published>2023-11-11T03:15:21.000Z</published>
    <updated>2024-08-09T09:26:46.970Z</updated>
    
    <content type="html"><![CDATA[<p>昨天刷到新挂的LLaVA-Plus的Arxiv论文，讲怎么做多模态的ReACT与训练模型。正好发现LMM(LargeMultimodalModel)系列的模型似乎怎么讲过。那么LLaVA系，三篇论文，今天一次说完。</p><blockquote><p><a href="https://llava-vl.github.io">Visual InstructionTuning</a></p><p><a href="https://llava-vl.github.io">Improved Baselines with VisualInstruction Tuning</a></p><p><a href="https://llava-vl.github.io/llava-plus/">LLaVA-Plus: Learningto Use Tools for MulitModal Agents</a></p></blockquote><p><del>flamingo、Kosmos 2.5下次有时间说啊</del></p><span id="more"></span><p><img src="../../files/images/LLaVA/authors.png"></p><p>首先，在作者上，这三篇论文基本上是一脉相承，没有出现LLaMA的黑吃黑现象。他们的鼻祖LLaVA一代发表在neurIPS2023Oral，上Arxiv的时间是4月份。当时LLM基本还是蛮荒时期，大家都是被GPT-4V发布时的惊艳骗进来，想搞个猴版，技术路径和研究思路有迹可循。不像现在成熟以后，论文都是天马行空地说，很难把握住核心思想。</p><h2 id="llava">LLaVA</h2><p>所谓的large multimodal model,就是想把LLM的能力范围再往前推一步，让他可以"see and hear".</p><p>LLaVA的主要思路是：由用一个CLIP作为imageencoder，然后训一个轻量级的链接器，把clipembedding连接到一个LLM的空间，由此让一个LLM理解图片，进而变成LMM。</p><p>上面的这套流程，重点就是需要图文数据集，而且需要是instruction-follow数据集。目前的图文数据对大多是imagecaption的，文字主要是描述文字内容。另外有一些VQA的数据集，其问答式针对图片里的一些元素，总体还是比较简单。</p><p>LLaVA的作者对上面的思路做了一下梳理，构造了一个数据集。里面所有的数据的格式都是类似于下面这样<span class="math display">\[X_q X_v&lt;STOP&gt;\text{\\n Assistant} : X_c &lt;STOP&gt;\text{\\n}\]</span>其中最前面会有一个图片，然后会有一个问题，接下来是回答。总体是多轮对话形式的。作者定下了三种数据类型，共158K数据</p><ul><li>conversation：58k</li><li>detailed descrption: 23k</li><li>Complex reasoning: 77k</li></ul><h3 id="model-and-training">model and training</h3><p>模型结构上，非常简单。作者使用CLIP ViT作为imageencoder，用LLaMA作为LLM。然后clip embedding通过一个Linear层投射到wordembedding层。接下来直接将他作为一个word embedding和其他wordembedding一起去跑LLM后面的流程</p><p><img src="../../files/images/LLaVA/model.png"></p><p>作者设计了一个两阶段的训练任务。第一阶段是对齐文本图像空间。作者直接使用图文数据对作为数据集:图在前，文在后。然后训练的时候只训练W的权重。</p><p>接下来是instruction follow阶段。这一部分训练imageencoder和W的权重，用他的158k数据集训练出来的instruction follow模型</p><p><img src="../../files/images/LLaVA/format.png"></p><p>训出来的模型基本都是按照这种形式。可以看到，传入的图片基本上是一个原图，加上一些的形式boundingbox，然后几种数据格式在表现上就是对话。作者在训练的时候，只有answer的部分是有loss的。</p><p>LLaVA是一个很干脆的论文，把一个思路清晰的做了出来，并且比较重视原始数据。</p><h2 id="llava-1.5">LLaVA 1.5</h2><p><img src="../../files/images/LLaVA/1.5performance.png" style="zoom: 33%;" ></p><p>到了二期，论文只有短短5页。作者在方法上没什么更新，只是把LLM基座模型换成了13B，把imageencoder换成了更大更强的CLIP-ViT-L-336px，然后把连接层的Linear换成了双层MLP。</p><p>另外作者还观测到之前LLaVA由于图片分辨率的问题，会看不清楚输入，新的imageencoder可以看得更清楚</p><p>作者从scaling的视角来看他们的方法，提出了一个问题：158k数据够了吗？</p><p><img src="../../files/images/LLaVA/scale.png" ></p><p>作者把数据集混入了一些VQA、OCR的数据，另外对instruction-followingprompt中要求对response给出格式要求，这样更方便模型对学习，比如说：</p><blockquote><p><em>Answer the question using a single word or phrase</em>.</p></blockquote><p>作者在上图中报告了对于数据、模型大小，和图片分辨率做scale后的效果。看起来提升模型大小是涨点最有效的办法</p><p>最后用最大最强的模型去刷了个榜。这篇论文就是经典的二期论文的写法：找到最严重的问题，并修复之。另外，从scale的视角看整个问题，很好的视野。</p><h2 id="llava-plus">LLaVA-Plus</h2><p>在LMM基座模型效果提升了以后，作者瞄准了现在比较火的工具学习场景：能不能让LMM去通过工具调用来进一步提升tasksolving的能力？由此写出了LLaVA-Plus(Plug and Learn to Use Skills)</p><p><img src="../../files/images/LLaVA/intro.png" ></p><p>既然立足是一篇Agent的论文，作者论文写法都变了，用了storyoriented的写法，故事性变得很浓。可见作者的写作功底还是很好的。在Introduction中甚至搬出来了祖师爷"Societyof Mind"的理论(1988)</p><blockquote><p>Society of Mind: each tool is originally designed for a specificskill and by itself is only useful for specific scenarios, but thecombinations of these tools lead to emergent abilities that show signsof higher intelligence.</p></blockquote><p><img src="../../files/images/LLaVA/capbility.png" ></p><p>LLaVA的总体流程和ToolBench非常相似，具体可以看这个<a href="/660d5dc5.html" title="论文阅读[粗读]-TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS">这个笔记</a>。大致上就是</p><ul><li>先找了一些target image</li><li>找GPT4对image造出来一个query too observationl answer这样的tuple</li><li>把这个tuple弄成一个数据集。以此作为训练数据，训练出来LLaVA-Plus</li></ul><p><img src="../../files/images/LLaVA/tool.png" ></p><p>具体的数据格式看起来和ReACT完全一致，经典的thought、action、observation、answer。作者一共制作了一个LLaVA-158K的数据集，另外把测试集搬出来做了一个叫LLaVA-Bench的测试系统。</p><p>作者说明，训练出来的模型达到SOTA水平。</p><h2 id="我的思考">我的思考</h2><p>可以看到，从今年4月走到11月，作者在LLaVA的道路上一路深耕，提高基础能力。再基础能力提上去以后，逐渐做到Agent能力。估计后续随着能力进一步提高，也许可以尝试多步工具调用以及多模态planning。LLaVA是一种lightweight的多模态连接方式，对textencoder和image encoder的模型结构都不做要求，从结果来看，效果还挺好。</p><p>为什么不在预训练阶段就用多模态的模型？一方面，作者在论文里说到的一个问题其实很有道理：目前的多模态数据主要就是图文caption对，这样的数据可以让模型去理解图片，但也没有进一步的能力需求了(不像纯文本数据那样需要推理等)，即使是VQA也以简单的数数、区分左右等等为主。训练数据决定模型学到的能力，我们可能得找到更好的多模态预训练数据。另一方面，多图多文结合也是一条路子。像GPT-4v就是天生多图的，这样的多图数据某种意义上和多步推理有着更紧密的联系。</p><p>最后，我很喜欢scaling的视角，我觉得scaling的结论是最可信的结论，也最有可能是未来大规模应用的前提。也不知道以后的LMM到底是单模态模型的整合，还是预训练级的多模态……</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;昨天刷到新挂的LLaVA-Plus的Arxiv论文，讲怎么做多模态的ReACT与训练模型。正好发现LMM(Large
Multimodal
Model)系列的模型似乎怎么讲过。那么LLaVA系，三篇论文，今天一次说完。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io&quot;&gt;Visual Instruction
Tuning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io&quot;&gt;Improved Baselines with Visual
Instruction Tuning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io/llava-plus/&quot;&gt;LLaVA-Plus: Learning
to Use Tools for MulitModal Agents&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;del&gt;flamingo、Kosmos 2.5下次有时间说啊&lt;/del&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多模态" scheme="https://www.yynnyy.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="tool-learning" scheme="https://www.yynnyy.cn/tags/tool-learning/"/>
    
  </entry>
  
  <entry>
    <title>OpenAI开发者大会的所有可能结局</title>
    <link href="https://www.yynnyy.cn/75adad28"/>
    <id>https://www.yynnyy.cn/75adad28</id>
    <published>2023-10-21T12:44:18.000Z</published>
    <updated>2024-08-09T09:26:46.945Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，OpenAI打算在2023/11/6，ChatGPT问世(2022/11/30)大约1一年以后，召开第一届开发者大会，距离现在还有15天。我们不如来大胆预测一下开发者大会可能更新的所有内容吧！即是预测，也是我对OpenAI接下来开发的功能的期望。你觉得哪种结局最有可能呢？</p><blockquote><p>所有图片均由DALL·E 3生成</p></blockquote><span id="more"></span><h2 id="大一统结局">大一统结局</h2><p>目前的ChatGPTPlus我们需要在各种实验性功能中选择一个使用，大家都猜测这背后是GPT4在各种下游任务中特化的finetune版本。大家现在每次只能选一个，直接选择困难症，哪个都想要，但不能同时存在于一个context下。</p><p>有没有可能在11.6开发者大会中，OpenAI大一统所有checkpoint，将会使用一个统一的接口做完所有事情：有视觉可以看图，也能上网，能使用工具，还能做jupternotebook执行，最后能用DALL·E3画图(画出来的图也可以直接用视觉去理解)。</p><blockquote><p>大一统结局：ChatGPT Plus的选择困难症可以休矣</p></blockquote><figure class="half"><img src="../files/images/all_ends_for_11.6/consistence.png" width="35%"/><img src="../files/images/all_ends_for_11.6/c2.png" width="35%" /></figure><h2 id="模态联结结局">模态联结结局</h2><p>Google准备训练Gemini多模态大模型抢OpenAI的风头，据知情人士透露：OpenAI准备在Gemini上线之前训练一个多模态大模型Gobi来对抗Google的竞争。如果开发者大会上，Gobi已经训练完成了，Google还有后手吗？</p><blockquote><p>模态联结结局：Gobi is all you need</p></blockquote><p><img src="../files/images/all_ends_for_11.6/gobi.png" style="zoom:50%;" ></p><h2 id="openagent结局">OpenA(gent)结局</h2><p>Agent技术目前非常火，过了大约半年，一直没看到OpenAI出手。按照其一惯逻辑，在找到一以贯之的思想之前，他们可能会做总结对比的工作。就像强化学习一样，OpenAI有没有可能推出来一个Agent-gym框架，从此大家开发Agent都是基于Agent-gym的接口和设计理念。</p><blockquote><p>OpenA(gent)结局：OpenAI的阴影笼罩整个Agent研究。</p></blockquote><p><img src="../files/images/all_ends_for_11.6/agent.png" style="zoom:50%;" ></p><h2 id="true-openai结局">True OpenAI结局</h2><p>虽然之前业界普遍承认最强模型是GPT4，但开源和实际构建应用时大家会选择Llama和Llama2为主。OpenAI事实上并没有真正的Open，在开源界被MetaAI统治了。</p><p>因此之前在流传小道消息：OpenAI打算开源一个语言模型，代号是G3PO，也许开发者大会就是G3PO问世的时间，OpenAI用一年时间转形成了TrueOpenAI</p><blockquote><p>True OpenAI结局：你的llama3，何必是llama</p></blockquote><p><img src="../files/images/all_ends_for_11.6/TrueOpenAI.png" style="zoom:50%;" ></p><h2 id="超人主义结局">超人主义结局</h2><p>OpenAI开启了SuperAlignment小组，准备在2030年之前实现AGI。按照他们一贯是先研究、再宣发的特性，保守估计他们在2024年就研究完成了AGI，后面做5年的对齐工作。如果在开发者大会上，他们让AGI技术初步亮相，会不会进一步推进全世界的研究热情呢？</p><blockquote><p>超人主义结局：北大的通班不用再办了</p></blockquote><p><img src="../files/images/all_ends_for_11.6/AGI.png" style="zoom:50%;" ></p><h2 id="硅基飞升结局">硅基飞升结局</h2><p>OpenAI部署各种GPT服务，可能是世界上最缺GPU的人。之前听说OpenAI在研究自研AI芯片，如果OpenAI已经研究出来了AI芯片，可以把GPT4加密打印在门电路里，以后只要买到了GPT4-i7芯片，就能直接通过芯片激活来做推理了。每个时钟周期就是一个流水推理周期，64个周期就能推完64层transformer</p><blockquote><p>硅基飞升结局：人类只不过是一段boosting程序，引导硅基生命的到来。</p></blockquote><p><img src="../files/images/all_ends_for_11.6/chip.png" style="zoom:50%;" ></p><h2 id="one-more-thing结局">One More Thing结局</h2><p>6个月前的WWDC上，库克在发布的结束用一句"one morething"引出了最重要的Apple VisionPro发布。OpenAI会不会也在憋一个"终极大招"等着大家在开发者大会最放松警惕的时候引出来。比如有人统计了目前模型的运行速度，发现随着时间推移变得越来越快。有没有可能OpenAI找到了一种把稀疏大模型同时变得稠密的办法。最后来一句王炸。</p><blockquote><p>One MoreThing结局：通过最新的训练方法，我们成功找到了用100M稠密模型比肩100B稀疏模型的办法。</p></blockquote><p><img src="../files/images/all_ends_for_11.6/turbo.png" style="zoom:50%;" ></p><h2 id="快进结局">快进结局</h2><p>如果……上面的一切同时发生呢？</p><blockquote><p>快进结局：开发者大会马蹄疾，一日看尽长安花！</p></blockquote><p><img src="../files/images/all_ends_for_11.6/fast.jpg"  ></p><p>那么，你觉得哪一种结局最有可能呢？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;众所周知，OpenAI打算在2023/11/6，ChatGPT问世(2022/11/30)大约1一年以后，召开第一届开发者大会，距离现在还有15天。我们不如来大胆预测一下开发者大会可能更新的所有内容吧！即是预测，也是我对OpenAI接下来开发的功能的期望。你觉得哪种结局最有可能呢？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有图片均由DALL·E 3生成&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>2023-09-29总结</title>
    <link href="https://www.yynnyy.cn/6c02904f"/>
    <id>https://www.yynnyy.cn/6c02904f</id>
    <published>2023-09-29T11:06:44.000Z</published>
    <updated>2024-08-09T09:26:46.966Z</updated>
    
    <content type="html"><![CDATA[<p>今天第一次尝试将Arxiv最新论文同步到博客。</p><p>扫描Arxiv的工作现在基本每天都做，最开始可能还要追溯到两年多前。曾经用过各种各样的方式完成这件事：</p><ul><li>最开始是超哥带着大家每天扫描，每人按日期做分工</li><li>后面一段时间我自己每天刷一刷</li><li>后来形成习惯了，要写一个飞书文档同步进去，后来觉得太麻烦，最后就不了了之了</li></ul><p>从今天开始，试着每天把新扫描到的有趣的论文更新到博客，看看大家的反应如何。可能一个良性的循环是：一方面有人反馈我有遗漏，或者推荐哪篇论文，我就可以仔细看看，或者写一些阅读笔记。</p><span id="more"></span><p>我读论文、扫描Arxiv论文，按照优秀程度大致分为几个粒度：</p><ul><li>最差的是点都不会点进去</li><li>好一些的会点进去看看abstract和作者</li><li>再好点的我会放到Arxiv Insights里</li><li>再好点的我会加入 paper reading TODO list</li></ul><p>写论文阅读笔记是这样：</p><ul><li>读完论文，觉得很好玩的东西，我会加入到blog TODO list</li><li>每次有时间的时候，就会写一篇阅读笔记</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天第一次尝试将Arxiv最新论文同步到博客。&lt;/p&gt;
&lt;p&gt;扫描Arxiv的工作现在基本每天都做，最开始可能还要追溯到两年多前。曾经用过各种各样的方式完成这件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最开始是超哥带着大家每天扫描，每人按日期做分工&lt;/li&gt;
&lt;li&gt;后面一段时间我自己每天刷一刷&lt;/li&gt;
&lt;li&gt;后来形成习惯了，要写一个飞书文档同步进去，后来觉得太麻烦，最后就不了了之了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从今天开始，试着每天把新扫描到的有趣的论文更新到博客，看看大家的反应如何。可能一个良性的循环是：一方面有人反馈我有遗漏，或者推荐哪篇论文，我就可以仔细看看，或者写一些阅读笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
</feed>
