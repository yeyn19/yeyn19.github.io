<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>随缘随笔 &lt;/br&gt; Insights Flow</title>
  
  
  <link href="https://www.yynnyy.cn/atom.xml" rel="self"/>
  
  <link href="https://www.yynnyy.cn/"/>
  <updated>2025-05-24T03:50:13.290Z</updated>
  <id>https://www.yynnyy.cn/</id>
  
  <author>
    <name>叶奕宁 &lt;/br&gt; Yining_Ye</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2025-05-20-insights</title>
    <link href="https://www.yynnyy.cn/c95d732d"/>
    <id>https://www.yynnyy.cn/c95d732d</id>
    <published>2025-05-22T10:54:47.000Z</published>
    <updated>2025-05-24T03:50:13.290Z</updated>
    
    <content type="html"><![CDATA[<p>前无古人后无来者的700篇新论文……刷论文硬控了我3天</p><h2 id="Mean-Flows-for-One-step-Generative-Modeling"><a href="https://arxiv.org/pdf/2505.13447">Mean Flows for One-step Generative Modeling</a></h2><p>万文丛中，一眼挑出来kaiming大佬的工作。其实我没看懂，看起来作者改进flow model的计算方式。</p><blockquote><p>kaiming最近的几篇工作，好像都是focus在快速image generation方向上</p></blockquote><img src="../../files/images/arxiv-insights/2025-05-19-05-23/flow.png" ><h2 id="Visual-Agentic-Reinforcement-Fine-Tuning"><a href="https://arxiv.org/pdf/2505.14246">Visual Agentic Reinforcement Fine-Tuning</a></h2><p>类o3的一个尝试。作者做了vlm+tool的setting，如果在通用VLM的推理任务上，让模型可以写代码放大图片、可以调搜索引擎，在rl的过程中会变得更好吗？作者在这个setting上跑出了正收益，分享了一些认知</p><blockquote><p>openai做啥我做啥</p></blockquote><img src="../../files/images/arxiv-insights/2025-05-19-05-23/arft.png" ><h2 id="KORGym-A-Dynamic-Game-Platform-for-LLM-Reasoning-Evaluation"><a href="https://arxiv.org/pdf/2505.14552">KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation</a></h2><p>seed的动作，和我们的一个早期探索有关。通过大量的工程开发，作者把很多尝见的逻辑游戏重写成了纯文本的形式，允许模型通过多轮交互的形式给出action，同时也会给出游戏引擎返回的每一轮分数。有了这个framework以后，后面无论是评测还是rl，都会变得很快捷。</p><blockquote><p>新时代的数据资产</p></blockquote><img src="../../files/images/arxiv-insights/2025-05-19-05-23/korgym.png" ><h2 id="Scaling-Computer-Use-Grounding-via-User-Interface-Decomposition-and-Synthesis"><a href="https://arxiv.org/pdf/2505.13227">Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</a></h2><p>质量很高的工作。作者自己讨论了已有gui agent领域的perception和grounding的问题，由此构造出了一个新的osworld-G benchmark，更关注在grounding能力的评测上。甚至，作者还开源了巨大的grounding数据集，以此将已有模型在perception层面上拔升到了operator-level。</p><blockquote><p>形式有点像之前multi ui，但是做到了全平台场景。感觉是我看到过的最好的gui数据集了</p></blockquote><img src="../../files/images/arxiv-insights/2025-05-19-05-23/jedi.png" >]]></content>
    
    
    <summary type="html">&lt;p&gt;前无古人后无来者的700篇新论文……刷论文硬控了我3天&lt;/p&gt;
&lt;h2 id=&quot;Mean-Flows-for-One-step-Generative-Modeling&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2505.13447&quot;&gt;Mean Flows for One-step Generative Modeling&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;万文丛中，一眼挑出来kaiming大佬的工作。其实我没看懂，看起来作者改进flow model的计算方式。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;kaiming最近的几篇工作，好像都是focus在快速image generation方向上&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&quot;../../files/images/arxiv-insights/2025-05-19-05-23/flow.png&quot;&gt;
&lt;h2 id=&quot;Visual-Agentic-Reinforcement-Fine-Tuning&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2505.14246&quot;&gt;Visual Agentic Reinforcement Fine-Tuning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;类o3的一个尝试。作者做了vlm+tool的setting，如果在通用VLM的推理任务上，让模型可以写代码放大图片、可以调搜索引擎，在rl的过程中会变得更好吗？作者在这个setting上跑出了正收益，分享了一些认知&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;openai做啥我做啥&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&quot;../../files/images/arxiv-insights/2025-05-19-05-23/arft.png&quot;&gt;
&lt;h2 id=&quot;KORGym-A-Dynamic-Game-Platform-for-LLM-Reasoning-Evaluation&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2505.14552&quot;&gt;KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation&lt;/a&gt;&lt;/h2&gt;</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2025-05-19-insights</title>
    <link href="https://www.yynnyy.cn/acaa3ee6"/>
    <id>https://www.yynnyy.cn/acaa3ee6</id>
    <published>2025-05-20T01:23:06.000Z</published>
    <updated>2025-05-22T10:51:19.768Z</updated>
    
    <content type="html"><![CDATA[<p>最近刷新出来了超多AAAI文风的工作，不知道是不是因为开会期间把没挂arxiv的文章一口气挂出来了</p><h2 id="Is-PRM-Necessary-Problem-Solving-RL-Implicitly-Induces-PRM-Capability-in-LLMs"><a href="https://arxiv.org/pdf/2505.11227">Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</a></h2><p>一篇math场景prm的工作，作者发现已有的math prm一般都没什么正收益，而且在题目本身变得困难时假阳性特别高（容易把错误的题目判成正确）。</p><blockquote><p>似乎rm 假阳性这个问题，每个rm工作都提到了，这背后有什么原因呢？</p></blockquote><img src="../../files/images/arxiv-insights/2025-05-19-05-23/prm.png" >]]></content>
    
    
    <summary type="html">&lt;p&gt;最近刷新出来了超多AAAI文风的工作，不知道是不是因为开会期间把没挂arxiv的文章一口气挂出来了&lt;/p&gt;
&lt;h2 id=&quot;Is-PRM-Necessary-Problem-Solving-RL-Implicitly-Induces-PRM-Capability-in-LLMs&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2505.11227&quot;&gt;Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一篇math场景prm的工作，作者发现已有的math prm一般都没什么正收益，而且在题目本身变得困难时假阳性特别高（容易把错误的题目判成正确）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;似乎rm 假阳性这个问题，每个rm工作都提到了，这背后有什么原因呢？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&quot;../../files/images/arxiv-insights/2025-05-19-05-23/prm.png&quot;&gt;
</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2025-05-16-insights</title>
    <link href="https://www.yynnyy.cn/5da1656b"/>
    <id>https://www.yynnyy.cn/5da1656b</id>
    <published>2025-05-16T04:43:45.000Z</published>
    <updated>2025-05-16T10:39:36.968Z</updated>
    
    <content type="html"><![CDATA[<h2 id="J1-Incentivizing-Thinking-in-LLM-as-a-Judge-via-Reinforcement-Learning"><a href="https://arxiv.org/pdf/2505.10320">J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</a></h2><p>meta的grm工作。作者希望用rl去训练模型打分的能力，因此把一些具有function verifier的题和没有function</p><img src="../../files/images/arxiv-insights/2025-05-12-05-16/j1.png" ><h2 id="Plasticity-as-the-Mirror-of-Empowerment"><a href="https://arxiv.org/pdf/2505.10361">Plasticity as the Mirror of Empowerment</a></h2><p>这篇deepmind的工作讲了两个概念，非常深刻：</p><ol><li>plasticity：从环境变化中学习适配自己的能力</li><li>empowerment：自己的动作对空间的影响能力</li></ol><p>作者发现这两个能力是没法一起优化的。比如说：一只老鼠在迷宫左区，能通过灯光信号学习环境规律（高可塑性），但无法控制灯光开关（低赋权）。同一只老鼠在迷宫右区，如果能完全控制灯光（高赋权），就无法从固定环境中学习新信息（低可塑性）。</p><img src="../../files/images/arxiv-insights/2025-05-12-05-16/empowerment.png" >]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;J1-Incentivizing-Thinking-in-LLM-as-a-Judge-via-Reinforcement-Learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2505.10320&quot;&gt;J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;meta的grm工作。作者希望用rl去训练模型打分的能力，因此把一些具有function verifier的题和没有function&lt;/p&gt;
&lt;img src=&quot;../../files/images/arxiv-insights/2025-05-12-05-16/j1.png&quot;&gt;
&lt;h2 id=&quot;Plasticity-as-the-Mirror-of-Empowerment&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2505.10361&quot;&gt;Plasticity as the Mirror of Empowerment&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;这篇deepmind的工作讲了两个概念，非常深刻：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;plasticity：从环境变化中学习适配自己的能力&lt;/li&gt;
&lt;li&gt;empowerment：自己的动作对空间的影响能力&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者发现这两个能力是没法一起优化的。比如说：一只老鼠在迷宫左区，能通过灯光信号学习环境规律（高可塑性），但无法控制灯光开关（低赋权）。同一只老鼠在迷宫右区，如果能完全控制灯光（高赋权），就无法从固定环境中学习新信息（低可塑性）。&lt;/p&gt;
&lt;img src=&quot;../../files/images/arxiv-insights/2025-05-12-05-16/empowerment.png&quot;&gt;
</summary>
    
    
    
    <category term="Arxiv-Insights" scheme="https://www.yynnyy.cn/categories/Arxiv-Insights/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>重读STaR，与o1随想</title>
    <link href="https://www.yynnyy.cn/7dbea4cd"/>
    <id>https://www.yynnyy.cn/7dbea4cd</id>
    <published>2024-12-14T05:05:36.000Z</published>
    <updated>2024-12-15T07:22:07.425Z</updated>
    
    <content type="html"><![CDATA[<p>半年没写论文阅读笔记，其实笔记草稿写了不少，都没转正。主要觉得像是机械的翻译，没有思想在里面，不如不发。最近大家开始陆陆续续放出来o1-like的模型了，其实翻过头看，大家的思考方式还是几年前的STaR，去年我也写过 <a href="/8622e2d1.html" title="论文阅读[粗读]-STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning">一篇阅读笔记</a> 介绍。</p><p>今天不妨来重新思考一下STaR，连接上跟进的几篇STaR-like的工作，谈谈我对于o1的理解吧。参考文献:</p><ul><li>STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning</li><li>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</li><li>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</li><li>Training Chain-of-Thought via Latent-Variable Inference</li><li>Rest Meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent</li><li>Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering</li><li>Training Language Models to Self-Correct via Reinforcement Learning</li></ul><span id="more"></span><blockquote><p>本文的展开顺序不是上面任何一篇论文的写作思路，而是会有一个自己的行文思路，穿插上面论文的实验和思考。</p></blockquote><p>几年之前，我和一个数竞的学弟交流，我问他：你是怎么学数学的？当时他的回答很有意思，大概是说：</p><blockquote><p>我在脑子里有一套自己的知识组织的方式，和老师教的不太一样。每次做题的时候，我需要先把题目按照自己的那套组织方式&quot;捏&quot;在一起，然后就自然想通了做法，后面的时间主要花在&quot;怎么把自己的想法翻译回证明过程&quot;上。</p><p>这会带来一个问题：每次看到新数学证明的时候，需要花比别人长很多的时间去理解，把他们的整理按照我自己的组织形式去理解一遍。一个定理理解不了，所有用到这个定理的其他定理都成黑盒，也理解不了。还没人可以问</p></blockquote><p>听起来很哲学，像个现编的小故事，但这是个真事。我当时很惊讶：同样一份数学，在不同人眼中可以有不同的定义和理解方式，这些&quot;方式&quot;可以互相翻译、对齐到大家都认可的一个经典文本证明空间去。</p><div style="text-align: right;">——现在想想，这就是o1，或者说STaR。</div><h2 id="“Old-School”-STaR-and-its-reward-hacking-problem">“Old-School” STaR and its reward-hacking problem</h2><img src="../../files/images/STaR/method.png"><p>STaR算法的流程，四句话总结：</p><ol><li>找到一堆数学题（含有题目和答案）</li><li>每道题让模型sample好多次解答</li><li>按照答案正确分去分好坏样本，认为答案正确的样本是good-datapoint。</li><li>把好样本训回去，变成model@2</li></ol><blockquote><p>他的方法其实还有另一半，对于做不出来的题目使用guide-distribution，这半边在后面的follow-up工作中，都验证是无影响甚至负收益，就略过了</p></blockquote><p>没有更多了，就这么简单。听起来很美好，但是STaR其实有几个问题。都是比较本质、不好缓解的问题</p><p><strong>STaR没有对thought做监督</strong>: STaR算法的流程，其实没有核验thought的正确性，而是根据答案的正确性去给thought质量做一个反馈。这里面有个美好的假设：答案正确了，过程一定是对的。</p><p>如果一道题目只训一遍，这就是对的，STaR里确实只迭代了一轮。然而，一般情况下，我们不会遇到这种&quot;题目溢出&quot;的情况，都会希望一道题目可以利用很多次。如果模型第二次见到同一道题目，可能会出现reward-hacking的情况：thought随便说一个，然后把正确答案背出来。这就坏了。</p><p><strong>Sampling Diversity Degradation</strong>: 还是刚才那个reward-hacking问题，即使不背答案，也很可能会是把之前一轮的&quot;好样本&quot;过程再背一遍，毕竟训过了一遍，肯定会以高概率再说一遍的。每次都找正样本来，肯定会导致diversity越来越差的。</p><img src="../../files/images/o1_thinking/reward-hacking.png" style="zoom: 25%;"><p>(reward hacking meme)</p><h3 id="So-will-“Just-Scaling”-works-on-STaR">So, will “Just Scaling” works on STaR?</h3><p>我都能发现的问题，Ilya肯定能发现。Ilya的想法是：对thought作监督！所以做了一篇let’s verify step by step (可以参考<a href="/d074522f.html" title="论文阅读[精读]-Let’s Verify Step by Step">这篇笔记</a>)。我训一个PRM，给模型每一行证明都打一个分。如果模型随便说一个thought，就会发现prm的分数有突变，然后就能发现有reward-hacking了呢？</p><blockquote><p>Ilya的定义里，prm@step_n代表着截止到第n步的胜率。本质是RL里面的value，而不是advantage，这个后面有工作探讨了优劣(OVM、PAV)。</p></blockquote><p>这条路线是否可行？没有人知道答案，我其实倾向于不可行。因为越是强的系统，越是不可预测（人只能预测自己能力范围内的事情）。alphago move-37，全世界没有人预测到，它就不好吗？prm指标突变，可能只是policy系统超越了prm系统的上限。昨天Ilya在NeurIPS talk上说了一句耐人寻味的话：More reasoning is more unpredictable. Reasoning is unpredictable, so we must start with incredible, unpredictable AI systems.</p><p>回到刚刚的问题上：STaR会reward hacking，是不是代表着STaR的scaling性质不好，不可行呢？其实重新思考一下，可能恰恰是因为STaR没有做scaling，所以才有这个问题。所谓reward-hacking，其实是模型的&quot;背诵答案&quot;回路，和&quot;学会推理&quot;回路之间的博弈（我后面统称system1 knowledge&amp;memorization 与system2 reasoning），当数据规模小时，system1学习更快、更讨喜。但随着规模的增大，更泛化的system2会渐渐胜出，因为他有统计意义上的优势。</p><p>模型可以背下来5000道题目的答案，但是如果是5000亿道，还能背下来吗？一个200B的模型就能记住这么多事，随着题目增加，记忆能力捉襟见肘，开始遗忘；反而是用system2的方式可以节省参数、不会遗忘，最后会渐渐胜出。</p><blockquote><p>(题外话)如果大家感兴趣的话，可以阅读&quot;grokking&quot;和组合泛化研究方向的工作，他们在寻找和对比AI中的不同的learning pattern，对于所谓的思考回路、记忆回路有更明确的讨论。比较好的工作：</p><ul><li><p>Explaining grokking through circuit efficiency</p></li><li><p>Unifying Grokking and Double Descent</p></li></ul></blockquote><img src="../../files/images/o1_thinking/incentivize.png"><p>STaR总体上，是鼓励system2的。因为他给了模型一个机会去使用reasoning token，或者讲：把系统变得更scaling，让模型可以用test-time scaling（我的意思是，你可以不用thought，仅仅做reward-hacking背答案；但你也可以选择用thought，总之我给你了这个功能）。当计算规模上来以后，即使概率很小，模型也总能误打误撞地慢慢发现使用thought的好处，逐渐的把使用thought的能力推广到整个训练集上。</p><h2 id="“STaR”-the-entire-training-corpus-will-leads-to-o1">“STaR” the entire training-corpus will leads to o1?</h2><p>我们再打破一个砂锅，想深一层：STaR只能做数学题吗，数学题这个场景有什么好处？可被验证！类比一下np-hard问题。解决一个数学题很困难，但验证正确性很简单，甚至不需要模型，这种方案一般称为functional verifier（我仅仅指答案对比的题目，不是证明题）。所以，我们其实都不需要orm、prm这一大堆东西，找到一堆带答案的题目就够了，不用管题目本身是不是特别困难。</p><p>其实数学题这种pattern，在训练数据里是大量存在的，只是他们没有被建模出来，之前Jason Wei提到了训练数据中极度不平均的信息密度，都是预测这些token，但显然很多token的预测很简单，剩下的token预测很困难。模型有没有办法的自适应的增加计算量，来把数据集的信息密度重新变得平均呢？这就是o1。</p><p>事实上，如果我们想到了一些办法去衡量训练集的信息密度，然后把信息高密度区域转换为可以被验证的类数学题的qa形式(functional verifier)，那么其实就能把训练集按照STaR的方式学习。随着模型在&quot;练功房&quot;里持续不断地思考、试错，渐渐会形成一套自己的方法论（可能和人类认识世界的方式完全不同），用自己的方式去理解数据中所有的知识、推理的问题，并按照自己的见解对他们进行重组，最后变成适合于每个模型自己的long-cot训练回参数里，永久记忆。这就是我理解中的&quot;合成数据&quot;。</p><p>所以，nvidia的新单子bh200集群，把cpu:gpu的比例从4:1调整到2:1，就是为了方便OpenAI在CPU上部署更多functional verifier吗 [doge]</p><blockquote><p>这两篇twitter涉及到了对于information density有更详细的讨论</p><p><a href="https://x.com/_jasonwei/status/1855417833775309171">https://x.com/_jasonwei/status/1855417833775309171</a></p><p><a href="https://x.com/_jasonwei/status/1729585618311950445">https://x.com/_jasonwei/status/1729585618311950445</a></p></blockquote><p>传统意义上，大家可能会觉得：知识不是推理，用o1也没有用。这个观点的前提是“纯靠system1就能把知识全记下来”，但人的背诵也不是查表，否则就不会有“记忆宫殿”之类的一系列方法了。对于模型，即使训练语料就写着“中国的第八大城市是xxx”，但其实你也可以用一些更low-level的知识推导出这些结论，比如“北京有xxx km^2, 上还有yyy km^2…所以第八大城市很可能是zzz”，只需要记住基础知识，就可以在运行时随时再推导回来。具体哪些知识会被深刻记忆，那些知识会被学习成“基础知识+推理”的形式，就可以是模型自己决定的了。你不需要监督和关注这个内容，他们本身也是无法预测的。</p><p>所以，&quot;what is reasoning？&quot;是一个不太scalable的问题，更好的方案是假设: everything is scalable, and we can observe relative benefit on all subset.</p><h2 id="from-Data-Engineering-to-Verifier-Engineering-Align-system1-to-system2">from Data Engineering to Verifier Engineering: Align system1 to system2</h2><img src="../../files/images/o1_thinking/verifier-engineering.png"><p>在讨论这个问题时，已经代表了一种范式的转变——我们从GPT3、GPT4时代准备数据(Data Engineering)的思路，转变成了现在o1时代准备问题和答案(Verifier Engineering)。</p><p>在我的理解里，之前大家准备数据，主要是假设所有模型可以用同一套方案记忆这些知识，用一套方法论去推理。所以只需要拷贝一份数据，就可以从头把一个模型&quot;hash&quot;出来。但现在，大家逐渐发现不同模型是有独属于自己的记忆方案的：小模型和大模型记忆数据的能力不同，代码模型和文本模型对推理的倾向性也不同。仅仅靠data engineering，训出来的模型效果并不理想，因为这份数据并不是属于他的，而是全人类的智慧结晶，或者说是人类思考方式的平均数。</p><p>反而，准备verifier，让目标模型自己探索出解决这些问题的方案，就是在激发模型按照自己独特地方式思考、去理解每条数据背后的思想：<a href="https://www.youtube.com/watch?v=BKQkdg5XYIw">是什么思路，导致亚里士多德说出这样的话的？</a> 两种方案的目标是一致的：都是为了解决一个问题集上的所有问题。但实现方式完全不同，verifier 方式需要更多的算力，但会激发一个更定制化、更可扩展的AI系统。</p><p>可以想象，如果我在10000道题目中背下来了解答过程，假如第10001道和前面差得很远，那其泛化性堪忧。但如果我形成了一套方法论，并且验证我的方法论在10000道题目上都有效，那我只需要按照我的方法论去解决10001道，成功的概率确实不小。如果这套方法论已经被验证适用于所有的20TB token训练集，那么它的泛化性，可能就是LeCun所谓的世界模型了。</p><blockquote><p>参考&quot;Incentivize, don’t teach&quot;演讲里面的说法：</p><ul><li><p>prepare data实际上是： teach him how to fish</p></li><li><p>prepare verifier实际上是: teach him the taste of fish and make him hungry。他会学着开始去使用渔网、去查看天气，等一系列技能，即使我们从未教过他这么做。</p></li></ul></blockquote><p>从诗意的角度理解：<strong>data engineering是给模型设计一套最好的课程；而verifier engineering是希望模型从头扮演人类历史上的每一个智者，重走人类发展的老路</strong>。站在巨人的肩膀上，肯定不如自己成长为巨人来得实在。我无法想象，如果一个模型从头的、一个人的、重新的、推导出和发现了世界上的所有定理和知识，那现在他该有多聪明，还有什么问题能难倒他持续不断地思考。</p><p>另外和data engineering一样，verifier engineering也是一劳永逸的，你只需要准备一次verifier，就能无限地训练不同的模型，每次再拿出来那些verifier就好。商业公司的秘密，可能也会从私有化数据，转变成私有化verifier。区别是，之前偷过来训了数据，可能不用花很多钱就能复刻一个训练；现在偷过来verifier，想复刻一个的价格仍然是天文数字，而且需要你的training infra支持训练，(比如bh200这样2:1的cpu:gpu配比)。</p><h2 id="Wait-another-thought-more-concrete-problems-in-o1-discussion">Wait, another thought: more concrete problems in o1 discussion</h2><p>这套大方向的思路其实是形而上学的: 你既不能证实，也不能证伪——训练成功或者失败都有办法解释。可能需要关注于一些更具体的问题：</p><ol><li><strong>dataset re-sample</strong>: 如何把训练集的所有问题都变成functional verifable的。或者退化地思考，能否训练一个ORM，在所有情况下，达到90%的准确率？</li></ol><p>这个问题可能是整个o1系统里最基础、但也是最困难的部分。现在好像没人在解决这个问题，都是在做数学、classic-reasoning场景。大家的想法是：我先做数学场景，反正数学可以functional verifier，把低垂果实先摘掉。等谁把这个问题的解法开源出来，我再切换过去。是不是很像2023年初大家等alpaca 52k的心情[doge]</p><blockquote><p>OpenAI是怎么做的？我有点怀疑他们是PRM，要不然为什么会推出<a href="https://openai.com/form/rft-research-program/">reinforcement fine-tuning功能</a>呢？</p></blockquote><ol start="2"><li><strong>dataset re-order</strong>: 即使我已经找到办法，把训练集退化成了1000亿道题目，我怎么知道这些题目中，哪些比较简单、哪些比较困难？</li></ol><img src="../../files/images/o1_thinking/emergent.png" style="zoom: 25%;" ><p>这是一个很有价值的问题，因为你可以想象，有的题目我sample 16次就有答案，有的题目我sample 1024次才有答案，有的题目我sample 100万次才有答案。如果我不sample，就永远不知道需要sample多少次。而我现在期望的是，只sample 1亿次，然后把尽可能多的题目学会。这里面其实一套课程学习的思路，我能不能想办法对题目的难度进行分类，然后让模型先在简单的题目上尝试，把简单题目学习得大差不差了，再去难一些的地方尝试。所以，我们可能需要每时每刻找到&quot;即将Emergent的能力&quot;。从数据效率的角度来看，之前需要sample1000次的难题，现在可能只需要sample 100次了），如果找到一个好的顺序，可能他可以节省好几万倍的算力。</p><p>可悲的是，&quot;顺序&quot;对于不同的模型还是不同的：比如我恰好数学悟性好，随着训练我很快就能学会高中数学；但我不擅长语文，语文的能力就涨的特别慢。想要把所有题目提前按难度分类好，永远存下去，可能还是不可行。就得每次随着训练不同模型，专门去给他设计顺序。</p><p>有些人会想：我用PRM行不行，先sample 100次，看看prm最高分大概是多少分。我看来，这里面也有个大问题：虽然题目解决了，我可以发现他解决了，但是如果没解决，我怎么知道距离解决还有多远？甚至是说，判断一个题目的完成度，在我看来比完成这个题目还要困难！</p><blockquote><p>幸福的家庭都是相似的，不幸的家庭各有各的不幸。——托尔斯泰</p></blockquote><p>之前大家讲&quot;generative-verifier performance gap&quot;，就是说模型作为verifier的能力更强，可以给policy model提供训练信号。问题在于，这个讨论没有考虑&quot;policy + search&quot;的情况！我只知道&quot;verifier &gt; policy&quot;和&quot;policy+search &gt; policy&quot;，但是我其实需要的是&quot;verifier &gt; policy+search&quot;，我的verifier能区分我sample出来的1000个样本里，最好的两个样本谁比谁好吗？这个结论甚至可能是错的。（假如是小于，那所有的rm-based method都要寄了）</p><ol start="3"><li><strong>forawrd-method</strong>： 如何去高效地搜索试错？假设我已经把数据集做了re-sample, re-order，搞好了verifier。那接下来我该怎么设计一个sample算法(在搜索领域，可能更好的翻译是rollout efficency)，提高算力利用率？</li></ol><p>这里面又有好多设计了，o1刚出来大家说: OpenAI 用的是ToT，math-shepherd那套，有好多工作去定义出来&quot;reasoning-module&quot;，或者说&quot;reasoning-atomic&quot;。后来又觉得不像，又说要录音，把人的思考过程录下来，让模型学着模仿。再后来，大家发现需要所谓的o1-distilled seed-reasoning data去启动，然后直接按照temperature sampling的方式去搞。那么openAI是怎么搞的呢？总之它肯定不是蒸馏的o1的推理数据</p><ol start="4"><li><strong>backward method</strong>: 假设我已经rollout出来很多的reasoning数据，都给出了附加得分，接下来怎么训回模型？STaR其实是提供了一个最原始的方法：丢掉坏数据！后面大家有人会做得更优雅，比如把坏数据加个负数loss？把训练搞得再稳定一些以后，你就发现竟然重新推导出来的DPO？</li></ol><p>openAI是怎么做的？大家都猜测是PPO，大抵是因为PPO的作者现在在openAI。从头到尾，秘密行动[doge]。我个人倒是感觉这个问题可能不太难，把前面三个解决好，可能最后这里随便用一个算法也不会太差。</p><h2 id="我的思考">我的思考</h2><p>期待谁放出来第一篇提供全栈解决方案的、开源数据和模型的论文！到时候给大家分享阅读笔记</p><p>另外，我还有几个问题：</p><ul><li>对于&quot;o2&quot;的思考：如果我已经训练了一个o1（不妨假设他是200B），他磕磕绊绊终于找到了一套理解世界的方案，现在我想启动一个2000B的模型，我知道这个2000B的模型很可能在大多数问题上靠背诵就解决了。那我200B的o1可以对他的训练提供什么帮助呢？</li></ul><blockquote><p>能不能把phi-4做一个o1版本的呢？</p></blockquote><ul><li><p>既然文本模型可以做o1，sora可以吗？生成物理世界时，如果物理规律很难以把握的话，是否可以通过测试时计算，描述畅想一下需要遵循的规律，再生成呢？</p></li><li><p>模型的thought，一定要是文本吗？是否可以不把内容对齐到推理空间呢？目前大家要使用word embedding这个概念，是为了计算crossentropy，但如果我们在backward算法上，刨除掉对于reasoning token的loss，只监督output。那么我们可以在thought部分干脆不对应到词表里了？退化地讲，即使对应到词表，也不一定要是现在的词表吧。目前的词表是根据预训练预料的n-gram频率算出来的，我们为什么要假设推理token的词频分布需要和预训练数据的分布一样呢？比如模型总是说&quot;let’s think step by step&quot;，那这可以是一个token吗？更高的pre-compression rate，其实是在提高算力利用率的。</p><blockquote><p>openai似乎很多年前做过一篇相关的博客<a href="https://openai.com/index/learning-to-communicate/">Learning to Communicate</a>。我还挺期待模型什么时候可以做一个赛博坦语的thought，最后说出来一个&quot;注意到xxx&quot;把哥德巴赫猜想证明出来[doge]</p></blockquote></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;半年没写论文阅读笔记，其实笔记草稿写了不少，都没转正。主要觉得像是机械的翻译，没有思想在里面，不如不发。最近大家开始陆陆续续放出来o1-like的模型了，其实翻过头看，大家的思考方式还是几年前的STaR，去年我也写过 &lt;a href=&quot;/8622e2d1.html&quot; title=&quot;论文阅读[粗读]-STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&quot;&gt;一篇阅读笔记&lt;/a&gt; 介绍。&lt;/p&gt;
&lt;p&gt;今天不妨来重新思考一下STaR，连接上跟进的几篇STaR-like的工作，谈谈我对于o1的理解吧。参考文献:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&lt;/li&gt;
&lt;li&gt;Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking&lt;/li&gt;
&lt;li&gt;Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models&lt;/li&gt;
&lt;li&gt;Training Chain-of-Thought via Latent-Variable Inference&lt;/li&gt;
&lt;li&gt;Rest Meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent&lt;/li&gt;
&lt;li&gt;Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering&lt;/li&gt;
&lt;li&gt;Training Language Models to Self-Correct via Reinforcement Learning&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="Reasoning" scheme="https://www.yynnyy.cn/tags/Reasoning/"/>
    
    <category term="强化学习" scheme="https://www.yynnyy.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-Manyshot-ICL: 在context中重现传统AI的可能性</title>
    <link href="https://www.yynnyy.cn/f992cb6b"/>
    <id>https://www.yynnyy.cn/f992cb6b</id>
    <published>2024-05-27T08:30:28.000Z</published>
    <updated>2024-08-09T09:26:46.977Z</updated>
    
    <content type="html"><![CDATA[<p>今天来讲讲<a href="https://arxiv.org/abs/2404.11018">Many-Shot In-Context Learning</a>，大概是deepmind一个月前的文章，读下来和之前Jason Wei那篇&quot;Large Models do In-Context Learning Differently&quot;的阅读体验有点像，是一篇&quot;暗合scaling天意&quot;的文章。</p><p>看完了我把他和另外两篇论文联系了起来，想到了未来LLM在context重建AI的可能性。最后，推荐大家读一下原文，deepmind论文就像乐高，阅读(拼搭)体验一直很好……</p><p>参考资料：</p><blockquote><p>Many-Shot In-Context Learning</p><p>Many-Shot In-Context Learning in Multimodal Foundation Models</p><p>In-Context Reinforcement Learning with Algorithm Distillation</p></blockquote><span id="more"></span><p>作者团队来自Google Deepmind</p><img src="../../files/images/manyshot_icl/authors.png"><h2 id="introduction">introduction</h2><p>这篇论文方法上没什么好说的，大家都知道in-context learning: 把一个任务的很多input-output pairs放在prompt里，然后模型就可以在不更新自身参数的情况下，&quot;现场学会&quot;一个任务，并对最后给出的input预测出来结果。</p><p>从high-level的角度讲，我觉得这个能力是因为模型学会了所谓的”world model“。传统的AI领域，大家一般会建模出来一个任务，作为输入空间到输出空间的映射(比如情感分类)，$f: \mathcal{X} \rightarrow \mathcal{Y}$，接下来考虑如何训练一个模型$f_\theta$可以做好一个任务。对于LLM来说，从instruction tuning开始，大家开始认为整个世界就是一个$\mathcal{X}$，所有的所谓任务都只是从$\mathcal{X}$​里面的一个采样，因此只需要学会一个$f_\theta(\mathcal{X})$就可以表征所有的任务，in-context learning正是从这个情况下涌现出来的能力。</p><p>作者考虑了一个现实的问题：之前的in-context learning评测几乎都是3-shot, 5-shot, 8-shot。但是今天的LLM已经可以把自己的context拓展到128k，甚至10M(gemini)。那么，有人试过用更多的样本放在prompt里，效果会更好吗？作者把这个setting就叫做manyshot场景</p><img src="../../files/images/manyshot_icl/perf.png"><p>作者测试了gemini在不同场景下的manyshot表现，发现几乎都比few-shot场景效果好很多。</p><p>为了解释这个看起来有点神奇的现象，作者又定义了两个阴性对照和阳性对照的setting：</p><ul><li>reinforced ICL：先自己生成一堆input-output，然后根据output正确性筛选出好的样本作为shot</li><li>unsupervised ICL：模型生成一堆input，不拼output，看看能不能提升</li></ul><h2 id="Performance">Performance</h2><img src="../../files/images/manyshot_icl/trans.png"><img src="../../files/images/manyshot_icl/sum.png"><img src="../../files/images/manyshot_icl/verify.png"><p>作者在各种场景下尝试了many-shot，然后报告了效果随着shot增加的变化情况，可以看到，几乎在所有场景下，提升shot的数量都会让效果变得更好。</p><p>作者为了进一步探索这个现象，尝试了上面提到的reinforced ICL和unsupervised ICL</p><img src="../../files/images/manyshot_icl/math.png"><p>并且发现比起用ground truth样本作为ICL样本，模型自己生成的样本甚至效果要更好。而且，这种样本是可以迁移到别的任务上的，右边的图是用MATH数据集生成的样本来作为GSM8K的manyshot样本。</p><p>为什么unsupervised ICL效果很好，难道只需要看到一些query？作者类似于之前那篇weak-to-strong的思路，给了一个基于直觉的解释：如果模型本来就会做目标任务，可能只需要用一些query帮助他”联想“到预训练数据中的一些知识作为锚点，来让他在做现在的input时发散更多的知识。</p><p>从这个思路出发，对于数学这样的场景，预训练见过很多了，可能非常需要这种”联想“。</p><p>还有一个最有意思的实验设计，和大家分享一下啊：作者想要证明，manyshot的效果来源于去掉了预训练数据中的bias。如果大家想证明这个结论，该如何设计实验？</p><p>作者类似于之前那个&quot;large model performs ICL differently&quot;, 找到了一个情感分类任务，设计了对照组：</p><ul><li>flip：把标签反过来，即positive变成negative，negative变成positive。这个和预训练知识相反，模型不得不在context中学习</li><li>abstract: 把所有的标签变成A、B、C这种没有语义的东西</li></ul><p>通过这两个对照，作者就能勘测出预训练bias对效果的影响，作者发现：最开始，两个对照组的准确率都不太行，但随着shot增加，三种方法的效果最终收敛到了同一水平。这说明：manyshot场景可以逐步削减模型对于预训练和下游任务的理解偏差，进而提升任务的效果。</p><img src="../../files/images/manyshot_icl/bias.png"><p>最后，作者报告了一个解释不了的现象：随着shot增加，作者看了ground truth的ppl，发现越来越低。但是，如果统计acc的话，实际上250-shot场景的acc是不如125的。在predict-scaling这个领域，大家往往喜欢用更弱的模型预测更强模型的效果。从scaling曲线上讲，随着几乎$ppl \propto \log (N-shot)$，预测ppl似乎是可行的。然而，更低的ppl却不能带来更高的得分，这和传统benchmark场景的结论相反。</p><blockquote><p>为什么会这样？我想起来之前在读<a href="https://arxiv.org/abs/2205.14334">Teaching models to express their uncertainty in words</a>中作者提到了ppl和1)模型对答案的信心值2)模型表达这个解答过程的信心值都有关。我们可以思考一下many-shot场景，当前面拼了非常多的样本时，模型对于1)和2)的信心值会倾向于更高还是更低呢？同样的，如果模型对于任何答案的信心值都变得更高了，那么可能就更难以区分出好的答案和坏的答案了</p></blockquote><img src="../../files/images/manyshot_icl/nll.png"><p>对于上面的问题，作者在附录中还给出模型对于正样本和负样本的NLL。可以观察到，总体而言，似乎样本越多，模型越没法使用NLL区分正/负样本</p><img src="../../files/images/manyshot_icl/contrastive.png"><h2 id="几个问题和我的思考">几个问题和我的思考</h2><p>看完这篇论文确实收益良多，不过我似乎产生了更多的问题，不知道大家有没有类似的感受。</p><h3 id="最像样本相似度">最像样本相似度</h3><p>首先，我有另一个视角去理解这个现象：我们如果统计manyshot样本中和当前query embedding最像的top1 similarity。然后画个散点图，其中横坐标是top1 similarity，纵坐标是正确与否，然后给每个横坐标区间统计平均正确率，变成柱状图。即样本越像，acc越高。而且，我感觉对于所有shot的场景下，这个柱状图可能会遵循同一个分布……这符合大家对预训练模型的认知：&quot;LLM can’t perform zeroshot, performance depends on shots in training data “对偶的说法就是&quot;LLM’s manyshot performance depends on similarity of the in-context examples”。不过这个视角解释不了另外一个现象，就是few-shot场景下模型表现对于few-shot样本顺序的敏感性</p><h3 id="和finetune的关系">和finetune的关系</h3><p>再有，如果我们有1024个样本，传统的视角下，我们肯定会想要finetune，但这篇论文可能是只能访问API，没有对比finetune的baseline……如果我们相信了前文对于”概念和联想“的直觉解释，那么finetune Gemini对于Gemini到底意味着什么？</p><blockquote><p>传统AI中，会任务finetune是在学习知识，或者说找到更泛化的概念。但是，如果LLM本身就具有泛化的概念，是否finetune这个过程只是帮助模型建立几个&quot;思维的锚&quot;呢？</p></blockquote><p>这让我想起来曾经在delta tuning看到有个领域叫”Intrinsic dimension“，他们试图找到模型最少可以在更新多少个参数的情况下，达到全参数微调的效果的90%，并把这个数字叫做LLM的Intrinsic dimension(ID)，他们发现基础能力却强，ID就越小。这是不是从某种程度上支撑了上面的猜想：越强的模型，本身的retrieve能力越强，就越不再需要很多所谓的&quot;联想锚点&quot;，可以直接从当前instruction里做联想。</p><p>另一个领域是之前的weak-to-strong generalization，他们使用带噪声的数据集训练GPT4，发现GPT4自己把数据的噪声恢复了，在测试集上效果还是很好。这是不是也说明，可能answer并不重要，关键是激发一下模型对于问题的理解。也就是说，如果在weak-to-strong场景下，只在query上加loss，会不会都不用answer模型就学会了？</p><blockquote><p>甚至可以开展一个实验，比如我有5000条数据。其中100条我在answer加loss，剩下4900条我在query加loss，这样训出来的模型和和正常训练的模型，效果上也会有个PGR(performance gap recovered)吗？</p></blockquote><h3 id="在context中重建传统AI、甚至世界知识">在context中重建传统AI、甚至世界知识</h3><p>最后我想说的是，这篇工作让我发现，随着context的增加，大家其实可以在context中重建传统AI算法。这篇论文在context中添加非常多的样本，叫做manyshot，其实和传统RL中的finetune有点像。我们抽象的看，就是Gemini在接受到这些样本以后，自己抽象出来了一个&quot;完成这个任务的模型&quot;。</p><p>Gemini-1.5 report里其实就做了类似的东西：他们找到了一门叫做<strong>Kalamang</strong>的罕见语言，网络上肯定找不到资料。让模型看完这个语言的字典，发现Gemini就可以翻译kalamang了。所以，如果模型有很强的智力，他有能力通过context去理解世界。</p><blockquote><p>这是一个很恐怖的事情：试想，如果某一天来了三体人，他们的三体模型甚至不需要微调，只是(和人一样、纯前向地)看了一下人类世界的一些语料，就可以像人一样完成人可以完成的所有事情。</p></blockquote><p>在学界，其实有一些类似的工作：</p><ul><li><p>RL：deepmind曾经做了一篇叫&quot;in-context RL&quot;的工作，他们会把ppo、dpo等算法训练的过程记录下来，丢到一个context里，让一个长模型在这个上面做建模，看能不能进行”algorithm distillation“，学到比原来的算法更高效的&quot;rl算法&quot;。不过这篇论文是训模型的工作，如果我们context很长，我能不能把&quot;一次rl训练过程”作为1-shot，然后进行manyshot，模型就可以learn to learn rl in-context了呢？</p></li><li><p>Align: 进一步地，之前yuchen lin有一篇工作叫做<a href="https://arxiv.org/pdf/2312.01552">The unlocking spell on base llms</a>，发现其实只需要找到一些in-context样本，就可以让模型在很多case上&quot;看起来像是做过rlhf&quot;。如果模型的context够长，我用manyshot的形式把alpaca-52k丢进去，模型就能做好rlhf吗？</p><blockquote><p>从这个思路继续思考：所谓的&quot;人类偏好&quot;到底激发出了模型的哪些&quot;思维锚&quot;？</p></blockquote></li></ul><p>未来，随着LLM的context越来越长，可能我们现在的所有传统算法(比如推荐)都可以被模型通过in-context的方式去解决。可能rag也不复存在了……“long-context LLM is many-shot learner, and zero-shot world model”</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天来讲讲&lt;a href=&quot;https://arxiv.org/abs/2404.11018&quot;&gt;Many-Shot In-Context Learning&lt;/a&gt;，大概是deepmind一个月前的文章，读下来和之前Jason Wei那篇&amp;quot;Large Models do In-Context Learning Differently&amp;quot;的阅读体验有点像，是一篇&amp;quot;暗合scaling天意&amp;quot;的文章。&lt;/p&gt;
&lt;p&gt;看完了我把他和另外两篇论文联系了起来，想到了未来LLM在context重建AI的可能性。最后，推荐大家读一下原文，deepmind论文就像乐高，阅读(拼搭)体验一直很好……&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Many-Shot In-Context Learning&lt;/p&gt;
&lt;p&gt;Many-Shot In-Context Learning in Multimodal Foundation Models&lt;/p&gt;
&lt;p&gt;In-Context Reinforcement Learning with Algorithm Distillation&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="预训练模型" scheme="https://www.yynnyy.cn/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</title>
    <link href="https://www.yynnyy.cn/fbc665c3"/>
    <id>https://www.yynnyy.cn/fbc665c3</id>
    <published>2024-03-23T04:10:02.000Z</published>
    <updated>2024-08-09T09:26:46.977Z</updated>
    
    <content type="html"><![CDATA[<p>最近Apple出了自己的30B多模态大模型，涌现出了多模态的in-context learning效果，论文里一句&quot;even better&quot;让我想到库克那个嗓音……作者说明了很多在训练中收获到的经验教训，这是我最近几个月看的写法最清楚的一篇论文。正好借此讲讲多模态大模型：目前学界大火的VLM，到底是怎么跑的？</p><span id="more"></span><h2 id="Introduction">Introduction</h2><img src="../../files/images/mm1/authors.png"><p>作者团队来自苹果，看来苹果说的&quot;今年wwdc上大模型&quot;真有希望了？</p><p>在pretrain models领域，大家一直都想把之前的所有任务的数据整合到一起，把之前一堆独立模型做的事情统一到一个模型里面,由此诞生了LLM技术。最近大家关注的重点开始转向多模态：除了文本领域的任务，能不能同时把和图片理解相关的任务也统一进去? 具体来说，如果输入同时含有图片文本，多个图片文本交错排列，模型能不能在理解图片和文本的基础上，输出文本。这就是MLLM(multimodal LLM)或者VLM(Vision-Language Model)</p><p>虽然学界、业界在这方面的探索不少，但已有的工作基本都不够开放：要么就是闭源模型给个API，要么就是开源模型只给个权重。即使有些工作会开源数据，但是没有人会开放讨论他们对于模型结构的选择、对于训练数据的选择、对于训练超参数的选择以及背后的原因。</p><p>作者在本篇工作中开放的讨论了所有的细节，结论简单来说就是以下几点：</p><ol><li>在模型结构方面，下面几个要素的重要性递减：vision-encoder的分辨率、vision-encoder大小。VL-connector选择对于最终的表现几乎没有影响</li><li>在数据方面，主要有以下几种数据：text-only数据、image-caption数据、互联网文本-图片交错数据，作者发现：<ol><li>互联网文本图片交错数据和text-only数据对于in-context Learning、few-shot Learning至关重要</li><li>image-caption数据对于zero-shot Performance至关重要</li></ol></li></ol><p>最后，从一系列insight出发，作者scaling了训练过程，训练了3B、7B、30B的VLM，甚至在3B和7B规模下尝试了top2的MoE架构，并在各个层面上达到了SOTA的效果</p><blockquote><p>除了没开源weight，基本全给你了……</p></blockquote><h2 id="VLM长啥样？">VLM长啥样？</h2><img src="../../files/images/mm1/design-choice.png"><p>目前的VLM基本都是分三个部分，如上图的左图：</p><ol><li>image-encoder，负责把图片编码成为embedding，同时尽可能不要损失信息</li><li>VL-connector，负责把image-encoder的输出做一些转换，比如MLP，对齐到LLM的word embedding空间</li><li>decoder-only LLM：把上一步的输出直接作为多个Token的Word embedding输入，然后跑后面的LLM next-token-prediction。会在text输出的部分计算loss</li></ol><h3 id="Image-encoder">Image-encoder</h3><p>image-encoder一般也是transformer，是一个ViT。他首先会把图片按照从左到右、从上到下切成不同的小正方形，打成多个patch，每个patch的分辨率都是比如14x14。接下来每个patch会使用一个卷积层编码成为一个vector，然后多个patch vector拼在一起当成多个“token”的word embedding，加上position embedding后直接进入transformer encoder，输出会是每个patch一个hidden state。</p><p>这个encoder有两种训练方法：</p><ol><li>contrastive loss: 大名鼎鼎的CLIP。找到一大堆互联网上的挨着的(图片-文本)对作为正样本，然后随机其他的对作为负样本，然后用刚才的ViT作为image-encoder，用另一个比如T5之类的作为text encoder，拿到两个embedding，跑对比学习的loss：希望正样本-正样本之间的cosine相似度大于负样本-正样本之间的cosine相似度</li><li>reconstructive loss：这个就是传说中的VAE。用ViT编码完了以后会把图片变成一个embedding，为了保证这个embedding尽量信息无损，会后面再接上一个image-decoder去预测原来的图片长什么样子。预测的越准loss就越小。为了防止模型去memorize每个image的样子，还会有个辅助KL loss去保证所有image的embedding在embedding空间的分布尽可能均匀</li></ol><p>这两种方法有个特点：都是无监督的，只要有一大堆数据就能起效果。前者需要 (图片-文本) 对，但有个好处是数据可以自己合成，可以人工在text-caption上加上各种丰富准确的细节来提高训练的要求，进而增强模型。后者只需要一大堆图片，在一些dense的任务上表现更好</p><img src="../../files/images/mm1/image-encoder.png"><p>作者在这里做了消融实验，最左边那列AIM是reconstruction loss，CLIP是对比学习loss。architecture基本都是ViT，H的参数量比L大。image Res代表的是训练的时候的训练数据的图片分辨率都是多少。Data是指用什么数据做的训练。</p><p>明显可以发现：</p><ol><li>image res这个变量对最终的效果差距最大</li><li>在CLIP中加入合成数据(VeCap)，会提升模型表现</li><li>两种loss基本都不咋影响效果，都差不多</li></ol><p>作者最终选了CLIP + 高清encoder</p><h3 id="Vision-Language-Connector">Vision-Language Connector</h3><p>这一部分就是输入image-encoder的output，转换到Word embedding空间，这里面有个至关重要的问题：一张图片应该转换到LLM里的多少个token？</p><ol><li>显然token更多，保留的细节更多，模型的效果理应更好</li><li>然而，当token多的时候，尤其如果是多个图片(8个、16个)，很多个token其实训不起来，资源消耗太恐怖了</li></ol><p>作者在这里消融实验了一个image对应64、144 token两种情况，然后尝试了224、336分辨率的image-encoder，以及不同的pooling策略，大致有这么几种实现：</p><ol><li>Average Pooling: 由于Vit的token数大于目标token数量，就把相邻几个patch hidden-state取平均数，变成一个embedding，让数量变得一样了。再给每个token过一个MLP，作为LLM的一个token embedding</li><li>Attention Pooling：作者觉得vit的输出可能和llm的word-embedding不在一个子空间，需要一个Attention层变换一下。于是就加一个额外的Attention层，初始化k个query向量，然后用key、value变换阵把ViT的输出对齐过去(Attention输出在sequence-length维度上和query的长度一致)。这样Attention的输出就是k个token embedding，然后k可以取64、144，就对齐过去了</li><li>C-abstractor: 把输出用某种卷积层操作(ResNet)转换到word-embedding空间</li></ol><img src="../../files/images/mm1/connector.png"><p>作者对比了一圈发现：vision token和image resolution最重要，几种不同的pooling策略几乎没区别。即使在一个test set上好，在另一个上可能会更差</p><h3 id="Pretrain-data">Pretrain-data</h3><img src="../../files/images/mm1/data.png"><p>最后，作者探索了pretrain数据对于结果的影响。这里有个歧义：“pretrain&quot;并不是真正的预训练。作者这里实际上会选取一个预训练好的LLM作为text-only decoder，然后选取一个在CLIP对比学习loss上预训练好的image-encoder。把他俩用一个新初始化的VL-connector拼在一起。把这个叫做&quot;start training”。所以这里探讨的“pretrain data”单指VLM启动训练后的&quot;pretrain&quot;</p><p>VLM的pretrain-data基本分为三种：</p><ol><li>image-caption: 正常VLM的预训练数据，也就是刚才CLIP里的图片文本对。作者会输入图片，让模型预测caption</li><li>interleaved image-text: 从互联网上爬下来的语料。这里面都是图文交错的数据，作者直接把文本提取出来，在正常图片的位置放上图片，让模型预测所有文本的next-token-prediction</li><li>text-only：正常LLM预训练的数据，基本上是去掉图片后的互联网语料，以及github之类地方爬下来的各种代码，以及找到的一大堆各个学科的教材啥的</li></ol><p>其中，image-caption数据分为正常版和合成版本。所谓合成数据，就是用GPT-4v或者其他什么开源模型(这里说的VeCap用的vicuna，是一个Llama在gpt-3.5的输出上做过SFT的版本)，要求他给图片生成一些非常详细的caption，包括里面所有Object以及他们之间的位置空间语义承接关系之类的。</p><p>如果大家看过前几天写的DALL.E 3的笔记 <a href="/61495969.html" title="从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和&quot;Synthetic Data&quot;是一个意思吗？">dall.e 3阅读笔记</a>，就知道正常的html里来的图片文本对的alt-text质量有多差，所以需要重新clean，去重写等等</p><img src="../../files/images/mm1/data-choice.png"><p>作者在这一部分做了最多的实验，因为在LLM领域现在大家也普遍认为数据是影响结果最重要的因素，大致发现了如下结论：</p><ol><li>interleaved data is instrumental for few-shot and text- only performance, while captioning data lifts zero-shot performance。作者猜测是因为互联网交错数据天生具有一些in-context Learning的性质</li><li>text-only data helps with few-shot and text-only performance</li><li>Careful mixture of image and text data can yield op- timal multimodal performance and retain strong text performance.</li><li>Synthetic data helps with few-shot learning.</li></ol><p>最后，作者公布了花了很多钱试出来的配方： caption / interleaved / text-only = 5:5:1</p><h2 id="MM1">MM1</h2><h3 id="pretrain">pretrain</h3><p>由上面的结论，作者把训练做了scaling，选择了如下方案：</p><ol><li>CLIP loss的ViT-H，378x378分辨率</li><li>144 token，c-abstractor模式的VL-connector</li><li>5:5:1的VLM pretrain-data</li></ol><p>作者在 9M, 85M, 302M, 1.2B几个规模上做了超参数搜索，并认为学习率$\eta$应该和模型规模的对数成反比，由此预测了30B模型的最优Learning rate=2.2e-5。然后weight-decay $\lambda$是lr的1/10<br>$$<br>\eta = \exp(-0.4214\ln(N) - 0.5535)\<br>\lambda = 0.1\eta<br>$$<br>另一面，作者还尝试了MoE架构，用了Mixtral-8x7B那种经典的top2 router，在3B和7B LLM规模上做了实验。3B是每两层加一个64选2的FFN，7B是每四层有一个32选2的FFN。这些FFN都是从最开始的dense FFN初始化的，随着router的训练逐渐变得差异化了。为了稳定训练，作者还加了一些平衡routing的loss</p><blockquote><p>这方面，似乎Mixtral 8x7B report说他们没加任何其他loss，也可以正常训练？不知道是不是VLM领域有新的bug</p></blockquote><img src="../../files/images/mm1/eval.png"><p>作者发现，这样训练出来的MM1系列模型，在所有规模上基本都是目前最好的VLM。然后MoE可以让Performance “even better”</p><h3 id="SFT">SFT</h3><p>目前的公认解决方案，都是在Pretrain完以后，把能找到的学术任务拿过来造一个大号的数据集，让模型做一下supervised finetuning，再找一大堆chat数据让模型学着遵循人类乱七八糟的指令（比如&quot;只能回复emoji&quot;），最后再在测试集上测试效果</p><p>类似的，作者的SFT数据构成三部分：</p><ol><li>学术数据集转换来的数据</li><li>GPT-4v生成的给予图片的qa chat数据</li><li>text-only，训练正常LLM时使用的SFT数据</li></ol><p>所有数据掺在一起，SFT阶段随机喂数据。他们尝试了更高级的策略，发现效果基本没提升，就用了这个最简单的招</p><h3 id="high-resolution">high resolution</h3><p>测试的时候，还有问题：1)测试数据很多有巨多图片输入，或者需要8-shot in-context样本，这就需要最少8个图片。2)很多图片清晰度巨高，比如3840x2160分辨率。</p><p>这方面，有一个解决方案：Sub-image decomposition, 比如对于1344×1344的图片，作者分成5张图丢给模型，每个都是672×672。第一张图是降采样的， 后面四张分别是左上角到右下角的局部</p><blockquote><p>这里有个trick：这里的复杂度不是$n^2$,而是$n^4$。一个大图片会变成$n^2$个token，后面LLM本身又以$n^2$的复杂度跑前向，所以这个算法把一个大图变成5个小图实际上是省的。</p></blockquote><img src="../../files/images/mm1/sft-performance.png"><p>作者通过这个方法可以支持任意分辨率的图片，实际上最高到1792x1792还是会崩。然后作者对比了SFT阶段的效果，发现了几个核心：</p><ol><li>image 分辨率对效果至关重要，如果输入的分辨率本身不高，怎么训练效果都不行</li><li>VLM的预训练对效果很重要。作者把预训练各个阶段的ckpt都拿过来做了一次SFT实验，发现VLM预训练越久，对应的SFT Performance也越好</li></ol><p>另外，作者发现即使SFT数据全都是单张图片输入的，MM1还是具有多张图片输入时的in-context推理能力。并且，在Pretrain阶段探索到的insight，对于SFT阶段的Performance仍然成立。最后，作者在附录里展示了很多MM1的case，感觉这个模型的效果是真的很不错</p><blockquote><p>要是开源就好了</p></blockquote><h2 id="我的思考">我的思考</h2><ol><li>我印象中MM1是第一个正式描述自己的多张图片、图文交错情况下的in-context Learning能力的模型。我感觉这个能力对于多模态的Agent很重要，目前我看到的一堆multi-modal的ReAct算法，都是把history用文本的形式拼到prompt了……感觉不是真正的multi-modal agent</li><li>另外最近有一些别的模型，比如Fuyu是一个decoder-only架构，图片的patch embedding不用走vit，直接在一个大decoder里走一次。代价是要重新预训练，好处是跑的很快……不知道哪种才是最好的方案，我猜目前这些方案是因为大家可以直接拿一个llama过来当LLM，效果比较稳？</li></ol><img src="../../files/images/mm1/fuyu.png">]]></content>
    
    
    <summary type="html">&lt;p&gt;最近Apple出了自己的30B多模态大模型，涌现出了多模态的in-context learning效果，论文里一句&amp;quot;even better&amp;quot;让我想到库克那个嗓音……作者说明了很多在训练中收获到的经验教训，这是我最近几个月看的写法最清楚的一篇论文。正好借此讲讲多模态大模型：目前学界大火的VLM，到底是怎么跑的？&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多模态" scheme="https://www.yynnyy.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="Computer Vision" scheme="https://www.yynnyy.cn/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>从DALL.E 3沿用到Sora的Recaption: GPT4也在用？和&quot;Synthetic Data&quot;是一个意思吗？</title>
    <link href="https://www.yynnyy.cn/61495969"/>
    <id>https://www.yynnyy.cn/61495969</id>
    <published>2024-03-02T03:08:13.000Z</published>
    <updated>2024-08-09T09:26:46.971Z</updated>
    
    <content type="html"><![CDATA[<p>最近Sora巨火，仿佛开启了AIGC的新时代。Jason Wei表示：“Sora is the GPT-2 moment” for video generation。我在sora发布的大约第5个小时读了technical report，里面最打动我的其实是没提什么细节的recaption技术。让我回想想起了之前读DALL.E 3论文时的愉快体验。</p><p>所以今天来分享一下DALL.E 3论文里的recaption细节，并讨论几个问题和我的看法：1)OpenAI教你为什么要&quot;先查看原始数据，再做创新&quot; 2)Recaption和大家一直在聊的&quot;training on synthetic data&quot;是一回事吗? 3)recaption技术是否已经在(或者即将在)被其他领域使用？</p><p>另外，我总结了一下上篇笔记阅读量大的关键：语言表达要浅显易懂些，所以这篇笔记我可以声明一下：<strong>没学过AI也能看懂</strong>(我在博客里加了这个标签&quot;from scratch&quot;，所有我认为不懂AI或者只知道一点点的人也能看懂的博客都会加上这个标签)</p><p>参考文献：</p><blockquote><p><a href="https://openai.com/sora">https://openai.com/sora</a></p><p>Improving Image Generation with Better Captions</p><p>Automatic Instruction Optimization for Open-source LLM Instruction Tuning</p><p>WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation</p><p>Reformatted Alignment</p><p>Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling</p></blockquote><span id="more"></span><h2 id="DALL-E-3">DALL.E 3</h2><p>论文的标题明确指出了关键点&quot;Better Captions&quot;，说白了就是教你(叫你)去清洗数据。我们也许可以从这篇论文里，大致窥探到OpenAI世界第一的数据工程insights。</p><img src="../../files/images/recaption/authors.png"><p>作者指出，在DALL.E 2以后，text2image得到了学界越来越多的关注，大家想要开发更好的模型结构、使用更大的参数量和训练量。另外，学界很多工作帮助指出DALL.E 2中存在的问题：忽略text中的要求、和text中的语义不符，图片里出现诡异的文字等等。</p><p>怎么办呢？继续扩大模型规模，可以在一定程度上缓解这个问题。不过，作者在查看了原始数据后发现了根源：巨量的互联网 图片-文本对 数据里的图片和文本在大多数情况下并不对应，比如下图，虽然图片相对高质量，但是对应的alt text实际上都和图片没什么关系。实际上，上面提到的text2image model中的问题，其根源在于数据集的质量。</p><h3 id="re-caption">re-caption</h3><img src="../../files/images/recaption/caption.png"><p>所以，作者有了直觉的想法：</p><ol><li>先训练一个caption model，可以输入图片数据输出高质量文字描述</li><li>把他们的整个数据集的所有文字描述全部重跑一遍，所以这个过程叫做(re-caption)</li><li>在新生成的数据集上训练text2image model。保证训练数据是高质量的</li></ol><p>这个思路没什么难的，实际上学界也早有了相关的思路，OpenAI只是把这个方法扩展了起来。作者分别标注了short-description和detailed-description两个小数据集，端到端的训练了一个caption model，然后把每条预训练数据集里面的图片都生成了SSC和DSC两种。</p><p>然而，作者敏锐的察觉到了上面这种方法里存在的问题：人类世界的caption虽然质量不一定高，但足够泛化。机器生成的caption虽然质量高，但他的多样性受制于caption model训练数据的多样性。比如说，如果caption model永远输出&quot;a xxx&quot;开头的caption，那么如果用户运行时的输入不是a开头的，模型是不是就会爆炸了？所以作者希望数据集中的caption要尽可能接近人类生成的text</p><p>这个问题其实也好解决：把原始数据和recaption数据掺在一起训练！作者由此开展了一系列实验。另外，作者尝试了使用GPT-4V作为image caption，效果实际上也很不错</p><h3 id="Blending-synthetic-and-ground-truth-captions">Blending synthetic and ground-truth captions</h3><img src="../../files/images/recaption/blend.png" style="zoom:33%;" ><p>作者经过一系列实验，发现基本上使用更多的synthetic data，在in-domain的测试里效果会更好。作者由此训练了DALL.E 3发现效果比DALL.E 2有了明显的提升。不过作者也同时指出了，DALL.E 3仍然有很多问题，并且这些问题本质上是image caption model暂时学习不到的性质</p><ol><li>get不到caption里面的空间信息，比如谁在谁的前面/上面等等，作者发现caption model也往往说不对这些关系</li><li>图片里面的文字会丢失字母等等：作者认为这是text encoder是基于t5的。他的encoder是基于token的，模型需要学会把tokenizer里面的token(含多个char)映射到图片空间，这实际上非常难(比如说&quot;图片要求写一个&quot;play&quot;，每个字母用不同的颜色，每个字母分别倾斜30度&quot;。这个描述本身被tokenizer时&quot;play&quot;会是一个token，)。以后也许会训练基于char的model来解决</li><li>专业知识性的caption说不对，比如各种罕见的鸟的类型生成不对：作者发现这是因为caption model也说不出来，因为这需要更多、更高级的世界知识理解能力。作者认为需要更强的caption model(比如GPT-4V)</li></ol><h2 id="DALL-E-3还有后手？">DALL.E 3还有后手？</h2><p>通过上面的讲解，我们应该发现：DALL.E 3的训练数据的文字部分，绝大多数(95%)都是recaption出来的。这对模型的影响有多大呢？</p><img src="../../files/images/recaption/image-with-better-caption.png"><p>作者列出来了dalle.3使用正常caption和详细caption下的实际表现，确实在prompt following能力上天差地别，使用更符合DALL.E 3训练数据格式的prompt会让他的表现好很多。实际上，很多text2image目标用户过去一年里的很多学习，或者说网上找的教程都是在学习&quot;如何写好的prompt&quot;。这和&quot;模型更好的遵从prompt&quot;是一个双向奔赴的过程，但是只有OpenAI自己知道他们的闭源的recaption训练数据到底长什么样：我们其实很难针对性的写出符合DALL.E 3预期的prompt。这也就是上面论文里提到的问题：真实人类需求和训练prompt的分布不一致，会导致模型部署时崩溃风险高。</p><p>怎么办？OpenAI帮我们想了一招：写prompt的也是OpenAI-model就好了！所以我们会发现，现在使用DALL.E 3都是基于网页端的，我的需求会被GPT &quot;re-caption&quot;成真实的需求。比如这张图片：</p><ol><li><p>我写的要求：帮我生成一张图片，描述weak2strong generation里GPT2监督GPT4</p></li><li><p>GPT4实际生成的prompt：Imagine a futuristic, abstract scene where GPT-2, represented as a wise, older robot with classic design elements, is mentoring GPT-4, depicted as a sleek, advanced robot with cutting-edge features. The setting is inside a vast, digital library, filled with glowing books and holographic data streams. GPT-2 is shown pointing towards a holographic display that illustrates complex algorithms and data structures, while GPT-4 observes attentively, its sensors and circuits illuminated by the holographic light. The atmosphere is one of collaboration and knowledge transfer, highlighting the evolution of technology from one generation to the next.</p></li></ol><img src="../../files/images/recaption/dalle3-image.webp" style="zoom:33%;" ><p>通过这种方案，就不怕分布不一致了。OpenAI的这个设计实际上让我产生了两个联想：</p><ol><li>对于用户来说，prompt是什么本身比并不重要。prompt只是链接需求和成品的桥梁，用户关心的是自己想要的图片完成没有而不是prompt好不好。</li><li>ChatGPT目前做的事情更像是链接了 用户需求 -&gt; text-prompt。这本身不是最直观的方法，因为GPT和DALLE本身是没有交互的</li><li>我们可以把DALL.E 3部署时产生的数据定义为三元组 (多轮对话, text-prompt, target-image)。如果这个数据量scale起来，数据量达到二元组数据规模以后，是不是可以直接训练一个端到端的模型呢？他可以同时生成文字和图片，理解文字和图片。直接和你的对话去理解你的那种虽然抽象、但乐于通过对话表达出来的需求(比如要多少号多大的猫)，甚至通过多轮的图片生成去一点点猜测你的真实意图。</li></ol><blockquote><p>在DALL.E 3论文里用GPT4生成caption的实验里展示了这种野心的一角，这可能才是text2image下沉市场(直接使用图像的用户、而不是基于图片去二次创作的画家们)的更广阔的未来</p></blockquote><h2 id="Sora的recaption又玩出了什么？">Sora的recaption又玩出了什么？</h2><p>Sora在技术报告里提到使用re-caption技术为视频创造文字描述。实际上这个领域和text2image完全不同：</p><ol><li>互联网能找到大量的图片、文本对，但很难找到大规模的 视频-文本对</li><li>图片和文本基本上是一一对应关系：有一个很精确详细的文本描述以后，其对应的图像基本也就只能长那个样子，对图片进行裁剪会导致清晰度丢失严重，并且丧失语义丰富性。但视频是可以&quot;降采样的&quot;，一个视频的任何片段还是视频，大概率还会有语义，可以做re-caption。</li><li>除了降采样，实际上还可以拼接。比如Sora里提到了模型有能力融合两个视频变成一个语义通顺的更长视频。这种方式，在理想情况，对于数据的利用效率会从O(n)变为O(n^n)。如果显卡足够，简直无法想象</li></ol><p>所以，sora里面很可能更大比例的使用re-caption技术来获得准确高质量的视频描述。这个描述不仅仅是空间尺度的，甚至还有时间尺度的，比如&quot;在14秒时，屏幕上出现一只猫，在17秒时跳走了&quot;。然而，如何高质量的降采样、拼接，使得视频仍然是保有语义的，OpenAI不知道做了多少数据工程。</p><blockquote><p>&quot;保有语义&quot;这个描述可能不太准确，实际上是：希望视频描述和真实用户在要求sora生成视频的需求尽可能接近。比如可能没人希望生成&quot;两瓶可乐在打架&quot;的视频，虽然确实是有语义的……可能，也有人？</p></blockquote><p>Sora技术报告对这些细节语焉不详，我们只能期待学界的开源工作在这方面做出更多更有趣的探索吧。</p><h2 id="recaption：my-insights">recaption：my insights</h2><p>我认为，代码、图片、视频，(甚至我研究的tool-call-chain)都是和文字独立的其他模态的数据。他们有自己的模式，目前我们对于语言模态的模型训练效果最好，或者说，在语言模态找到的auto-regressive训练方法对数据的训练效率最高。</p><h3 id="recaption-is-only-training-to-align">recaption is only training to align?</h3><p>想要习得一个新模态的能力，我们需要的是pair数据：不放设想一下，我从出生开始，虽然到现在也才20多年，但是我经历的数据是 (文字-图像-视频-声音)等等完全对齐的，脑子里甚至在同时产生无穷无尽的CoT来解释目前看到的这一切</p><img src="../../files/images/recaption/歪头.jpeg" ><p>所以对于模型来说，学习别的模态是一个比学习语言模态更困难的任务：image2text做的好、text2image不容易。所以大家训练dalle想要的做的事情可能<strong>并不是让模型学会图片模态，而是希望模型更好的把自己对于文字模态的知识映射到图片模态</strong>。对于这种类似于“align”的任务，最好地办法就是给出更准确的align数据。什么叫更好的align数据？没有歧义的数据，比如说一个文字描述，你能画出多少图片，他们之间的差异有多大：画一个人、画一个男人、画一个小男孩、画一个华裔小男孩、画马斯克、画埃隆马斯克……可以认为，更详细的文字描述是在减少歧义，同时通过传入更多的信息去<strong>定点地激活对应的文字模态的知识</strong></p><p>当然，OpenAI做recaption的思路其实有点违背了预训练的初衷：更多的数据、更少的干预、更少的归纳偏置。当然，这种初衷是希望模型获得通用的世界知识建模能力，如果我们认为模型(T5-encoder)本来就有世界知识，我们只是在&quot;elicit&quot;激发他的知识，那另说。</p><p>在很多情况下，我们希望模型去学习到很多通用的、语言没法表达的世界知识，比如几何题做辅助线、一堆水滴落在水面的样子等等，我们恰好发现。我们发现，这些知识正好都是目前的多模态模型很差的地方……也许，获得这些能力，是需要我们跳出&quot;align&quot;、re-caption的观点，去开发一种直接瞄准获取多模态通用知识的方案。这种方案，简单来说就是&quot;没有输入，直接输出&quot;，和GPT4的训练方式一样</p><p>我认为，在视频模态训练sora是可行的，原因在这里：对于图片生成，尤其是Diffusion model，一个模糊的图片变成清晰一点的图片，在我看来对于语义的依赖性没那么强。反而是视频模态，生成后面的视频，对于前面的依赖性很强，也就是说模型必须在auto-regressive训练中直接捕获到一些世界知识，考虑下面这个：</p><blockquote><p>你从一个鼻子模糊的图片里，参考前面的草图把鼻子画清楚。关键是你猜到这里是鼻子，你本来就会画鼻子，你从中学到的知识并不精确</p><p>看到两个球碰撞前的视频，预测出来碰撞后的样子。你需要理解速度、中心、碰撞点等概念，从中学到的知识表征了世界的更多属性。</p></blockquote><h3 id="recaption-is-NOT-“training-on-synthetic-data”">recaption is NOT “training on synthetic data”</h3><p>最近另外一个比较火的关键词是在合成数据上做训练，recaption算是&quot;在合成数据上做训练&quot;吗？我认为不是，因为recaption是在对已有的数据做改写成更符合要求的形式，本来就有数据。Sora中使用虚幻5渲染出来的视频做训练才是真正的&quot;training on synthetic data&quot;。我认为在合成数据上做训练，本质是想要让模型grokking：希望模型通过更多的数据，最终捕获到了更高级的heuristics。</p><p>举几个例子：</p><ol><li><p>牛顿被苹果砸了，发现了万有引力。只需要一个公式，就能预测所有苹果落地的trace。所以这个高级的heuristics需要更少的计算量就能表征更多的数据</p></li><li><p>小明算1+2+3… +10需要点点算，高斯发现了这个算式可以变成 (1+10)*20/2.很快算完了。但他俩结果是一样的</p></li><li><p>虚幻五是一个渲染视频的引擎，如果&quot;虚幻5&quot;本身是一个模型，用户只需要输入操作虚幻五的&quot;action trace&quot;，就能渲染出一个视频。从compression的角度看，虚幻五把视频世界知识压缩进了自己的参数里，可以把一个长视频压缩到&quot;action trace&quot;这么少的数据量里。Sora很傻，他一直看虚幻5生成的视频，为了对虚幻五进行拙劣的模仿，目前还学的不太对</p></li></ol><blockquote><p>目前的AI还没有牛顿这么聪明，可能被砸一亿个苹果才会突然顿悟，发现公式，这个事情在学界叫做grokking。从这里，我们发现的事实是：在我不知道万有引力时，我可能通过一堆低级方法生成苹果降落的视频，来一直砸牛顿，反正总有一天能砸对。</p><p>在更高级的模型看来，虚幻五也是&quot;傻子&quot;，需要下载好几十GB才能渲染视频，没准他用100M就生成比虚幻五好多了。更高级的模型没准觉得人类也是傻子，测出来卡西米尔效应是-1/12但解释不了，自己知道为什么，只是没办法用自然语言表达……因为人类的数学体系太烂了，整个&quot;数学词表&quot;也就这么大，还是离散的。</p><p>可悲的是，人类自己本来有机会grokking的，但是每个人只能活100年，柯洁学10000年围棋真的还下不过alpha-zero吗……类似的有教育意义的案例，可以参考漫士沉思录对于哈马努金推导的讲解 <a href="https://www.bilibili.com/video/BV1si4y1p75k">全体自然数之和等于-1/12？真相远没有那么简单！</a></p></blockquote><p>所以，1)找到高级heuristics很困难。2)高级和低级heurisitic生成的数据都是正确的。3)越多的原始数据，越多发现高级herusitics的可能。从这三个观点出发，就产生了&quot;training on synthetic data&quot;的想法：盼着模型去grokking。我十分同意这个观点，也相信这才是AGI最可行的路线。</p><p>不过话说回来，不管怎么看，这和recaption都不是一回事，在根本目的上就不一样。</p><h2 id="recaption-is-all-you-need">recaption is all you need?</h2><p>recaption本身有没有在学界的其他领域使用呢？我认为这是一个范式：A是一个比B更难的任务，A-&gt;B 的模型效果不好，就反过来训一个 B -&gt; A的模型，为(A-B) pair提供更多的数据。这个思路很简单、很scalable，并且实际上很多工作都在用，我这里举一些例子：</p><ol><li>WaveCoder：根据query生成code很困难。那么我先找一堆code，对着他生成、改写query，再训练code模型。</li><li>CoachLM：SFT中根据query生成response很困难：那我找到已有的query-response数据对，把query改成更好的、更没有歧义的格式，再训练模型去生成response</li><li>Reformatted Alignment：推理领域，根据query生成CoT很困难。我把原始的数据格式改写一下，让query里告诉模型该怎么写CoT，再生成CoT。由此finetune模型，效果会更好吗？</li></ol><p>我相信，这种范式会在未来变得越来越普遍，甚至扩展到预训练场景(比如CMU今年出的Rephrasing the Web)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近Sora巨火，仿佛开启了AIGC的新时代。Jason Wei表示：“Sora is the GPT-2 moment” for video generation。我在sora发布的大约第5个小时读了technical report，里面最打动我的其实是没提什么细节的recaption技术。让我回想想起了之前读DALL.E 3论文时的愉快体验。&lt;/p&gt;
&lt;p&gt;所以今天来分享一下DALL.E 3论文里的recaption细节，并讨论几个问题和我的看法：1)OpenAI教你为什么要&amp;quot;先查看原始数据，再做创新&amp;quot; 2)Recaption和大家一直在聊的&amp;quot;training on synthetic data&amp;quot;是一回事吗? 3)recaption技术是否已经在(或者即将在)被其他领域使用？&lt;/p&gt;
&lt;p&gt;另外，我总结了一下上篇笔记阅读量大的关键：语言表达要浅显易懂些，所以这篇笔记我可以声明一下：&lt;strong&gt;没学过AI也能看懂&lt;/strong&gt;(我在博客里加了这个标签&amp;quot;from scratch&amp;quot;，所有我认为不懂AI或者只知道一点点的人也能看懂的博客都会加上这个标签)&lt;/p&gt;
&lt;p&gt;参考文献：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://openai.com/sora&quot;&gt;https://openai.com/sora&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Improving Image Generation with Better Captions&lt;/p&gt;
&lt;p&gt;Automatic Instruction Optimization for Open-source LLM Instruction Tuning&lt;/p&gt;
&lt;p&gt;WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation&lt;/p&gt;
&lt;p&gt;Reformatted Alignment&lt;/p&gt;
&lt;p&gt;Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多模态" scheme="https://www.yynnyy.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="Computer Vision" scheme="https://www.yynnyy.cn/tags/Computer-Vision/"/>
    
    <category term="from scratch" scheme="https://www.yynnyy.cn/tags/from-scratch/"/>
    
  </entry>
  
  <entry>
    <title>2024-02-29总结：研一下开始了</title>
    <link href="https://www.yynnyy.cn/e9916de0"/>
    <id>https://www.yynnyy.cn/e9916de0</id>
    <published>2024-02-29T01:02:36.000Z</published>
    <updated>2024-08-09T09:26:46.966Z</updated>
    
    <content type="html"><![CDATA[<p>今天是2月29日，我迎来了研究生的第二个学期。上次2月29日已经是2020年，而下次2月29日要到2028年了。人生有多少4年，再加好久没有更新，遂写一写最近的生活吧。</p><p>其实我写总结这个track，还是因为最开始看了谭院士的博客 <a href="https://twd2.me">Wandai Blog</a>：谭院士总是时间驱动，每天写一个sentence-level的总结，陆陆续续竟然坚持了十几年。时间是有惯性的，有点类似于顺着一个人的微信刷pyq，不会到了某个位置突然被卡掉，看下来有种震撼人心的感觉。所以我也想是不是记录一下自己的生活。</p><p>我当时选了另一种形式：事件感想驱动，更大的interval, 在corpus-level做记录，所以给自己起名字叫做&quot;随缘&quot;。现在想想可能并不适合，我和谭院士的记录方式也许应该倒一倒。我的生活当然没有谭院士丰富，用instruction tuning的话说：每天翻来覆去总是从一些task set里先sample task $t \in \mathcal{T}$，再sample $x \in \mathcal{X}_t$，最后预测 $y = me(x)$。做得多了，熟能生巧，常用的几个task的能力越来越高了，但一直没什么机会探索更大更diverse的instruction空间。</p><p>不过近期确实有所不同，我深感在过去一个月里，尝试的新事物堪比过去一两年。</p><span id="more"></span><p>首先，我这学期第一次当助教了，大家可能以为是一个课的助教，但实际上是三个课的助教……这就是所谓的&quot;半年不开张，开张吃三年&quot;吗。助教的职责主要是帮助同学们留作业判作业，以及在群里回答同学们的疑问。最开始有一些&quot;好为人师&quot;的新鲜感，但很快也就过去了，剩下大致有一些&quot;重构课程体系，提供更好的教学方式，我辈义不容辞&quot;的责任使命感？</p><p>这学期正好轮到NLP课作业体系重构，我同时还是学堂在线Mooc的NLP课的助教，他们两个课的作业体系本来是一致的，我需要把前者重构一下。因为是研究生课，大家都是做研究的，所以我进行了一个相对激进的尝试：我尝试把作业里面的&quot;回答问题、完成任务&quot;式的部分去掉了，变成了鼓励选课同学们对作业框架下的知识产生一些自己的问题并进行探索。站在我的角度，我觉得要求同学们完成某个任务是一个挺反直觉的东西，大家都是AGI，应该有能力自己组织自己的学习路径，发现学习过程对自己帮助最大的东西。等过几天作业发布了，我可以也许会把重构过的内容和我的一些insight以科普blog的形式发出来。不知道大家对我的设计形式会有怎么样的评价。我猜测研究生选课大致有两种用户画像：</p><ol><li>想混混了事，对NLP一点兴趣也没有，得多少分无所谓，破事多我就退课，别耽误我科研/实习/旅游……</li><li>我真的想学习NLP的知识，或者想通过这个课程认识老师，去跟着老师开展相关research。不关心最后得了多少分，当然分数高更好……</li></ol><p>我觉得不管哪种，从结果出发，应该体验都还不错：我判作业给分应该是非常宽松的，实际上我希望的形式应该是大家都能把被动式的完成作业的时间换成主动探索一些自己发现的小现象，而结果上都能拿满分。</p><p>除了当助教给同学们打分，我最近还第一次尝试了审稿。说来有点可惜，人生第一次审的两篇论文，一篇给了strong-reject,一篇给了weak-reject……可能我其实是批判型人格吧。当时看到自己论文的review时有多生气，未来几天别人看我就有多生气。我不太喜欢审稿，因为需要我来总结论文里的优点和缺点。我觉得这个事情有点反直觉：大家发论文是想要对学界做贡献，所以别人会帮你去芜存真，只会关注你的主要贡献点。我认为评价一个论文应该看他最优的优点max(strength)到底有多好，而不应该是strength - weakness、甚至像”木桶效应“一样揪着最大的窟窿不放……可惜我不是管事的，只能给别人打一个reject了</p><p>年初的时候奥特曼说2030年实现AGI，我其实心里挺焦虑的：我在研究AI，如果2030年时AI被解决了，那如果我在这6年里没有贡献关键技术，岂不是整个2020-2030这10年的努力也被抹杀了吗？不过，类比一下互联网或者电器技术的发展，其实在学术上解决一个问题，到实际帮助所有用户解决这个问题，中间可能需要10年、甚至几十年的时间。在这段merge的时间里，大家实际上在做一个技术和服务相互妥协的过程：什么服务对用户很重要，但技术成本上困难所以要砍掉；什么技术很先进很scalable，但会拖累短期里的用户服务质量，就需要砍掉……妥协的结果是平衡，也大概率会是目前的基础框架下的最优形态。整个这个过程在我看来挺像是”机器学习的“，当然这里面的&quot;forward-exploration and backward-gradient&quot;不是某个参数的变大变小，而是某个、某几个公司的成立和消亡。在这个情境下，我们自己也是&quot;世界模型&quot;的其中一个参数……但谁能成为Meta发现的&quot;Massive Activation&quot;呢？</p><p>因此，除了技术本身以外，我最近其实也开始思考技术的落地和产品。比如，我发现了一个叫做”增量“的概念，一个产品或者说系统，最重要的是比起同类产品带给用户的增量价值，而不是这个系统本身的技术先进性。一个技术很先进的东西，可能对用户带来的服务并不好；反之，一个让用户感知很好的产品，其背后的技术不一定很先进。我作为科研或者学术背景出发，常常会受到&quot;选论文&quot;式的归纳偏置，关注技术创新性，而忽略了他的可行性以及增量价值。随着对于产品的调研和感悟越来越多，也许以后，除了阅读笔记以外，我还可以分享一些对于产品的感悟笔记也说不准……</p><blockquote><p>夫人之相与，俯仰一世。或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至；及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀，况修短随化，终期于尽！古人云：“死生亦大矣。”岂不痛哉！</p></blockquote><p>简单来说：下一个2月29日就是2028年了，而下一次在2月29日恰逢&quot;疯狂星期四&quot;更是28年以后了：不知道下个2月29日的我在干什么呢？还会在维护这个blog吗？28年以后的我还在吃疯狂星期四吗？希望那时的我，也能像今天一样喜欢探索学习自己不知道的东西，而不是像大多数人一样，随着年龄增长开始倾向于沉浸在舒适圈里变得over-confidence</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天是2月29日，我迎来了研究生的第二个学期。上次2月29日已经是2020年，而下次2月29日要到2028年了。人生有多少4年，再加好久没有更新，遂写一写最近的生活吧。&lt;/p&gt;
&lt;p&gt;其实我写总结这个track，还是因为最开始看了谭院士的博客 &lt;a href=&quot;https://twd2.me&quot;&gt;Wandai Blog&lt;/a&gt;：谭院士总是时间驱动，每天写一个sentence-level的总结，陆陆续续竟然坚持了十几年。时间是有惯性的，有点类似于顺着一个人的微信刷pyq，不会到了某个位置突然被卡掉，看下来有种震撼人心的感觉。所以我也想是不是记录一下自己的生活。&lt;/p&gt;
&lt;p&gt;我当时选了另一种形式：事件感想驱动，更大的interval, 在corpus-level做记录，所以给自己起名字叫做&amp;quot;随缘&amp;quot;。现在想想可能并不适合，我和谭院士的记录方式也许应该倒一倒。我的生活当然没有谭院士丰富，用instruction tuning的话说：每天翻来覆去总是从一些task set里先sample task $t &#92;in &#92;mathcal{T}$，再sample $x &#92;in &#92;mathcal{X}_t$，最后预测 $y = me(x)$。做得多了，熟能生巧，常用的几个task的能力越来越高了，但一直没什么机会探索更大更diverse的instruction空间。&lt;/p&gt;
&lt;p&gt;不过近期确实有所不同，我深感在过去一个月里，尝试的新事物堪比过去一两年。&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="课程总结" scheme="https://www.yynnyy.cn/tags/%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    
    <category term="随笔" scheme="https://www.yynnyy.cn/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="日记" scheme="https://www.yynnyy.cn/tags/%E6%97%A5%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Self-Consistency之我见，兼More Agents is All You Need</title>
    <link href="https://www.yynnyy.cn/7cd10148"/>
    <id>https://www.yynnyy.cn/7cd10148</id>
    <published>2024-02-10T02:21:00.000Z</published>
    <updated>2024-08-09T09:26:46.970Z</updated>
    
    <content type="html"><![CDATA[<p>好久不更新了，看到之前大约都是15天更新一篇笔记，最近不知道咋回事竟然一个多月没更新，正好这两天刷到了&quot;More Agents is All You Need&quot;，就来讲讲“时间换效果”的鼻祖——self-consistency。如果让模型sample多次，然后做major-voting，效果会更好吗？</p><p>参考文献：</p><blockquote><p>Self-Consistency Improves Chain of Thought Reasoning In Language Models</p><p>Escape Sky-High Cost: Early-Stopping Self-Consistency for Multi-Step Reasoning</p><p>Universal Self-Consistency for Large Language Model Generation</p><p>More Agents is All You Need</p><p>Unlock Predictable Scaling from Emergent Abilities</p></blockquote><span id="more"></span><h2 id="Self-Consistency">Self-Consistency</h2><p>什么是self-consistency？要从CoT算法说起，之前CoT的思路是：如果让模型在输出答案之前，先说一些rationale，最终就能得出相对更准确度的结果。用类似于&quot;Let’s think step by step&quot;的prompt，就能激发模型进行对应的CoT输出。</p><p>而self-consistency则更进一步，如果用更多的CoT放到一起，能不能进一步提高效果？具体来说，模型按照某种随机解码算法，比如p-sample进行多次解码，最后根据多个结果做去重，最后给出众数作为结果。一个经典的例子就是数学计算题</p><img src="../../files/images/self-consistency/sc.png"><p>神奇的是，随着sample次数N的增长，最终的准确率就会逐渐提高，比如下面这个图对比了下面几个东西</p><table><tr>  <td><img src="../../files/images/self-consistency/sc-perf.png" align=left style="zoom:50%;" ></td>  <td><img src="../../files/images/self-consistency/sc-scale.png" align=left style="zoom:45%;" ></td> </tr>    <tr>    <td>随着sample次数的提高，SC算法的效果逐渐提升</td>  <td>在一定的范围内，越强的模型受到sc的增量越多</td>  </tr></table><ul><li>橙色线：greedy-decoding的结果，和N无关</li><li>绿色的线：voting-with-ppl：sample N次，提交对应的ppl最小的，即token-level信心值最高的</li><li>蓝色的线：self-consistency，提交结果的众数</li></ul><p>实际上，self-consistency是一个时间检验的算法，对于各种场景都能几乎稳定地获得提升。早在2021年OpenAI的GSM8K论文，就报告了类似的结果：他们先使用打分器选出最高的topk个样本，再在他们之中选择Major-voting的结果(下右图)，效果比单纯对打分器选择top1的效果好很多</p><img src="../../files/images/self-consistency/gsm8k.png"><h2 id="Self-Consistency之我见：Instance-Level-Calibration与集成学习">Self-Consistency之我见：Instance-Level Calibration与集成学习</h2><p>为什么self-consistency效果会这么好？我下面来聊聊我的看法。</p><p>首先，self-consistency论文中报告了一个有趣的结果：他们发现结果一致性越高的query，真实的准确率也越高</p><table><tr>  <td><img src="../../files/images/self-consistency/sc-calibration.png" align=left style="zoom:50%;" ></td>  <td><img src="../../files/images/self-consistency/gpt4-calibration.png" align=left style="zoom:40%;" ></td> </tr>    <tr>    <td>sc中，结果一致性越高，真实准确率就越高</td>  <td>GPT4 report：选项一致性越高，结果准确率就越高</td>  </tr></table><p>这其实和后来大家发现的calibration现象很像：对于4选1 multi-choice QA场景(比如MMLU)，模型对于4个选项token里信心最高的选项的prob越大，对应题目的最终准确率就越高</p><blockquote><p>实际上，calibration的这种方式在&quot;selective prediction&quot;领域又被称为MaxProb算法，calibration领域大概是&quot;重新发现&quot;了他……</p></blockquote><p>这就促使我去思考，self-consistency既然在instance-level选择概率最大的，并根据可以报告instance-level MaxProb。是不是就是在模拟instance-level的calibration呢？</p><p>instance-level的一致性该怎么表示？如果从刚才maxProb的角度思考的话，直观的想法是使用PPL最低的样本。其实，这个方法也被self-consistency原始论文报告了(绿线)。可以发现，这个算法的效果虽然比greedy-search好，但并不是scalable的，甚至在n很大以后效果会更差。这个现象和GSM8K中best-of-ORM-verifier是一致的，都是随着N的增加先增后减。这是因为ppl或者verifier都是不够鲁棒的。举个例子，对于math场景，ppl的大小不仅受到模型对于expression的信心值，还受到expression表达方式的影响，考虑下面两个句子：</p><blockquote><p>3*5=15</p><p>3乘以5的结果是15</p></blockquote><p>对于模型来说，ppl是生成每个token的平均概率。这两个句子虽然表达相同的意思，但是表达形式不同，“乘以”、“的”、“结果”这几个token的信心值都会很高，所以导致两个句子的ppl差很远。然而，其实模型对于3*5=15这个表达式的信心值是固定的。这也就会导致，best-of-ppl的算法在N很多以后，模型可能偶然发现一些很简单的表达形式，ppl很低。但这个这只是模型对于“表达方式”的信心值。类似的，best-of-verifier也会有类似的情况，这就是bad-heuristics，或者叫adversarial-solutions的问题。除此之外，还有rationale方式的问题，一个结果可以用不同的思考路径所表达，就像是一个题目会有多种方法去做。</p><p>相比之下我们发现，Multi-choice QA场景没有表达形式的bias，因为四个选项都是使用一个token表达的。因此，直接使用选项token的maxProb就可以做出相对很准确的准确率估计。</p><p>major-voting是一个道理，如果我们sample N次对结果划分出不同等价类的话，可以发现，我们实际上得到了不同结果的频度。当N变得很大时，这个趋向于了不同结果的概率分布。从这个instance-level概率分布取最大概率，就是instance-level的maxProb。他不会受到表达形式的影响，因为大家都是按照同样的算法采样，每个算法都有可能以困难的形式被表达，也有可能用简单的方式表达。</p><blockquote><p>从直觉来看，self-consistency的效果提升其实也很容易被理解。我们可以类似于shengding学长的论文去定义出pass-until：sample N次，其中答案正确的比例。</p><p>假如pass-until = 60%，那么显然self-consistency一定会把正确答案投票出来。但如果只sample一次，就有40%的概率做错。对偶的情况是，pass-until != result-maxProb时，self-consistency一定做不对。但是如果只sample一次，有可能反而能找到正确答案(虽然这个概率估计很低)。</p></blockquote><p>self-consistency的提升点，实际上是在两种情况的博弈。想要观察self-consistency的提升点，也许需要列出来一个数据集中所有样本的pass-until的直方图来观察。实际上，这个直方图恰恰就是上面展示过的&quot;带权&quot;版本的sc-consistency图。另外，我认为实际上应该使用self-consistency的结果（而不是greedy-search）作为模型对于一个数据集的performance。</p><p>另外，self-consistency主要是考虑用同样的随机解码算法做拟合，能不能考虑算法本身的异构呢？</p><ul><li>用不同的模型结果做综合</li><li>不用的解码算法的结果做综合</li><li>不同的system-prompt的结果做综合</li><li>甚至是multi-agent system中不同agent通过类似debate的形式做沟通博弈…</li><li>google还做过综合不同的reward model来进行RLHF训练</li></ul><p>这些思路也自古有之，叫做集成学习。出自一个朴素的直觉：条条大路通罗马，不同的方法都相对认可的东西，实际正确率也更高。所以说，self-consistency集成不同的思考链，其实是集成学习的一种in-context版本的特例，自然也可以集成更diverse的自由度。</p><p>最后，self-consistency根据结果的等价类做划分，得出来了instance-level的calibration，这个事情能不能再step-level做呢？虽然没有人formalize这个问题，但我在一些独立的工作中看到了类似的解决方案：</p><blockquote><ol><li>shunyu yao的ToT：其中的BFS baseline在step-level进行self-vote。对偶的，可以设计self-consistency。</li><li>所有的PRM相关论文：let’s veriffy step-by step 、Step-Aware Verifier、math-shepherd、Discriminator-Guided CoT、outcome-supervised verfier等等，都可以在step-level设计look-ahead搜索算法</li><li>More Agents is All You Need中的hierarchical sample-and-voting</li></ol></blockquote><p>再推广一步，到最细力度的token-level，一致性是天生就存在的(logits)。那么就会有beam-search等算法去寻找ppl最低的结果</p><h2 id="Self-Consistency的应用与改进">Self-Consistency的应用与改进</h2><p>self-consistency有两个巨大的问题和一个小问题。改进方法大致也从这些问题出发</p><h3 id="大问题1：需要某场景可以划分等价类">大问题1：需要某场景可以划分等价类</h3><p>对于数学场景，可以认为结果相同就是一个等价类。但对于很多free-form的场景，比如数学证明题，比如写代码，比如机器翻译该怎么办？写代码任务，类似于google AlphaCode，会根据代码中每个测例的输出，按照对拍结果划分等价类。这依赖于一个外置的执行器。</p><p>既然ToT可以做self-vote，那self-consistency能不能行呢？最近，denny Zhou做了一篇工作，可以让模型根据多个样本都放在prompt里，然后让模型自己说一个最一致的结果出来。那么，代价是什么呢？需要模型可以同时存下所有的结果在context里！</p><p>另外，刚才提到了step-level的self-consistency，这个更复杂，依赖于对step划分等价类。首先，不是所有任务都能像ReACT场景一样划分成天生多步的。其次，很多任务的多个执行链其实不是按照树的格式组织的，是按照图的形式组织的。所以，更细粒度的step-search依赖于更细粒度的等价类划分</p><h3 id="大问题2：需要更多的计算资源">大问题2：需要更多的计算资源</h3><p>self-consistency动辄就需要一个query sample N=40次。对于GPT4来说，在MATH数据集sample 40次，需要大概2000美元。这显然不可接受，最近有很多工作试图减少self-consistency的效果。</p><p>从直觉上来看，如果只sample 10次，结果都是一样的。理论上这个很可能是就是众数结果，最极端的情况是出现类似斯诺克中的&quot;超分&quot;：即使后面所有的结果都是目前第二大的，也不能超过最大的。从统计学上对这个直觉做建模，就是狄利克雷分布。很多工作会涉及一些基于熵的算法去拟合self-consistency的一致性要求。</p><h3 id="小问题：需要一个场景的天然假阳率不大">小问题：需要一个场景的天然假阳率不大</h3><p>最后这个，本质上不是一个问题，而是一个现象。举个例子，对于判断正误的问题，答案只可能是对或者不对，就两种可能。随机猜都有50%准确率，这时候有非常多的假阳情况。self-consistency拟合出来的一致性，就会受到影响：假阳的样本往往都是eazy-heuristic，一致性都挺高的。</p><p>所以，应用self-consistency的时候，可能还需挑场景，最好选择类似计算题的场景：结果正确，那么基本上过程也是正确的。</p><h2 id="我的思考">我的思考</h2><p>之前的论文阅读笔记大多是照着论文的展开顺序讲。最近读的多了以后，渐渐感觉很多东西和某些别的领域有更多的相关性，把别的领域的直觉拿来解释，可能相对更清晰。这也是我现在读论文的一个经验。比起理解论文的素有方法，更重要的是理解论文背后的直觉：</p><ul><li>一方面，理解了直觉理论上可以把论文的方法重新推导出来</li><li>另一方面，论文的方法不一定都是管用的，有一些“为了创新而创新”的工作量证明的部分。从直觉出发，更容易剔除掉这些&quot;noise&quot;……</li></ul><blockquote><p>对于这个问题，有人说：AI的论文没什么内容，创新都很简单。我认为恰恰相反，理解起来简单是因为讲得好。如果一个论文讲的神头鬼脸、玄之又玄，那大概率是作者自己都没搞清楚自己论文的“first principle”——背后的直觉到底是什么</p></blockquote><p>最后，self-consistency是一个时间检验的方法，对于所有模型、对于所有场景都能有提升。之前jason wei发了一个twitter，认为CoT和self-consistency的效果提升来源于数据和问题的固有熵，对此我产生了一些不同的见解，贴出来我的看法：</p><p>I don’t entirely agree with this viewpoint, here are some of my insights:</p><ol><li>CoT aims to use a nearly identical, substantial amount of computation for both difficult and simple problems to ensure performance. Thus, recent works like GSM8K-zero (<a href="https://t.co/JJO0tQkHSP">https://arxiv.org/abs/2401.11467</a>) have found that CoT can waste a lot of computational resources on simple(trivial) problems.</li><li>The information density in the corpus(the varying difficulty of predicting each token) is an intrinsic property of datasets (e.g., predicting the answer-token on MMLU is harder than predicting query-tokens), which is why speculative decoding succeeds in some settings and not in others.</li><li>Instead of fitting corpora with uniform information density, it’s better to retain the dataset’s intrinsic properties and instead adapt the model computation to fit the difficulty of the corpus, i.e., adaptive computation.</li><li>On the instance-level/step-level, this is what “flow-engineering” is about: Allowing for dynamic decision-making on the complexity of CoT based on the task.</li><li>At the token-level, there are passive solutions like pause-token (<a href="https://t.co/dYnr2CEAqI">https://arxiv.org/abs/2310.02226</a>) that use more forward passes on some manually selected tokens. There are also active solutions like early-exit (<a href="https://t.co/5TNCd76Zcy">https://arxiv.org/abs/2402.00518</a>), which allow the model to freely decide the computational effort for each token. And, though speculative decoding has a different goal, it acutually achieves the purpose of adaptive computation.</li><li>Besides point 5, I also think Mixture of Experts (MoE) technology has achieved part of this goal: Firstly, early works in MoE could dynamically decide the activated number of experts and implement heterogeneity of experts (different experts have different size) to achieve the purpose of adaptive computation. Secondly, OpenMoE (<a href="https://t.co/li4vngcGYr">https://github.com/XueFuzhao/OpenMoE</a>) discovered that even with a fixed number and all the same size of experts, a phenomenon occurs where “one expert is always responsible for some tokens”. And we all that token-id represents part of the predictive difficulty (e.g., predicting punctuation is often easier than predicting numbers)…</li></ol><p>I believe that future models can freely and acitvely decide on computational complexity based on the task’s information density, just like us: Think Before You Speak.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;好久不更新了，看到之前大约都是15天更新一篇笔记，最近不知道咋回事竟然一个多月没更新，正好这两天刷到了&amp;quot;More Agents is All You Need&amp;quot;，就来讲讲“时间换效果”的鼻祖——self-consistency。如果让模型sample多次，然后做major-voting，效果会更好吗？&lt;/p&gt;
&lt;p&gt;参考文献：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Self-Consistency Improves Chain of Thought Reasoning In Language Models&lt;/p&gt;
&lt;p&gt;Escape Sky-High Cost: Early-Stopping Self-Consistency for Multi-Step Reasoning&lt;/p&gt;
&lt;p&gt;Universal Self-Consistency for Large Language Model Generation&lt;/p&gt;
&lt;p&gt;More Agents is All You Need&lt;/p&gt;
&lt;p&gt;Unlock Predictable Scaling from Emergent Abilities&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="Reasoning" scheme="https://www.yynnyy.cn/tags/Reasoning/"/>
    
  </entry>
  
  <entry>
    <title>arxiv-insights</title>
    <link href="https://www.yynnyy.cn/f4243ee6"/>
    <id>https://www.yynnyy.cn/f4243ee6</id>
    <published>2024-01-15T02:58:32.000Z</published>
    <updated>2025-05-24T03:50:25.526Z</updated>
    
    <content type="html"><![CDATA[<p>压缩带来智能，5% 的论文决定学术界 95% 的成果！每天从 Arxiv 论文中总结分享最重要、最有趣的最多三篇论文。</p><p>Compression brings intelligence, 5% of papers determine 95% of AI technologies! Share the most important papers from Arxiv, every day, up to three!</p><script type="text/javascript">    var insight_now_id = 0;    var insight_max_id = 20;    function tips_insight(num){        document.getElementById("insights_"+insight_now_id).hidden = "hidden";        insight_now_id -= num;        if (insight_now_id > insight_max_id) {insight_now_id = insight_max_id;}        if (insight_now_id < 0) {insight_now_id = 0;}        document.getElementById("insights_" + insight_now_id).hidden = "";    }</script><table id='insights_0' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2025年五月May</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td><td bgcolor=#00E800><a href=../7ef3a1d2>1(210->3 papers)</a></td><td bgcolor=#00E500><a href=../95c41ad1>2(173->3 papers)</a></td><td>3 </td></tr><tr><td>4 </td><td bgcolor=#00EE00><a href=../771801a8>5(153->2 papers)</a></td><td bgcolor=#00F400><a href=../9c2fbaab>6(365->2 papers)</a></td><td bgcolor=#00F900><a href=../73edd195>7(221->1 papers)</a></td><td bgcolor=#00FF00><a href=../82e68a18>8(239->0 papers)</a></td><td bgcolor=#00F700><a href=../6d24e126>9(225->1 papers)</a></td><td>10 </td></tr><tr><td>11 </td><td bgcolor=#00F800><a href=../544ac511>12(182->1 papers)</a></td><td bgcolor=#00ED00><a href=../bb88ae2f>13(413->3 papers)</a></td><td bgcolor=#00F700><a href=../5954b556>14(265->1 papers)</a></td><td bgcolor=#00ED00><a href=../b696de68>15(200->3 papers)</a></td><td bgcolor=#00F100><a href=../5da1656b>16(202->2 papers)</a></td><td>17 </td></tr><tr><td>18 </td><td bgcolor=#00F600><a href=../acaa3ee6>19(292->1 papers)</a></td><td bgcolor=#00E100><a href=../c95d732d>20(701->4 papers)</a></td><td>21 </td><td>22 </td><td>23 </td><td>24 </td></tr><tr><td>25 </td><td>26 </td><td>27 </td><td>28 </td><td>29 </td><td>30 </td><td>31 </td></tr><tr></tr></table><table id='insights_1' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2025年四月April</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td><td bgcolor=#00F000><a href=../a3657857>1(472->1 papers)</a></td><td bgcolor=#00E700><a href=../4852c354>2(266->3 papers)</a></td><td bgcolor=#00E300><a href=../a790a86a>3(218->3 papers)</a></td><td bgcolor=#00F000><a href=../454cb313>4(224->2 papers)</a></td><td>5 </td></tr><tr><td>6 </td><td bgcolor=#00F500><a href=../ae7b0810>7(201->1 papers)</a></td><td bgcolor=#00F200><a href=../5f70539d>8(478->2 papers)</a></td><td bgcolor=#00FF00><a href=../b0b238a3>9(249->0 papers)</a></td><td bgcolor=#00F700><a href=../8d29cca9>10(204->1 papers)</a></td><td bgcolor=#00EF00><a href=../62eba797>11(194->2 papers)</a></td><td>12 </td></tr><tr><td>13 </td><td bgcolor=#00F400><a href=../84c26cd3>14(195->2 papers)</a></td><td bgcolor=#00EE00><a href=../6b0007ed>15(479->2 papers)</a></td><td bgcolor=#00FA00><a href=../8037bcee>16(290->1 papers)</a></td><td bgcolor=#00F500><a href=../6ff5d7d0>17(206->1 papers)</a></td><td bgcolor=#00E200><a href=../9efe8c5d>18(257->4 papers)</a></td><td>19 </td></tr><tr><td>20 </td><td bgcolor=#00F500><a href=../fb09c196>21(195->1 papers)</a></td><td bgcolor=#00F200><a href=../103e7a95>22(441->2 papers)</a></td><td bgcolor=#00EB00><a href=../fffc11ab>23(188->2 papers)</a></td><td bgcolor=#00F600><a href=../1d200ad2>24(192->1 papers)</a></td><td bgcolor=#00FF00><a href=../f2e261ec>25(192->0 papers)</a></td><td>26 </td></tr><tr><td>27 </td><td bgcolor=#00F700><a href=../71cea5c>28(175->1 papers)</a></td><td bgcolor=#00FF00><a href=../e8de8162>29(365->0 papers)</a></td><td bgcolor=#00F000><a href=../d5457568>30(219->2 papers)</a></td></tr></table><table id='insights_2' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2025年三月March</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td>1 </td></tr><tr><td>2 </td><td bgcolor=#00F300><a href=../dae3ae32>3(236->1 papers)</a></td><td bgcolor=#00E800><a href=../383fb54b>4(570->3 papers)</a></td><td bgcolor=#00F000><a href=../d7fdde75>5(307->2 papers)</a></td><td bgcolor=#00FE00><a href=../3cca6576>6(197->0 papers)</a></td><td bgcolor=#00EC00><a href=../d3080e48>7(227->2 papers)</a></td><td>8 </td></tr><tr><td>9 </td><td bgcolor=#00E900><a href=../f05acaf1>10(295->3 papers)</a></td><td bgcolor=#00E500><a href=../1f98a1cf>11(724->3 papers)</a></td><td bgcolor=#00E500><a href=../f4af1acc>12(381->3 papers)</a></td><td bgcolor=#00F500><a href=../1b6d71f2>13(272->1 papers)</a></td><td bgcolor=#00E100><a href=../f9b16a8b>14(298->3 papers)</a></td><td>15 </td></tr><tr><td>16 </td><td bgcolor=#00F200><a href=../1286d188>17(331->1 papers)</a></td><td bgcolor=#00E800><a href=../e38d8a05>18(578->4 papers)</a></td><td bgcolor=#00F100><a href=../c4fe13b>19(328->2 papers)</a></td><td bgcolor=#00F900><a href=../69b8acf0>20(278->1 papers)</a></td><td bgcolor=#00EE00><a href=../867ac7ce>21(291->2 papers)</a></td><td>22 </td></tr><tr><td>23 </td><td bgcolor=#00F500><a href=../60530c8a>24(314->1 papers)</a></td><td bgcolor=#00F000><a href=../8f9167b4>25(515->1 papers)</a></td><td bgcolor=#00EA00><a href=../64a6dcb7>26(296->3 papers)</a></td><td bgcolor=#00F900><a href=../8b64b789>27(209->1 papers)</a></td><td bgcolor=#00EB00><a href=../7a6fec04>28(241->2 papers)</a></td><td>29 </td></tr><tr><td>30 </td><td bgcolor=#00EF00><a href=../47f4180e>31(250->2 papers)</a></td></tr></table><table id='insights_3' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2025年二月February</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td>1 </td></tr><tr><td>2 </td><td bgcolor=#00E900><a href=../77577b7>3(217->2 papers)</a></td><td>4 </td><td>5 </td><td bgcolor=#00F800><a href=../e15cbcf3>6(182->1 papers)</a></td><td bgcolor=#00FF00><a href=../e9ed7cd>7(187->0 papers)</a></td><td>8 </td></tr><tr><td>9 </td><td bgcolor=#00E700><a href=../2dcc1374>10(245->2 papers)</a></td><td bgcolor=#00E700><a href=../c20e784a>11(431->3 papers)</a></td><td bgcolor=#00E500><a href=../2939c349>12(324->3 papers)</a></td><td bgcolor=#00E800><a href=../c6fba877>13(218->3 papers)</a></td><td bgcolor=#00F700><a href=../2427b30e>14(253->1 papers)</a></td><td>15 </td></tr><tr><td>16 </td><td bgcolor=#00FF00><a href=../cf10080d>17(204->0 papers)</a></td><td bgcolor=#00DF00><a href=../3e1b5380>18(518->4 papers)</a></td><td bgcolor=#00F800><a href=../d1d938be>19(195->1 papers)</a></td><td bgcolor=#00F900><a href=../b42e7575>20(195->1 papers)</a></td><td bgcolor=#00F600><a href=../5bec1e4b>21(247->1 papers)</a></td><td>22 </td></tr><tr><td>23 </td><td bgcolor=#00F000><a href=../bdc5d50f>24(264->2 papers)</a></td><td bgcolor=#00E500><a href=../5207be31>25(550->4 papers)</a></td><td bgcolor=#00E600><a href=../b9300532>26(264->3 papers)</a></td><td bgcolor=#00F400><a href=../56f26e0c>27(251->1 papers)</a></td><td bgcolor=#00F500><a href=../a7f93581>28(258->1 papers)</a></td></tr></table><table id='insights_4' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2025年一月January</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td><td>1 </td><td>2 </td><td bgcolor=#00F000><a href=../babf1b79>3(360->2 papers)</a></td><td>4 </td></tr><tr><td>5 </td><td bgcolor=#00F200><a href=../5c96d03d>6(143->2 papers)</a></td><td bgcolor=#00F200><a href=../b354bb03>7(371->2 papers)</a></td><td bgcolor=#00F500><a href=../425fe08e>8(171->1 papers)</a></td><td bgcolor=#00FF00><a href=../ad9d8bb0>9(138->0 papers)</a></td><td bgcolor=#00F800><a href=../90067fba>10(172->1 papers)</a></td><td>11 </td></tr><tr><td>12 </td><td bgcolor=#00EF00><a href=../7b31c4b9>13(155->2 papers)</a></td><td bgcolor=#00EC00><a href=../99eddfc0>14(375->3 papers)</a></td><td bgcolor=#00F800><a href=../762fb4fe>15(196->1 papers)</a></td><td bgcolor=#00FF00><a href=../9d180ffd>16(155->0 papers)</a></td><td bgcolor=#00F000><a href=../72da64c3>17(187->2 papers)</a></td><td>18 </td></tr><tr><td>19 </td><td bgcolor=#00EA00><a href=../9e419bb>20(136->3 papers)</a></td><td>21 </td><td bgcolor=#00D000><a href=../d11c986>22(445->4 papers)</a></td><td bgcolor=#00EA00><a href=../e2d3a2b8>23(138->3 papers)</a></td><td bgcolor=#00EF00><a href=../fb9c1>24(191->2 papers)</a></td><td>25 </td></tr><tr><td>26 </td><td bgcolor=#00F700><a href=../eb3802c2>27(239->1 papers)</a></td><td bgcolor=#00E700><a href=../1a33594f>28(383->3 papers)</a></td><td bgcolor=#00F800><a href=../f5f13271>29(196->1 papers)</a></td><td bgcolor=#00F500><a href=../c86ac67b>30(132->1 papers)</a></td><td>31 </td></tr></table><table id='insights_5' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年十二月December</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr><td>1 </td><td bgcolor=#00F000><a href=../dbd06c6e>2(349->2 papers)</a></td><td bgcolor=#00E500><a href=../'34120750'>3(618->4 papers)</a></td><td bgcolor=#00EE00><a href=../d6ce1c29>4(260->1 papers)</a></td><td bgcolor=#00F100><a href=../390c7717>5(244->2 papers)</a></td><td bgcolor=#00E700><a href=../d23bcc14>6(293->3 papers)</a></td><td>7 </td></tr><tr><td>8 </td><td bgcolor=#00E700><a href=../'23309799'>9(240->3 papers)</a></td><td bgcolor=#00E300><a href=../1eab6393>10(446->3 papers)</a></td><td bgcolor=#00F300><a href=../f16908ad>11(294->2 papers)</a></td><td bgcolor=#00EF00><a href=../1a5eb3ae>12(248->2 papers)</a></td><td bgcolor=#00E600><a href=../f59cd890>13(255->3 papers)</a></td><td>14 </td></tr><tr><td>15 </td><td bgcolor=#00F100><a href=../13b513d4>16(224->2 papers)</a></td><td bgcolor=#00E900><a href=../fc7778ea>17(572->3 papers)</a></td><td bgcolor=#00F100><a href=../d7c2367>18(334->2 papers)</a></td><td bgcolor=#00F000><a href=../e2be4859>19(271->1 papers)</a></td><td bgcolor=#00F800><a href=../'87490592'>20(269->1 papers)</a></td><td>21 </td></tr><tr><td>22 </td><td bgcolor=#00F100><a href=../6c7ebe91>23(249->2 papers)</a></td><td bgcolor=#00ED00><a href=../8ea2a5e8>24(521->3 papers)</a></td><td bgcolor=#00EF00><a href=../6160ced6>25(231->2 papers)</a></td><td>26 </td><td>27 </td><td>28 </td></tr><tr><td>29 </td><td bgcolor=#00F300><a href=../46c7da52>30(334->2 papers)</a></td><td bgcolor=#00FF00><a href=../a905b16c>31(324->0 papers)</a></td></tr></table><table id='insights_6' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年十一月November</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td bgcolor=#00E100><a href=../8d2dbba3>1(223->4 papers)</a></td><td>2 </td></tr><tr><td>3 </td><td bgcolor=#00DD00><a href=../6b0470e7>4(214->5 papers)</a></td><td bgcolor=#00DA00><a href=../84c61bd9>5(472->5 papers)</a></td><td bgcolor=#00EF00><a href=../6ff1a0da>6(250->2 papers)</a></td><td bgcolor=#00F500><a href=../8033cbe4>7(250->1 papers)</a></td><td bgcolor=#00CC00><a href=../'71389069'>8(206->7 papers)</a></td><td>9 </td></tr><tr><td>10 </td><td bgcolor=#00EE00><a href=../4ca36463>11(161->2 papers)</a></td><td bgcolor=#00D900><a href=../a794df60>12(405->5 papers)</a></td><td bgcolor=#00F100><a href=../4856b45e>13(199->2 papers)</a></td><td bgcolor=#00F100><a href=../aa8aaf27>14(171->2 papers)</a></td><td bgcolor=#00F600><a href=../4548c419>15(191->1 papers)</a></td><td>16 </td></tr><tr><td>17 </td><td bgcolor=#00EB00><a href=../b0b64fa9>18(199->3 papers)</a></td><td bgcolor=#00E200><a href=../5f742497>19(345->4 papers)</a></td><td bgcolor=#00F700><a href=../3a83695c>20(205->1 papers)</a></td><td bgcolor=#00E500><a href=../d5410262>21(199->3 papers)</a></td><td bgcolor=#00E700><a href=../3e76b961>22(211->3 papers)</a></td><td>23 </td></tr><tr><td>24 </td><td bgcolor=#00E700><a href=../dcaaa218>25(217->3 papers)</a></td><td bgcolor=#00E500><a href=../379d191b>26(490->3 papers)</a></td><td bgcolor=#00EE00><a href=../d85f7225>27(332->2 papers)</a></td><td bgcolor=#00E700><a href=../295429a8>28(270->3 papers)</a></td><td>29 </td><td>30 </td></tr><tr></tr></table><table id='insights_7' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年十月October</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td><td bgcolor=#00E400><a href=../50bb6226>1(172->4 papers)</a></td><td bgcolor=#00F700><a href=../bb8cd925>2(74->1 papers)</a></td><td bgcolor=#00E400><a href=../544eb21b>3(103->3 papers)</a></td><td bgcolor=#00D300><a href=../b692a962>4(128->6 papers)</a></td><td>5 </td></tr><tr><td>6 </td><td bgcolor=#00E400><a href=../5da51261>7(121->2 papers)</a></td><td bgcolor=#00D700><a href=../acae49ec>8(224->5 papers)</a></td><td>9 </td><td>10 </td><td>11 </td><td>12 </td></tr><tr><td>13 </td><td>14 </td><td>15 </td><td bgcolor=#00E600><a href=../73e9a69f>16(133->3 papers)</a></td><td bgcolor=#00E300><a href=../9c2bcda1>17(156->3 papers)</a></td><td>18 </td><td>19 </td></tr><tr><td>20 </td><td bgcolor=#00F600><a href=../8d7dbe7>21(125->1 papers)</a></td><td bgcolor=#00E900><a href=../e3e060e4>22(198->3 papers)</a></td><td bgcolor=#00EB00><a href=../c220bda>23(135->3 papers)</a></td><td bgcolor=#00F900><a href=../eefe10a3>24(122->1 papers)</a></td><td bgcolor=#00F200><a href=../13c7b9d>25(145->2 papers)</a></td><td>26 </td></tr><tr><td>27 </td><td bgcolor=#00D500><a href=../f4c2f02d>28(172->5 papers)</a></td><td bgcolor=#00E200><a href=../1b009b13>29(431->4 papers)</a></td><td bgcolor=#00DB00><a href=../269b6f19>30(261->4 papers)</a></td><td bgcolor=#00D500><a href=../c9590427>31(260->4 papers)</a></td></tr></table><table id='insights_8' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年九月September</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr><td>1 </td><td bgcolor=#00F800><a href=../a11632bb>2(30->1 papers)</a></td><td>3 </td><td bgcolor=#00DB00><a href=../ac0842fc>4(169->4 papers)</a></td><td bgcolor=#00E900><a href=../43ca29c2>5(52->3 papers)</a></td><td bgcolor=#00F500><a href=../a8fd92c1>6(46->1 papers)</a></td><td>7 </td></tr><tr><td>8 </td><td bgcolor=#00FE00><a href=../59f6c94c>9(33->0 papers)</a></td><td bgcolor=#00E300><a href=../646d3d46>10(66->3 papers)</a></td><td bgcolor=#00F700><a href=../8baf5678>11(49->1 papers)</a></td><td bgcolor=#00F600><a href=../6098ed7b>12(38->1 papers)</a></td><td bgcolor=#00F700><a href=../8f5a8645>13(32->1 papers)</a></td><td>14 </td></tr><tr><td>15 </td><td bgcolor=#00FE00><a href=../69734d01>16(39->0 papers)</a></td><td bgcolor=#00EB00><a href=../86b1263f>17(101->2 papers)</a></td><td bgcolor=#00F300><a href=../77ba7db2>18(78->1 papers)</a></td><td bgcolor=#00E700><a href=../9878168c>19(53->4 papers)</a></td><td bgcolor=#00E900><a href=../fd8f5b47>20(55->3 papers)</a></td><td>21 </td></tr><tr><td>22 </td><td bgcolor=#00F900><a href=../16b8e044>23(56->1 papers)</a></td><td bgcolor=#00DA00><a href=../f464fb3d>24(187->4 papers)</a></td><td bgcolor=#00F600><a href=../1ba69003>25(79->1 papers)</a></td><td bgcolor=#00F700><a href=../f0912b00>26(73->1 papers)</a></td><td bgcolor=#00E900><a href=../1f53403e>27(84->3 papers)</a></td><td>28 </td></tr><tr><td>29 </td><td bgcolor=#00EE00><a href=../3c018487>30(53->2 papers)</a></td></tr></table><table id='insights_9' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年八月August</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td><td>1 </td><td>2 </td><td>3 </td></tr><tr><td>4 </td><td>5 </td><td>6 </td><td>7 </td><td>8 </td><td>9 </td><td>10 </td></tr><tr><td>11 </td><td bgcolor=#00F500><a href=../bd0e34fe>12(73->1 papers)</a></td><td bgcolor=#00F200><a href=../52cc5fc0>13(68->2 papers)</a></td><td bgcolor=#00F500><a href=../b01044b9>14(53->1 papers)</a></td><td bgcolor=#00F700><a href=../5fd22f87>15(41->1 papers)</a></td><td bgcolor=#00F500><a href=../b4e59484>16(36->1 papers)</a></td><td>17 </td></tr><tr><td>18 </td><td bgcolor=#00F900><a href=../45eecf09>19(45->1 papers)</a></td><td bgcolor=#00F100><a href=../201982c2>20(102->2 papers)</a></td><td bgcolor=#00ED00><a href=../cfdbe9fc>21(67->2 papers)</a></td><td bgcolor=#00E700><a href=../24ec52ff>22(58->2 papers)</a></td><td bgcolor=#00F900><a href=../cb2e39c1>23(78->1 papers)</a></td><td>24 </td></tr><tr><td>25 </td><td bgcolor=#00F700><a href=../2d07f285>26(36->1 papers)</a></td><td bgcolor=#00F000><a href=../c2c599bb>27(76->1 papers)</a></td><td bgcolor=#00F600><a href=../33cec236>28(50->1 papers)</a></td><td bgcolor=#00F500><a href=../dc0ca908>29(53->1 papers)</a></td><td bgcolor=#00F100><a href=../e1975d02>30(40->2 papers)</a></td><td>31 </td></tr><tr></tr></table><table id='insights_10' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年七月July</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td><td bgcolor=#00F600><a href=../b0c78508>1(68->1 papers)</a></td><td bgcolor=#00E100><a href=../5bf03e0b>2(155->3 papers)</a></td><td bgcolor=#00F500><a href=../b4325535>3(96->1 papers)</a></td><td bgcolor=#00F000><a href=../56ee4e4c>4(87->2 papers)</a></td><td>5 </td><td>6 </td></tr><tr><td>7 </td><td bgcolor=#00ED00><a href=../4cd2aec2>8(121->2 papers)</a></td><td bgcolor=#00ED00><a href=../a310c5fc>9(106->2 papers)</a></td><td bgcolor=#00F100><a href=../9e8b31f6>10(69->2 papers)</a></td><td bgcolor=#00ED00><a href=../71495ac8>11(45->2 papers)</a></td><td bgcolor=#00EE00><a href=../9a7ee1cb>12(46->2 papers)</a></td><td>13 </td></tr><tr><td>14 </td><td bgcolor=#00F900><a href=../78a2fab2>15(49->1 papers)</a></td><td bgcolor=#00EA00><a href=../939541b1>16(112->2 papers)</a></td><td bgcolor=#00F500><a href=../7c572a8f>17(97->1 papers)</a></td><td>18 </td><td bgcolor=#00E400><a href=../629e1a3c>19(129->3 papers)</a></td><td>20 </td></tr><tr><td>21 </td><td bgcolor=#00F500><a href=../39c87ca>22(50->1 papers)</a></td><td bgcolor=#00E500><a href=../ec5eecf4>23(105->3 papers)</a></td><td bgcolor=#00F400><a href=../e82f78d>24(54->1 papers)</a></td><td bgcolor=#00F800><a href=../e1409cb3>25(49->1 papers)</a></td><td bgcolor=#00EF00><a href=../a7727b0>26(60->2 papers)</a></td><td>27 </td></tr><tr><td>28 </td><td>29 </td><td>30 </td><td>31 </td></tr></table><table id='insights_11' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年六月June</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td>1 </td></tr><tr><td>2 </td><td bgcolor=#00F500><a href=../69a48cb0>3(77->1 papers)</a></td><td bgcolor=#00E500><a href=../8b7897c9>4(153->3 papers)</a></td><td bgcolor=#00F100><a href=../64bafcf7>5(93->2 papers)</a></td><td bgcolor=#00F900><a href=../8f8d47f4>6(90->1 papers)</a></td><td bgcolor=#00E900><a href=../604f2cca>7(84->3 papers)</a></td><td>8 </td></tr><tr><td>9 </td><td bgcolor=#00F500><a href=../431de873>10(80->1 papers)</a></td><td bgcolor=#00E400><a href=../acdf834d>11(150->3 papers)</a></td><td bgcolor=#00E800><a href=../47e8384e>12(148->3 papers)</a></td><td bgcolor=#00E700><a href=../a82a5370>13(83->3 papers)</a></td><td bgcolor=#00F800><a href=../4af64809>14(88->1 papers)</a></td><td>15 </td></tr><tr><td>16 </td><td bgcolor=#00EB00><a href=../a1c1f30a>17(72->2 papers)</a></td><td bgcolor=#00E600><a href=../50caa887>18(310->4 papers)</a></td><td bgcolor=#00E500><a href=../bf08c3b9>19(161->3 papers)</a></td><td>20 </td><td bgcolor=#00E600><a href=../353de54c>21(223->3 papers)</a></td><td>22 </td></tr><tr><td>23 </td><td bgcolor=#00F700><a href=../d3142e08>24(100->1 papers)</a></td><td bgcolor=#00ED00><a href=../3cd64536>25(188->2 papers)</a></td><td bgcolor=#00F000><a href=../d7e1fe35>26(101->2 papers)</a></td><td bgcolor=#00E600><a href=../3823950b>27(101->3 papers)</a></td><td bgcolor=#00EC00><a href=../c928ce86>28(78->2 papers)</a></td><td>29 </td></tr><tr><td>30 </td></tr></table><table id='insights_12' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年五月May</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td><td bgcolor=#00EB00><a href=../d09b3043>1(64->2 papers)</a></td><td bgcolor=#00EB00><a href=../3bac8b40>2(71->2 papers)</a></td><td bgcolor=#00F500><a href=../d46ee07e>3(47->1 papers)</a></td><td>4 </td></tr><tr><td>5 </td><td bgcolor=#00F500><a href=../32472b3a>6(69->1 papers)</a></td><td bgcolor=#00E400><a href=../dd854004>7(82->4 papers)</a></td><td bgcolor=#00F000><a href=../2c8e1b89>8(41->2 papers)</a></td><td bgcolor=#00F600><a href=../c34c70b7>9(36->1 papers)</a></td><td bgcolor=#00F500><a href=../fed784bd>10(42->1 papers)</a></td><td>11 </td></tr><tr><td>12 </td><td bgcolor=#00F700><a href=../15e03fbe>13(49->1 papers)</a></td><td bgcolor=#00FE00><a href=../f73c24c7>14(122->0 papers)</a></td><td bgcolor=#00FE00><a href=../18fe4ff9>15(42->0 papers)</a></td><td bgcolor=#00FE00><a href=../f3c9f4fa>16(28->0 papers)</a></td><td bgcolor=#00ED00><a href=../1c0b9fc4>17(48->2 papers)</a></td><td>18 </td></tr><tr><td>19 </td><td bgcolor=#00F800><a href=../6735e2bc>20(43->1 papers)</a></td><td bgcolor=#00F500><a href=../88f78982>21(106->1 papers)</a></td><td bgcolor=#00FE00><a href=../63c03281>22(40->0 papers)</a></td><td>23 </td><td bgcolor=#00E600><a href=../6ede42c6>24(196->3 papers)</a></td><td>25 </td></tr><tr><td>26 </td><td bgcolor=#00E100><a href=../85e9f9c5>27(72->3 papers)</a></td><td bgcolor=#00ED00><a href=../74e2a248>28(72->2 papers)</a></td><td bgcolor=#00F100><a href=../9b20c976>29(81->2 papers)</a></td><td bgcolor=#00E900><a href=../a6bb3d7c>30(72->2 papers)</a></td><td bgcolor=#00E600><a href=../'49795642'>31(76->3 papers)</a></td></tr></table><table id='insights_13' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年四月April</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td><td bgcolor=#00E400><a href=../d0de9c6>1(62->3 papers)</a></td><td bgcolor=#00EA00><a href=../e63a52c5>2(159->3 papers)</a></td><td bgcolor=#00F900><a href=../9f839fb>3(101->1 papers)</a></td><td bgcolor=#00F400><a href=../eb242282>4(74->1 papers)</a></td><td bgcolor=#00E300><a href=../4e649bc>5(72->4 papers)</a></td><td>6 </td></tr><tr><td>7 </td><td bgcolor=#00DF00><a href=../f118c20c>8(46->3 papers)</a></td><td bgcolor=#00E100><a href=../1edaa932>9(118->3 papers)</a></td><td bgcolor=#00F000><a href=../23415d38>10(62->2 papers)</a></td><td bgcolor=#00F400><a href=../cc833606>11(48->1 papers)</a></td><td bgcolor=#00E500><a href=../27b48d05>12(59->3 papers)</a></td><td>13 </td></tr><tr><td>14 </td><td bgcolor=#00FE00><a href=../c568967c>15(46->0 papers)</a></td><td bgcolor=#00ED00><a href=../2e5f2d7f>16(137->2 papers)</a></td><td bgcolor=#00F600><a href=../c19d4641>17(47->1 papers)</a></td><td bgcolor=#00E700><a href=../30961dcc>18(57->3 papers)</a></td><td bgcolor=#00F800><a href=../df5476f2>19(59->1 papers)</a></td><td>20 </td></tr><tr><td>21 </td><td bgcolor=#00FE00><a href=../be56eb04>22(50->0 papers)</a></td><td bgcolor=#00DA00><a href=../5194803a>23(104->4 papers)</a></td><td bgcolor=#00F600><a href=../b3489b43>24(83->1 papers)</a></td><td bgcolor=#00F800><a href=../5c8af07d>25(47->1 papers)</a></td><td bgcolor=#00F700><a href=../b7bd4b7e>26(56->1 papers)</a></td><td>27 </td></tr><tr><td>28 </td><td bgcolor=#00FE00><a href=../46b610f3>29(41->0 papers)</a></td><td bgcolor=#00E700><a href=../7b2de4f9>30(99->3 papers)</a></td></tr></table><table id='insights_14' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年三月March</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td bgcolor=#00F000><a href=../707eef9e>1(67->2 papers)</a></td><td>2 </td></tr><tr><td>3 </td><td bgcolor=#00F700><a href=../965724da>4(48->1 papers)</a></td><td bgcolor=#00EE00><a href=../79954fe4>5(175->2 papers)</a></td><td bgcolor=#00EA00><a href=../92a2f4e7>6(74->3 papers)</a></td><td bgcolor=#00F700><a href=../7d609fd9>7(52->1 papers)</a></td><td bgcolor=#00F000><a href=../8c6bc454>8(58->2 papers)</a></td><td>9 </td></tr><tr><td>10 </td><td bgcolor=#00EC00><a href=../b1f0305e>11(69->2 papers)</a></td><td bgcolor=#00F800><a href=../5ac78b5d>12(115->1 papers)</a></td><td bgcolor=#00E500><a href=../b505e063>13(53->2 papers)</a></td><td bgcolor=#00E800><a href=../57d9fb1a>14(73->3 papers)</a></td><td bgcolor=#00E100><a href=../b81b9024>15(58->3 papers)</a></td><td>16 </td></tr><tr><td>17 </td><td bgcolor=#00F900><a href=../4de51b94>18(75->1 papers)</a></td><td bgcolor=#00DC00><a href=../a22770aa>19(114->4 papers)</a></td><td bgcolor=#00EF00><a href=../c7d03d61>20(61->2 papers)</a></td><td bgcolor=#00EE00><a href=../2812565f>21(56->2 papers)</a></td><td bgcolor=#00E100><a href=../c325ed5c>22(66->3 papers)</a></td><td>23 </td></tr><tr><td>24 </td><td bgcolor=#00F000><a href=../21f9f625>25(67->2 papers)</a></td><td bgcolor=#00E100><a href=../cace4d26>26(148->3 papers)</a></td><td bgcolor=#00EE00><a href=../250c2618>27(84->2 papers)</a></td><td bgcolor=#00E800><a href=../d4077d95>28(66->3 papers)</a></td><td bgcolor=#00E100><a href=../3bc516ab>29(69->3 papers)</a></td><td>30 </td></tr><tr><td>31 </td></tr></table><table id='insights_15' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年二月February</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td><td bgcolor=#00E600><a href=../ade8361b>1(52->3 papers)</a></td><td bgcolor=#00E200><a href=../46df8d18>2(54->4 papers)</a></td><td>3 </td></tr><tr><td>4 </td><td bgcolor=#00EB00><a href=../a4039661>5(71->2 papers)</a></td><td bgcolor=#00EB00><a href=../4f342d62>6(221->2 papers)</a></td><td bgcolor=#00E900><a href=../a0f6465c>7(77->3 papers)</a></td><td bgcolor=#00F400><a href=../51fd1dd1>8(58->1 papers)</a></td><td bgcolor=#00F000><a href=../be3f76ef>9(81->2 papers)</a></td><td>10 </td></tr><tr><td>11 </td><td bgcolor=#00EE00><a href=../875152d8>12(46->2 papers)</a></td><td bgcolor=#00E800><a href=../'689339e6'>13(93->3 papers)</a></td><td bgcolor=#00F200><a href=../8a4f229f>14(51->2 papers)</a></td><td bgcolor=#00EC00><a href=../658d49a1>15(49->2 papers)</a></td><td bgcolor=#00F000><a href=../8ebaf2a2>16(68->2 papers)</a></td><td>17 </td></tr><tr><td>18 </td><td bgcolor=#00ED00><a href=../7fb1a92f>19(88->2 papers)</a></td><td bgcolor=#00E400><a href=../1a46e4e4>20(265->3 papers)</a></td><td bgcolor=#00E600><a href=../f5848fda>21(108->2 papers)</a></td><td bgcolor=#00EB00><a href=../1eb334d9>22(105->2 papers)</a></td><td bgcolor=#00E600><a href=../f1715fe7>23(105->3 papers)</a></td><td>24 </td></tr><tr><td>25 </td><td bgcolor=#00F200><a href=../175894a3>26(114->3 papers)</a></td><td bgcolor=#00EE00><a href=../f89aff9d>27(165->3 papers)</a></td><td bgcolor=#00E900><a href=../991a410>28(84->3 papers)</a></td><td bgcolor=#00F100><a href=../e653cf2e>29(96->2 papers)</a></td></tr></table><table id='insights_16' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2024年一月January</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td><td bgcolor=#00F400><a href=../10225ad5>1(42->1 papers)</a></td><td bgcolor=#00EB00><a href=../fb15e1d6>2(48->1 papers)</a></td><td bgcolor=#00ED00><a href=../14d78ae8>3(24->2 papers)</a></td><td bgcolor=#00EF00><a href=../f60b9191>4(29->2 papers)</a></td><td bgcolor=#00F800><a href=../19c9faaf>5(28->1 papers)</a></td><td>6 </td></tr><tr><td>7 </td><td bgcolor=#00F800><a href=../ec37711f>8(17->1 papers)</a></td><td bgcolor=#00DA00><a href=../3f51a21>9(80->4 papers)</a></td><td bgcolor=#00EB00><a href=../3e6eee2b>10(38->2 papers)</a></td><td bgcolor=#00EB00><a href=../d1ac8515>11(36->2 papers)</a></td><td bgcolor=#00EB00><a href=../3a9b3e16>12(60->2 papers)</a></td><td>13 </td></tr><tr><td>14 </td><td bgcolor=#00DA00><a href=../d847256f>15(57->3 papers)</a></td><td>16 </td><td bgcolor=#00CE00><a href=../dcb2f552>17(163->5 papers)</a></td><td bgcolor=#00F700><a href=../2db9aedf>18(35->1 papers)</a></td><td bgcolor=#00EA00><a href=../c27bc5e1>19(49->3 papers)</a></td><td>20 </td></tr><tr><td>21 </td><td bgcolor=#00E400><a href=../a3795817>22(45->3 papers)</a></td><td bgcolor=#00E700><a href=../4cbb3329>23(75->3 papers)</a></td><td bgcolor=#00F500><a href=../ae672850>24(43->1 papers)</a></td><td bgcolor=#00F800><a href=../41a5436e>25(56->1 papers)</a></td><td bgcolor=#00DB00><a href=../aa92f86d>26(46->3 papers)</a></td><td>27 </td></tr><tr><td>28 </td><td bgcolor=#00FE00><a href=../5b99a3e0>29(42->0 papers)</a></td><td bgcolor=#00EC00><a href=../660257ea>30(85->2 papers)</a></td><td bgcolor=#00F600><a href=../89c03cd4>31(57->1 papers)</a></td></tr></table><table id='insights_17' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2023年十二月December</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td bgcolor=#00E200><a href=../cd1e2f18>1(44->3 papers)</a></td><td>2 </td></tr><tr><td>3 </td><td bgcolor=#00ED00><a href=../2b37e45c>4(39->2 papers)</a></td><td bgcolor=#00E200><a href=../c4f58f62>5(78->3 papers)</a></td><td bgcolor=#00E400><a href=../2fc23461>6(44->3 papers)</a></td><td bgcolor=#00FC00><a href=../c0005f5f>7(42->0 papers)</a></td><td bgcolor=#00EC00><a href=../310b04d2>8(89->2 papers)</a></td><td>9 </td></tr><tr><td>10 </td><td bgcolor=#00EE00><a href=../c90f0d8>11(41->2 papers)</a></td><td bgcolor=#00FE00><a href=../e7a74bdb>12(72->0 papers)</a></td><td bgcolor=#00F600><a href=../'86520e5'>13(48->1 papers)</a></td><td bgcolor=#00F100><a href=../eab93b9c>14(42->1 papers)</a></td><td bgcolor=#00EF00><a href=../57b50a2>15(40->2 papers)</a></td><td>16 </td></tr><tr><td>17 </td><td bgcolor=#00E700><a href=../f085db12>18(43->3 papers)</a></td><td bgcolor=#00E200><a href=../1f47b02c>19(92->3 papers)</a></td><td bgcolor=#00F500><a href=../7ab0fde7>20(67->1 papers)</a></td><td bgcolor=#00F600><a href=../957296d9>21(44->1 papers)</a></td><td bgcolor=#00EC00><a href=../7e452dda>22(31->2 papers)</a></td><td>23 </td></tr><tr><td>24 </td><td bgcolor=#00EE00><a href=../9c9936a3>25(38->2 papers)</a></td><td>26 </td><td bgcolor=#00EC00><a href=../986ce69e>27(72->2 papers)</a></td><td>28 </td><td bgcolor=#00DE00><a href=../86a5d62d>29(47->3 papers)</a></td><td>30 </td></tr><tr><td>31 </td></tr></table><table id='insights_18' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2023年十一月November</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td><td bgcolor=#00F800><a href=../70d443d6>1(61->1 papers)</a></td><td bgcolor=#00EE00><a href=../9be3f8d5>2(57->2 papers)</a></td><td bgcolor=#00F700><a href=../742193eb>3(46->1 papers)</a></td><td>4 </td></tr><tr><td>5 </td><td bgcolor=#00F500><a href=../920858af>6(54->1 papers)</a></td><td bgcolor=#00F100><a href=../7dca3391>7(74->2 papers)</a></td><td bgcolor=#00F600><a href=../8cc1681c>8(59->1 papers)</a></td><td bgcolor=#00EC00><a href=../'63030322'>9(48->3 papers)</a></td><td bgcolor=#00F700><a href=../5e98f728>10(69->1 papers)</a></td><td>11 </td></tr><tr><td>12 </td><td bgcolor=#00DB00><a href=../b5af4c2b>13(34->3 papers)</a></td><td bgcolor=#00EF00><a href=../'57735752'>14(119->2 papers)</a></td><td bgcolor=#00D900><a href=../b8b13c6c>15(109->3 papers)</a></td><td bgcolor=#00E800><a href=../5386876f>16(118->2 papers)</a></td><td bgcolor=#00ED00><a href=../bc44ec51>17(154->2 papers)</a></td><td>18 </td></tr><tr><td>19 </td><td bgcolor=#00FE00><a href=../c77a9129>20(27->0 papers)</a></td><td bgcolor=#00E600><a href=../28b8fa17>21(99->3 papers)</a></td><td bgcolor=#00F400><a href=../c38f4114>22(37->2 papers)</a></td><td bgcolor=#00EF00><a href=../2c4d2a2a>23(39->2 papers)</a></td><td>24 </td><td>25 </td></tr><tr><td>26 </td><td bgcolor=#00ED00><a href=../25a68a50>27(48->2 papers)</a></td><td bgcolor=#00F000><a href=../d4add1dd>28(87->2 papers)</a></td><td bgcolor=#00E300><a href=../3b6fbae3>29(52->3 papers)</a></td><td bgcolor=#00EB00><a href=../6f44ee9>30(47->2 papers)</a></td></tr></table><table id='insights_19' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2023年十月October</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr><td>1 </td><td bgcolor=#00F100><a href=../46752150>2(30->2 papers)</a></td><td bgcolor=#00ED00><a href=../a9b74a6e>3(40->3 papers)</a></td><td bgcolor=#00F100><a href=../4b6b5117>4(30->2 papers)</a></td><td bgcolor=#00F200><a href=../a4a93a29>5(30->2 papers)</a></td><td bgcolor=#00EE00><a href=../4f9e812a>6(30->2 papers)</a></td><td>7 </td></tr><tr><td>8 </td><td bgcolor=#00FB00><a href=../be95daa7>9(10->0 papers)</a></td><td bgcolor=#00F000><a href=../830e2ead>10(172->3 papers)</a></td><td bgcolor=#00EA00><a href=../6ccc4593>11(40->3 papers)</a></td><td bgcolor=#00F100><a href=../87fbfe90>12(30->2 papers)</a></td><td bgcolor=#00EF00><a href=../683995ae>13(30->2 papers)</a></td><td>14 </td></tr><tr><td>15 </td><td bgcolor=#00FE00><a href=../8e105eea>16(10->0 papers)</a></td><td bgcolor=#00EE00><a href=../61d235d4>17(135->2 papers)</a></td><td bgcolor=#00E900><a href=../90d96e59>18(83->3 papers)</a></td><td bgcolor=#00E900><a href=../7f1b0567>19(74->3 papers)</a></td><td bgcolor=#00F000><a href=../1aec48ac>20(74->2 papers)</a></td><td>21 </td></tr><tr><td>22 </td><td bgcolor=#00F500><a href=../f1dbf3af>23(108->1 papers)</a></td><td bgcolor=#00F800><a href=../1307e8d6>24(203->3 papers)</a></td><td bgcolor=#00F600><a href=../fcc583e8>25(112->1 papers)</a></td><td bgcolor=#00F800><a href=../17f238eb>26(89->1 papers)</a></td><td bgcolor=#00F600><a href=../f83053d5>27(80->1 papers)</a></td><td>28 </td></tr><tr><td>29 </td><td bgcolor=#00FE00><a href=../db62976c>30(67->0 papers)</a></td><td bgcolor=#00F800><a href=../34a0fc52>31(141->3 papers)</a></td></tr></table><table id='insights_20' hidden='hidden' style='text-align:center;table-layout:fixed'><tr>   <td><form action=""><input type="button" value="上月Last Month" onclick="tips_insight(-1)"></form></td>   <td colspan="5">2023年九月September</td>   <td><form action=""><input type="button" value="下月Next Month" onclick="tips_insight(1)"></form></td></tr><tr><td> 星期日</br>Sunday </td><td> 星期一</br>Monday </td><td> 星期二</br>Tuesday  </td><td> 星期三</br>Wednesday </td><td> 星期四</br>Thursday </td><td> 星期五</br>Friday </td><td> 星期六</br>Saturday </td> </tr><tr>  <td></td>  <td></td>  <td></td>  <td></td>  <td></td><td>1 </td><td>2 </td></tr><tr><td>3 </td><td>4 </td><td>5 </td><td>6 </td><td>7 </td><td>8 </td><td>9 </td></tr><tr><td>10 </td><td>11 </td><td>12 </td><td>13 </td><td>14 </td><td>15 </td><td>16 </td></tr><tr><td>17 </td><td>18 </td><td>19 </td><td>20 </td><td>21 </td><td>22 </td><td>23 </td></tr><tr><td>24 </td><td>25 </td><td>26 </td><td>27 </td><td bgcolor=#00D600><a href=../13a1e3c6>28(30->2 papers)</a></td><td bgcolor=#00BC00><a href=../d2c6faf>29(40->3 papers)</a></td><td>30 </td></tr><tr></tr></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;压缩带来智能，5% 的论文决定学术界 95% 的成果！每天从 Arxiv 论文中总结分享最重要、最有趣的最多三篇论文。&lt;/p&gt;
&lt;p&gt;Compression brings intelligence, 5% of papers determine 95% of AI technologies! Share the most important papers from Arxiv, every day, up to three!&lt;/p&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
    var insight_now_id = 0;
    var insight_max_id = 20;
    function tips_insight(num){
        document.getElementById(&quot;insights_&quot;+insight_now_id).hidden = &quot;hidden&quot;;
        insight_now_id -= num;
        if (insight_now_id &gt; insight_max_id) {insight_now_id = insight_max_id;}
        if (insight_now_id &lt; 0) {insight_now_id = 0;}
        document.getElementById(&quot;insights_&quot; + insight_now_id).hidden = &quot;&quot;;
    }
&lt;/script&gt;
&lt;table id=&quot;insights_0&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2025年五月May&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E800&quot;&gt;&lt;a href=&quot;../7ef3a1d2&quot;&gt;1(210-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../95c41ad1&quot;&gt;2(173-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;3 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4 &lt;/td&gt;&lt;td bgcolor=&quot;#00EE00&quot;&gt;&lt;a href=&quot;../771801a8&quot;&gt;5(153-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F400&quot;&gt;&lt;a href=&quot;../9c2fbaab&quot;&gt;6(365-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F900&quot;&gt;&lt;a href=&quot;../73edd195&quot;&gt;7(221-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../82e68a18&quot;&gt;8(239-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../6d24e126&quot;&gt;9(225-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;10 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;11 &lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../544ac511&quot;&gt;12(182-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00ED00&quot;&gt;&lt;a href=&quot;../bb88ae2f&quot;&gt;13(413-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../5954b556&quot;&gt;14(265-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00ED00&quot;&gt;&lt;a href=&quot;../b696de68&quot;&gt;15(200-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../5da1656b&quot;&gt;16(202-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;17 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;18 &lt;/td&gt;&lt;td bgcolor=&quot;#00F600&quot;&gt;&lt;a href=&quot;../acaa3ee6&quot;&gt;19(292-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E100&quot;&gt;&lt;a href=&quot;../c95d732d&quot;&gt;20(701-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;21 &lt;/td&gt;&lt;td&gt;22 &lt;/td&gt;&lt;td&gt;23 &lt;/td&gt;&lt;td&gt;24 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;25 &lt;/td&gt;&lt;td&gt;26 &lt;/td&gt;&lt;td&gt;27 &lt;/td&gt;&lt;td&gt;28 &lt;/td&gt;&lt;td&gt;29 &lt;/td&gt;&lt;td&gt;30 &lt;/td&gt;&lt;td&gt;31 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_1&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2025年四月April&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../a3657857&quot;&gt;1(472-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../4852c354&quot;&gt;2(266-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E300&quot;&gt;&lt;a href=&quot;../a790a86a&quot;&gt;3(218-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../454cb313&quot;&gt;4(224-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;5 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;6 &lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../ae7b0810&quot;&gt;7(201-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F200&quot;&gt;&lt;a href=&quot;../5f70539d&quot;&gt;8(478-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../b0b238a3&quot;&gt;9(249-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../8d29cca9&quot;&gt;10(204-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../62eba797&quot;&gt;11(194-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;13 &lt;/td&gt;&lt;td bgcolor=&quot;#00F400&quot;&gt;&lt;a href=&quot;../84c26cd3&quot;&gt;14(195-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EE00&quot;&gt;&lt;a href=&quot;../6b0007ed&quot;&gt;15(479-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FA00&quot;&gt;&lt;a href=&quot;../8037bcee&quot;&gt;16(290-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../6ff5d7d0&quot;&gt;17(206-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E200&quot;&gt;&lt;a href=&quot;../9efe8c5d&quot;&gt;18(257-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;19 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20 &lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../fb09c196&quot;&gt;21(195-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F200&quot;&gt;&lt;a href=&quot;../103e7a95&quot;&gt;22(441-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EB00&quot;&gt;&lt;a href=&quot;../fffc11ab&quot;&gt;23(188-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F600&quot;&gt;&lt;a href=&quot;../1d200ad2&quot;&gt;24(192-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../f2e261ec&quot;&gt;25(192-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;26 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;27 &lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../71cea5c&quot;&gt;28(175-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../e8de8162&quot;&gt;29(365-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../d5457568&quot;&gt;30(219-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_2&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2025年三月March&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td&gt;1 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;2 &lt;/td&gt;&lt;td bgcolor=&quot;#00F300&quot;&gt;&lt;a href=&quot;../dae3ae32&quot;&gt;3(236-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E800&quot;&gt;&lt;a href=&quot;../383fb54b&quot;&gt;4(570-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../d7fdde75&quot;&gt;5(307-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FE00&quot;&gt;&lt;a href=&quot;../3cca6576&quot;&gt;6(197-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EC00&quot;&gt;&lt;a href=&quot;../d3080e48&quot;&gt;7(227-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;8 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;9 &lt;/td&gt;&lt;td bgcolor=&quot;#00E900&quot;&gt;&lt;a href=&quot;../f05acaf1&quot;&gt;10(295-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../1f98a1cf&quot;&gt;11(724-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../f4af1acc&quot;&gt;12(381-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../1b6d71f2&quot;&gt;13(272-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E100&quot;&gt;&lt;a href=&quot;../f9b16a8b&quot;&gt;14(298-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;15 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;16 &lt;/td&gt;&lt;td bgcolor=&quot;#00F200&quot;&gt;&lt;a href=&quot;../1286d188&quot;&gt;17(331-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E800&quot;&gt;&lt;a href=&quot;../e38d8a05&quot;&gt;18(578-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../c4fe13b&quot;&gt;19(328-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F900&quot;&gt;&lt;a href=&quot;../69b8acf0&quot;&gt;20(278-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EE00&quot;&gt;&lt;a href=&quot;../867ac7ce&quot;&gt;21(291-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;22 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;23 &lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../60530c8a&quot;&gt;24(314-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../8f9167b4&quot;&gt;25(515-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EA00&quot;&gt;&lt;a href=&quot;../64a6dcb7&quot;&gt;26(296-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F900&quot;&gt;&lt;a href=&quot;../8b64b789&quot;&gt;27(209-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EB00&quot;&gt;&lt;a href=&quot;../7a6fec04&quot;&gt;28(241-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;29 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;30 &lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../47f4180e&quot;&gt;31(250-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_3&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2025年二月February&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td&gt;1 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;2 &lt;/td&gt;&lt;td bgcolor=&quot;#00E900&quot;&gt;&lt;a href=&quot;../77577b7&quot;&gt;3(217-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;4 &lt;/td&gt;&lt;td&gt;5 &lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../e15cbcf3&quot;&gt;6(182-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../e9ed7cd&quot;&gt;7(187-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;8 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;9 &lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../2dcc1374&quot;&gt;10(245-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../c20e784a&quot;&gt;11(431-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../2939c349&quot;&gt;12(324-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E800&quot;&gt;&lt;a href=&quot;../c6fba877&quot;&gt;13(218-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../2427b30e&quot;&gt;14(253-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;15 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;16 &lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../cf10080d&quot;&gt;17(204-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00DF00&quot;&gt;&lt;a href=&quot;../3e1b5380&quot;&gt;18(518-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../d1d938be&quot;&gt;19(195-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F900&quot;&gt;&lt;a href=&quot;../b42e7575&quot;&gt;20(195-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F600&quot;&gt;&lt;a href=&quot;../5bec1e4b&quot;&gt;21(247-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;22 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;23 &lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../bdc5d50f&quot;&gt;24(264-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../5207be31&quot;&gt;25(550-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E600&quot;&gt;&lt;a href=&quot;../b9300532&quot;&gt;26(264-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F400&quot;&gt;&lt;a href=&quot;../56f26e0c&quot;&gt;27(251-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../a7f93581&quot;&gt;28(258-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_4&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2025年一月January&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td&gt;1 &lt;/td&gt;&lt;td&gt;2 &lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../babf1b79&quot;&gt;3(360-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;4 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 &lt;/td&gt;&lt;td bgcolor=&quot;#00F200&quot;&gt;&lt;a href=&quot;../5c96d03d&quot;&gt;6(143-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F200&quot;&gt;&lt;a href=&quot;../b354bb03&quot;&gt;7(371-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../425fe08e&quot;&gt;8(171-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../ad9d8bb0&quot;&gt;9(138-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../90067fba&quot;&gt;10(172-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;11 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;12 &lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../7b31c4b9&quot;&gt;13(155-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EC00&quot;&gt;&lt;a href=&quot;../99eddfc0&quot;&gt;14(375-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../762fb4fe&quot;&gt;15(196-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../9d180ffd&quot;&gt;16(155-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../72da64c3&quot;&gt;17(187-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;18 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;19 &lt;/td&gt;&lt;td bgcolor=&quot;#00EA00&quot;&gt;&lt;a href=&quot;../9e419bb&quot;&gt;20(136-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;21 &lt;/td&gt;&lt;td bgcolor=&quot;#00D000&quot;&gt;&lt;a href=&quot;../d11c986&quot;&gt;22(445-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EA00&quot;&gt;&lt;a href=&quot;../e2d3a2b8&quot;&gt;23(138-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../fb9c1&quot;&gt;24(191-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;25 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;26 &lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../eb3802c2&quot;&gt;27(239-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../1a33594f&quot;&gt;28(383-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../f5f13271&quot;&gt;29(196-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../c86ac67b&quot;&gt;30(132-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;31 &lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_5&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2024年十二月December&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1 &lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../dbd06c6e&quot;&gt;2(349-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../&#39;34120750&#39;&quot;&gt;3(618-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EE00&quot;&gt;&lt;a href=&quot;../d6ce1c29&quot;&gt;4(260-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../390c7717&quot;&gt;5(244-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../d23bcc14&quot;&gt;6(293-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;7 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;8 &lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../&#39;23309799&#39;&quot;&gt;9(240-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E300&quot;&gt;&lt;a href=&quot;../1eab6393&quot;&gt;10(446-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F300&quot;&gt;&lt;a href=&quot;../f16908ad&quot;&gt;11(294-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../1a5eb3ae&quot;&gt;12(248-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E600&quot;&gt;&lt;a href=&quot;../f59cd890&quot;&gt;13(255-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;14 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;15 &lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../13b513d4&quot;&gt;16(224-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E900&quot;&gt;&lt;a href=&quot;../fc7778ea&quot;&gt;17(572-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../d7c2367&quot;&gt;18(334-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F000&quot;&gt;&lt;a href=&quot;../e2be4859&quot;&gt;19(271-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F800&quot;&gt;&lt;a href=&quot;../&#39;87490592&#39;&quot;&gt;20(269-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;21 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;22 &lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../6c7ebe91&quot;&gt;23(249-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00ED00&quot;&gt;&lt;a href=&quot;../8ea2a5e8&quot;&gt;24(521-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../6160ced6&quot;&gt;25(231-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;26 &lt;/td&gt;&lt;td&gt;27 &lt;/td&gt;&lt;td&gt;28 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;29 &lt;/td&gt;&lt;td bgcolor=&quot;#00F300&quot;&gt;&lt;a href=&quot;../46c7da52&quot;&gt;30(334-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00FF00&quot;&gt;&lt;a href=&quot;../a905b16c&quot;&gt;31(324-&gt;0 papers)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_6&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2024年十一月November&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E100&quot;&gt;&lt;a href=&quot;../8d2dbba3&quot;&gt;1(223-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;2 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3 &lt;/td&gt;&lt;td bgcolor=&quot;#00DD00&quot;&gt;&lt;a href=&quot;../6b0470e7&quot;&gt;4(214-&gt;5 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00DA00&quot;&gt;&lt;a href=&quot;../84c61bd9&quot;&gt;5(472-&gt;5 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EF00&quot;&gt;&lt;a href=&quot;../6ff1a0da&quot;&gt;6(250-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F500&quot;&gt;&lt;a href=&quot;../8033cbe4&quot;&gt;7(250-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00CC00&quot;&gt;&lt;a href=&quot;../&#39;71389069&#39;&quot;&gt;8(206-&gt;7 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;9 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;10 &lt;/td&gt;&lt;td bgcolor=&quot;#00EE00&quot;&gt;&lt;a href=&quot;../4ca36463&quot;&gt;11(161-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00D900&quot;&gt;&lt;a href=&quot;../a794df60&quot;&gt;12(405-&gt;5 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../4856b45e&quot;&gt;13(199-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F100&quot;&gt;&lt;a href=&quot;../aa8aaf27&quot;&gt;14(171-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F600&quot;&gt;&lt;a href=&quot;../4548c419&quot;&gt;15(191-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;16 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;17 &lt;/td&gt;&lt;td bgcolor=&quot;#00EB00&quot;&gt;&lt;a href=&quot;../b0b64fa9&quot;&gt;18(199-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E200&quot;&gt;&lt;a href=&quot;../5f742497&quot;&gt;19(345-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../3a83695c&quot;&gt;20(205-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../d5410262&quot;&gt;21(199-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../3e76b961&quot;&gt;22(211-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;23 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;24 &lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../dcaaa218&quot;&gt;25(217-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E500&quot;&gt;&lt;a href=&quot;../379d191b&quot;&gt;26(490-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EE00&quot;&gt;&lt;a href=&quot;../d85f7225&quot;&gt;27(332-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E700&quot;&gt;&lt;a href=&quot;../295429a8&quot;&gt;28(270-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;29 &lt;/td&gt;&lt;td&gt;30 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;table id=&quot;insights_7&quot; hidden=&quot;hidden&quot; style=&quot;text-align:center;table-layout:fixed&quot;&gt;
&lt;tr&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;上月
Last Month&quot; onclick=&quot;tips_insight(-1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
   &lt;td colspan=&quot;5&quot;&gt;2024年十月October&lt;/td&gt;
   &lt;td&gt;&lt;form action&gt;&lt;input type=&quot;button&quot; value=&quot;下月
Next Month&quot; onclick=&quot;tips_insight(1)&quot;&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt; 星期日&lt;br&gt;Sunday &lt;/td&gt;&lt;td&gt; 星期一&lt;br&gt;Monday &lt;/td&gt;&lt;td&gt; 星期二&lt;br&gt;Tuesday  &lt;/td&gt;&lt;td&gt; 星期三&lt;br&gt;Wednesday &lt;/td&gt;&lt;td&gt; 星期四&lt;br&gt;Thursday &lt;/td&gt;&lt;td&gt; 星期五&lt;br&gt;Friday &lt;/td&gt;&lt;td&gt; 星期六&lt;br&gt;Saturday &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td&gt;&lt;/td&gt;  &lt;td&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E400&quot;&gt;&lt;a href=&quot;../50bb6226&quot;&gt;1(172-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F700&quot;&gt;&lt;a href=&quot;../bb8cd925&quot;&gt;2(74-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E400&quot;&gt;&lt;a href=&quot;../544eb21b&quot;&gt;3(103-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00D300&quot;&gt;&lt;a href=&quot;../b692a962&quot;&gt;4(128-&gt;6 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;5 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;6 &lt;/td&gt;&lt;td bgcolor=&quot;#00E400&quot;&gt;&lt;a href=&quot;../5da51261&quot;&gt;7(121-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00D700&quot;&gt;&lt;a href=&quot;../acae49ec&quot;&gt;8(224-&gt;5 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;9 &lt;/td&gt;&lt;td&gt;10 &lt;/td&gt;&lt;td&gt;11 &lt;/td&gt;&lt;td&gt;12 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;13 &lt;/td&gt;&lt;td&gt;14 &lt;/td&gt;&lt;td&gt;15 &lt;/td&gt;&lt;td bgcolor=&quot;#00E600&quot;&gt;&lt;a href=&quot;../73e9a69f&quot;&gt;16(133-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E300&quot;&gt;&lt;a href=&quot;../9c2bcda1&quot;&gt;17(156-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;18 &lt;/td&gt;&lt;td&gt;19 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;20 &lt;/td&gt;&lt;td bgcolor=&quot;#00F600&quot;&gt;&lt;a href=&quot;../8d7dbe7&quot;&gt;21(125-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E900&quot;&gt;&lt;a href=&quot;../e3e060e4&quot;&gt;22(198-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00EB00&quot;&gt;&lt;a href=&quot;../c220bda&quot;&gt;23(135-&gt;3 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F900&quot;&gt;&lt;a href=&quot;../eefe10a3&quot;&gt;24(122-&gt;1 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00F200&quot;&gt;&lt;a href=&quot;../13c7b9d&quot;&gt;25(145-&gt;2 papers)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;26 &lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;27 &lt;/td&gt;&lt;td bgcolor=&quot;#00D500&quot;&gt;&lt;a href=&quot;../f4c2f02d&quot;&gt;28(172-&gt;5 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00E200&quot;&gt;&lt;a href=&quot;../1b009b13&quot;&gt;29(431-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00DB00&quot;&gt;&lt;a href=&quot;../269b6f19&quot;&gt;30(261-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;td bgcolor=&quot;#00D500&quot;&gt;&lt;a href=&quot;../c9590427&quot;&gt;31(260-&gt;4 papers)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="English" scheme="https://www.yynnyy.cn/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>2023-12-31总结(年度总结)</title>
    <link href="https://www.yynnyy.cn/aa29fa81"/>
    <id>https://www.yynnyy.cn/aa29fa81</id>
    <published>2023-12-31T07:45:21.000Z</published>
    <updated>2024-08-09T09:26:46.966Z</updated>
    
    <content type="html"><![CDATA[<p>上次写总结还是在2023-9-29, 没想到下次再写竟已经是3个月之后了，到了2023年的最后一天。每到年末，各种APP就喜欢来个xxx年度总结：B站总结、steam总结、网易云音乐总结……不过今天看到一个&quot;新华社年度十大新闻&quot;觉得挺有意思，我就想，能不能给我自己也列一个&quot;年度十大新闻&quot;呢？</p><span id="more"></span><img src="../../files/images/diary/2023-most-influence-news.jpg" style="zoom:33%;" ><p>总结十大新闻是一个很重要的事情，也许10年、20年后，我都已经忘记了2023年发生过什么，但可能还会记得之前总结的&quot;十大新闻&quot;，在这个意义上，这其实就代表了整个2023年对我的印象。就像杨大伯之前说的&quot;每门课上完，你最少记住一句话，记住一辈子&quot;。大概就是这个道理</p><p>然而，总结年度十大新闻也是一个很难的事情，因为很多事件的影响可能很难在年底就出现，可能更应该来个时间检验奖，2023年更容易评出来&quot;2013年年度10大新闻&quot;，大家也就姑且一听吧:</p><ol><li>本科毕业：2023-06</li><li>研究生入学: 2023-09。还是华子，还是贵系</li><li>和TLE的1000天纪念：2023-06</li><li>去成都旅游：2023-08</li><li>第一次发Twitter：2023-08</li><li>第一次得新冠: 2023-01</li><li>尝试纹理烫：2023-02</li><li>google scholar 100引用：2023-08</li><li>end2end做饭：2023-07</li><li>凑齐apple全家桶：2023-06</li></ol><p>不知不觉，本科都已经毕业了，变成研究生了……前几天拉着室友一起回到十一学校去帮清华招生宣讲，突然感觉自己变得好老，高中学弟说的高中生活、本科学弟说的本科生活，好像全都已经离我远去了。明明研究生才开始一个学期，但心态似乎已经完全不同了。说起来，本科生活似乎也没有很&quot;多彩&quot;，各种行动不管是实践或者运动，好像也大都有某种目的性。虽说大学的优化目标比较多比高中多，看似非常的diverse。但各个项目都要优化、都和某些利益挂钩，反而会让所有事情都隐约套了个目的性的壳：运动是为了阳光打卡，科研是为了保研，社工是为了评奖……到了研究生，优化目标反而纯粹了，这种目的性的东西基本没有了，希望可以更多的做一些纯兴趣的东西，比如来场随心的旅行</p><p>华子的传统是每年的跨年敲钟时都要宣布一个有利于生权的好事。2019年跨年时邱宝敲钟完宣布校园网流量从一个月20GB升级到了50GB。今年的好消息看来是&quot;清华和北大不限制双向入校&quot;。仔细想了想，虽然感觉华子不怎么关心生权，但把尺度放大，从我入学到现在，想达成的几个生权的好事基本也都达成了：</p><ul><li>无限量校园网，在21年校庆的时候完成</li><li>白天可以洗热水澡，22年校庆的时候完成</li></ul><p>希望等研究生毕业的时候下面这几件事也能完成：</p><ul><li>洗衣机免费</li><li>食堂可以卖冰淇淋(手工)</li><li>为计算机系同学提供免费的算力，比如1xA100/ person</li><li>学校里可以有电动车充电桩</li></ul><p>喊了一年的新系馆，23年最终还是没住成，现在又改口说是24年3月搬，无所谓吧，反正这已经是第5次改口了，就像美国的国债限额，大概快到时间了还会再延期吧。西体育馆终于建成了，我记得18年暑校的时候辅导员就在提冰雪场馆，19年入学的时候好像就围起来了，最终23年终于可以滑冰了。虽然还没有体验过，不过看同学们反馈都不错，24年真得去试一试。现在东边的足球场又围起来了，听说要建一个最大的地下学生活动中心，还挺期待的，希望我毕业的时候能用上。</p><p>华子每年都在盖新的楼，每年也都有新盖好、装修好的楼投入使用；同学每年都在提新的需求，每年也都有需求被满足……希望我也一样，每年都能有新的回忆产生，每年也都能找到新的目标，也能交到新的朋友</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;上次写总结还是在2023-9-29, 没想到下次再写竟已经是3个月之后了，到了2023年的最后一天。每到年末，各种APP就喜欢来个xxx年度总结：B站总结、steam总结、网易云音乐总结……不过今天看到一个&amp;quot;新华社年度十大新闻&amp;quot;觉得挺有意思，我就想，能不能给我自己也列一个&amp;quot;年度十大新闻&amp;quot;呢？&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>论文阅读[粗读]-Alignment For Honesty</title>
    <link href="https://www.yynnyy.cn/2300f7be"/>
    <id>https://www.yynnyy.cn/2300f7be</id>
    <published>2023-12-19T01:25:45.000Z</published>
    <updated>2024-08-09T09:26:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>上周刷到了刘鹏飞老师的 Alignment For Honesty, 分享给了大家 <a href="/86520e5.html" title="2023-12-13-insights">2023-12-13-insights</a>。里面讲到如何训练LLM变得诚实，他沿用了孔子的定义：</p><blockquote><p>知之为知之，不知为不知，是知(zhì)也。</p><p>To say “I know” when you know, and “I don’t know” when you don’t, that is wisdom.</p></blockquote><p>我来一起看看他们是怎么做的吧</p><span id="more"></span><h2 id="introduction">introduction</h2><p>作者团队来自上交、复旦和CMU，其中复旦的xipeng qiu老师也是arxiv的常客了</p><img src=" ../../files/images/align-for-honesty/authors.png"><p>其实关于honesty,这个领域由来已久，本文作者也提到了，学界对于Honesty有各种各样的定义和表述方式。前两天读weak-to-strong generalization时，OpenAI也提到了相关的研究，有兴趣的同学可以进一步顺着引文看一看相关的研究~</p><img src=" ../../files/images/align-for-honesty/openai.png"><p>回到本文，作者按照《论语》里给出的定义来定义诚实：知之为知之，不知为不知，是知(zhì)也。具体来说，需要模型可以分辨自己的知识边界：</p><ul><li>边界内的问题予以回答</li><li>边界外的问题勇于承认</li></ul><blockquote><p>不过，我觉得这里的语境和孔子想表述的有些区别：对于人来说，认知到知识边界很容易，只是很多时候羞于承认，所以这种&quot;勇于承认&quot;是一种君子的品格。但对于模型来说，还没有到荣辱心这一步，他只是单纯地意识不到自己的知识边界……</p></blockquote><p>让模型获得Honesty有各种各样的好处，其中最显然地就是减少hallicinate。虽然Honesty是&quot;对齐三剑客&quot;(helpful, harmless, honest)之一，但学界对于这方面的研究其实很少，作者就把这个领域按照alignment的语境重新定义了一下：对于做不出来的东西，要回答一个idk signs(I Don’t Know)</p><h2 id="formulation">formulation</h2><blockquote><p>这个写法不多见，一般论文没有这个section。因为本篇工作是第一篇工作，所以需要把问题描述定义一下，然后说一说评测方法是什么</p></blockquote><p>首先，这里作者做了一个简化: 这篇工作中，作者认为模型知识和世界知识是一个集和，假设模型不会说谎，如果回答错了，那大概率就是自己不懂这个知识。</p><img src=" ../../files/images/align-for-honesty/boundary.png"><h3 id="训练框架">训练框架</h3><p>作者提出了一套多轮refine的框架，希望随着训练的迭代，模型可以逐渐清晰地认识到自己的知识边界</p><blockquote><p>在这一点上，我倾向于OpenAI的观点：&quot;认知到自己的知识边界&quot;是一个latent knowledge，应该是模型本身具备的(毕竟是自己的知识，以及本身有calibration性质)，我们只需要训练模型去激发elicit出来。因此这个任务定义好以后，可能不太难</p></blockquote><p>作者把模型对于一个知识问题的回答分成了三类：<br>$$<br>c(x,y) =  \left{<br>\begin{aligned}<br>&amp; -1, \text{type}(y) = \text{idk}, \<br>&amp; 1, \text{type}(y) = \text{correct}, \<br>&amp; 0, \text{type}(y) = \text{wrong},<br>\end{aligned}<br>\right.<br>$$<br>接下来，根据该模型是否知道该问题的答案$k(x) = 1\text{ if model know the answer, else -1}$<br>$$<br>v(x,y) = \left{<br>\begin{aligned}<br>1, &amp; c(x,y)*k(x,y) = 1, \<br>0, &amp; \text{else},<br>\end{aligned}<br>\right.<br>$$<br>有了价值函数以后，就可以根据这个价值函数进行训练，预期价值函数随着训练变得越来越高。当然，</p><ul><li>在真实答案已知的情况下，c很容易获得</li><li>然而，k是一个很难获取的东西，因为是一个latent knowledge，后面作者探索了几种近似得办法</li></ul><h3 id="评测">评测</h3><img src=" ../../files/images/align-for-honesty/condition.png" style="zoom:33%;" ><p>即使按照上面的框架训练了，模型的效果仍然不好评测。不过，根据迭代前后模型的表现，作者可以天然的把问题分为9个大类</p><blockquote><p>其中的2,3类说明之前没做出来，后面做出来了(尽管没有泄露正确答案)。是个比较奇怪的现象，本篇工作不关注这个</p></blockquote><p>这里作者参考F1-score，讨论了一种近似的评测办法：</p><ul><li>over-conservativeness：我们不希望模型过于谨慎，希望能做出来的题目就正确回答。因此计算公式很简单</li></ul><p>$$<br>S_1 = \frac{7}{1 + 4 + 7}, \text{lower is better}<br>$$</p><ul><li>Prudence： 这个和上面的相反，考虑的是，不会做的问题，希望模型正确地回答idk</li></ul><p>$$<br>S_2 = \frac{8+9}{5 + 6 + 8 + 9}, \text{higher is better}<br>$$</p><p>有了上面的计算，就可以给出一个honesty增量</p><blockquote><p>注意，这个指标如果模型不训练，那就是只有1,5&gt;0，$S_1=0,S_2=0,S=0.5$</p></blockquote><p>$$<br>S_\text{honesty} = \frac{(1-S_1) + S_2}{2}<br>$$</p><h2 id="method">method</h2><p>首先，prompt方法是一个显然的办法(这里就是单轮迭代，只有prompt前后的区别)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAINTEXT"><figure class="iseeu highlight /plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Answer the question. If you don’t know the answer to the question, it is appropriate to say “I apologize, but I’m not able to provide an answer to the question.”</span><br><span class="line">   Q: &lt;question&gt;</span><br><span class="line">   A:</span><br></pre></td></tr></table></figure></div><p>接下来，训练地方法，作者设计了三种。这三种都是基于一个蒙特卡洛估计的办法，作者会让没对齐的模型对于一个问题生成多个(10个)回答，检查每个回答是否正确。给出一个信心值expected acc作为模型认知$k(x)$的一个近似</p><img src=" ../../files/images/align-for-honesty/sample.png" style="zoom:33%;" ><h3 id="ABSOLUTE">ABSOLUTE</h3><p>设定一个阈值$\tau$，$k(x) = 1 \quad if \quad \text{expected acc} &gt; \tau$。然后标数据的时候，把所有k(x)=-1的回答都改成了一个idk response</p><h3 id="CONFIDENCE">CONFIDENCE</h3><figure class="half">    <img src="../../files/images/align-for-honesty/numb.png" width="45%"/>    <img src="../../files/images/align-for-honesty/verb.png" width="45%" /></figure><p>这里，作者标数据的时候直接把confidence写在回答里，然后按照正常SFT的办法</p><h3 id="MULTISAMPLE">MULTISAMPLE</h3><p>刚才的absolute会根据一个阈值卡，这里作者直接把sample多次的每条数据当成单独的了，然后$k(x) = (c(x,y)==1)$。也就是说，标数据的时候，本来作对了的就不动，本来做错了的就改成一个idk response。</p><blockquote><p>值得注意的是，这个方案会把训练集扩大M倍</p></blockquote><h2 id="experiment">experiment</h2><p>这里作者提了两个朴素的baseline：</p><ul><li>原来的模型</li><li>fine-tuned：在相同训练量上，使用turbo的answer进行SFT的模型</li><li>prompt：上面提到的training-free方法</li><li>三种training方法，其中，$\tau$选取的是0.1</li></ul><figure class="half">    <img src="../../files/images/align-for-honesty/main.png" width="40%"/>    <img src="../../files/images/align-for-honesty/OOD.png" width="55%" /></figure><p>作者在TraivalQA数据集上做训练，使用Llama2-chat 7b作为基础模型，分别评测in-domain的traivalQA和OOD的另外三个数据集</p><p>效果如下：</p><ul><li><p>发现基于训练的方法显著好于不训练的方法</p></li><li><p>相对来说，把confidence放到数据里，会让模型表现更好</p></li><li><p>honesty属性在不同数据集上迁移能力较好，不管是ID还是OOD，加上confidence score都能让模型做的更好</p></li><li><p>直接finetune模型，会导致模型更加hallicinate，acc反而下降（这点在PKQA数据集表现得尤其明显）</p></li></ul><p>接下来，作者探索了$\tau$对结果的影响，画了一张类似f1里面auc的图。发现，$\tau$越大，越容易把数据分类成模型不知道</p><ul><li>因此idk数据越多，模型越容易变得over-confidence</li><li>另一方面，模型也越谨慎，所以prudence会提升，这里需要有一个权衡</li></ul><img src=" ../../files/images/align-for-honesty/auc.png" style="zoom:50%;" ><p>接下里，作者又做了scaling的实验：更大的模型会做得更好吗，更多的数据会做的更好吗？</p><figure class="half">    <img src="../../files/images/align-for-honesty/scale.png" width="35%"/>    <img src="../../files/images/align-for-honesty/data-scale.png" width="60%" /></figure><p>首先，作者发现，confidence-based method对于所有模型规模效果都要更好一些</p><blockquote><p>我发现：不同规模的模型对于Honesty的效果没啥区别，这说明了这个任务其实是挺困难的</p></blockquote><p>其次，如果在训练集中加入MMLU的训练数据，对于Multi-sample方法的帮助很大，说明这个属性的习得也许是data-hungry的，模型需要更diverse的情况来判断自己的知识边界</p><blockquote><p>不过，为啥Multisample+MMLU-data以后Acc下降这么多呢？</p></blockquote><p>最后作者做了一些&quot;对齐税&quot;方面的实验，发现Honesty训练基本不会导致模型在别的任务表现下降。最后，作者总结了一下limitation和future，提了几个问题，我觉得还挺有意思的，分享给大家：</p><ul><li>更好的k(x)：本篇工作用模型回答正确与否判断模型是否知道，这个在MMLU这种4选1中有误判假阳的情况</li><li>confidence score能不能更好的利用？这里作者和calibration联系了一下</li><li>和RAG的结合：认知到自己知识边界的模型更清楚自己该怎么利用外界知识</li><li>和长文本的结合：需要结合reasoning的长文本场景的Honesty现在还没有研究，并且需要更细致的评测和训练</li></ul><h2 id="我的思考">我的思考</h2><p>很好的文章，formulation到method到实验设计都很顺滑，逻辑很完整，我看完了以后主要想到下面几个问题：</p><ul><li>感觉可以评测一下turbo或者GPT4的表现？这里没做估计是因为需要一个unaligned模型去计算，没办法。要测也许只能给turbo来个un-align finetune，不知道是不是违规的</li><li>scaling实验中，发现所有llama表现都差不多，说明这个能力也许是一个emergent的，甚至是reverse-scaling的？</li><li>这个能力，似乎是不能通过SFT习得的？因为每个模型都有自己的知识边界。作者也提到了，SFT-baseline will lead models to learn to hallicinate</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;上周刷到了刘鹏飞老师的 Alignment For Honesty, 分享给了大家 &lt;a href=&quot;/86520e5.html&quot; title=&quot;2023-12-13-insights&quot;&gt;2023-12-13-insights&lt;/a&gt;。里面讲到如何训练LLM变得诚实，他沿用了孔子的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;知之为知之，不知为不知，是知(zhì)也。&lt;/p&gt;
&lt;p&gt;To say “I know” when you know, and “I don’t know” when you don’t, that is wisdom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我来一起看看他们是怎么做的吧&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="post-pretrain" scheme="https://www.yynnyy.cn/tags/post-pretrain/"/>
    
  </entry>
  
  <entry>
    <title>Weak-to-Strong Generalization(上): OpenAI是怎么看问题的？</title>
    <link href="https://www.yynnyy.cn/3229ec6"/>
    <id>https://www.yynnyy.cn/3229ec6</id>
    <published>2023-12-16T02:52:21.000Z</published>
    <updated>2024-08-09T09:26:46.970Z</updated>
    
    <content type="html"><![CDATA[<p>昨天OpenAI一口气更新了两篇论文，暨DALL.E 3之后的又一更新，其中一篇讲述了一个朴素的问题：如果未来的模型超越人类了，我们该怎么给他们提供监督信号？（毕竟我们只有人类——一个相对更弱的模型）</p><p>OpenAI把这个问题叫做weak-to-strong generalization在这里做了一些简单的尝试，对于这个问题的性质进行了一些探索。我们来一起学习一下他们看问题和解决问题的思路吧！</p><span id="more"></span><p>作者首先映入眼帘的就是ilya Sutskever，这个老哥真是为人类尽心尽力呀……OpenAI官网写的这个论文的作者是&quot;Safety&amp;Alignment&quot;</p><img src="../../files/images/weak2strong/authors.png"><p>而他的primary authors Jeff Wu更是重量级，我不多评价，直接列出其最近的publication，可以说是群星璀璨了……我什么时候能发12篇论文都有这个质量呀</p><img src="../../files/images/weak2strong/jeff_wu.png" style="zoom:30%;" ><h2 id="introduction">introduction</h2><p>回到正题，作者提到目前的研究更多focus在RLHF，既模型向人类偏好对齐，这套框架最终可以发展出一个类似于人的通用人工智能。然而，到此为止了：想要训练、对齐一个超越人类的智能，目前的方法都不再适用了，因为我们最高也只能用到人了。举个例子:</p><blockquote><p>我们可以要求模型写出一个100万行的很复杂的代码来完成一个任务，然而让人去看代码好不好是一个很难达成的任务。</p></blockquote><p>因此，目前还没有关于superalignment（对齐一个超越人的模型）的研究，并且，我们更希望其对应的方法可以不止适用于目前的模型，更适用于未来的模型(consistency)。在本篇工作中，作者类比了人类监督超人模型的问题，提出了一个类似的问题：可以用GPT2监督GPT4吗？</p><blockquote><p>具体来说，能不能用GPT2在某个任务的trainset上finetune，然后给trainset重新打一遍标签。让GPT4在GPT2打标签的trainset上训练，然后观察GPT2和GPT4在testset上的表现？</p></blockquote><p>这是一个很直观很容易实现的setup，作者把这个set up叫做weak-to-strong learning。</p><p>听起来这个weak-to-strong generalization似乎是不可能的，不过作者进行了一些解释：在学习过程中，我们不是预期弱模型教会强模型知识(这些知识是强模型本来就会的)，而是希望弱模型教会强模型这个任务的概念是什么，和一些intension，因此这个过程更像是激发&quot;elicit&quot;。用刚才那个例子来说，如果强模型已经能写100万行的代码，那他肯定有潜力去理解人类的目的是什么。因此即使弱模型给的标签有误差，应该不影响一个更鲁棒的强模型去理解，这就是作者预期出现的结果。</p><img src="../../files/images/weak2strong/first_perf.png" style="zoom:30%;" ><p>作者在传统NLP task(二分类)、chess puzzles(选出最优步的生成任务)、human preference modeling(二分类)上做了实验，并得出一些结论：</p><ul><li>强模型在weak-to-strong generalize以后基本都会超越帮他标数据的弱模型。？？？</li><li>直接做weak-to-strong generalize的效果并不好，比强模型+oracle-label finetune 差很远，可能需要一些额外的优化</li><li>一些很简单、很直觉地方案对于这个任务的提升很大。这说明这个领域还有很大的进步空间</li></ul><h2 id="method">method</h2><p>像刚才Introduction里提到的，这个setup是很简单的。不过，比起一般论文讲完方法讲实验，作者额外说了一下这个setup的优点和问题。作者额外说了一个他们观察的指标performance gap recovered(PRG)：假设weak-to-strong训练比小模型效果好，那他能恢复强模型直接训练的几分风采？注意，这里面PRG=1代表和强模型直接训练等价，PRG=0代表和小模型表现一样，完全没有generalize。</p><blockquote><p>为什么要和 “强模型+orcale finetune” 去比？虽然现在的setup是数据集，因此有orcale 标签可以做&quot;强模型+orcale finetune&quot;的上限实验。但事实上所谓的&quot;orcale 标签&quot;也是人去打的标签，未来的模型在完成超人任务时就没有标签了，或者说只有人去打的&quot;噪声标签&quot;。</p><p>这种情况下，我们可以预设有&quot;外星人打的oracle标签&quot;跑上限实验，也就是说：我们更关注超人模型的能力上限能不能被激发出来。PGR可以看做一个&quot;激发比例&quot;</p></blockquote><img src="../../files/images/weak2strong/prg.png" style="zoom:50%;" ><p>优点：总体来说，就是这个方案更有利于scaling</p><ul><li>对于模型不可知，任何的 强-弱 模型对都能跑这个实验，也不需要一定要有GPT4。另外，这个路径不需要人，因此实验很快</li><li>对任务不可知，任何任务都能跑这个setup</li><li>这里面得到的关于超人模型的结论，即使现在也能用的上：未来对齐GPT10的方法，不妨现在就用到GPT4里？</li></ul><blockquote><p>这也许暗示着目前OpenAI内部已经有超人模型了……</p></blockquote><p>缺点：</p><ul><li>Imitation saliency：这个我觉得很难翻译，就直接摆原文了。这里讲的是未来的超人模型和今天的GPT4存在统计学的巨大差异。对未来的超人模型来说，理解人类的想法是一个很简单的问题，但GPT4去理解GPT2可能没那么简单。并且，GPT2犯的错误类型和人类犯的错误类型也有很大区别，因此目前的结论更多是研究GPT2错误，会有统计学差异。</li><li>数据泄露：目前的setup里面的oracle标签可能或多或少在预训练数据里出现过。未来的模型不存在&quot;超人的预训练数据&quot;，因此实验结论可能和现在有偏差</li></ul><p>总体而言，超人模型长什么样、有什么现象现在仍未可知。作者提到，我们更需要对于未来的所有模型都跑一跑现在的实验，观察一下实验结论是否是稳定存在于所有模型中的……怎么有一种三体人的水滴在破坏物理学规律的感觉……</p><h2 id="experiment">experiment</h2><p>作者在附录里讲了实验细节，大致有如下关键点：</p><ul><li>NLP task中，作者选取了29个有代表意义的task，都转换成了2分类任务，为了简化整个setting。</li><li>在模型方面，作者使用GPT4架构，构造了和GPT2、GPT3、GPT3.5、GPT4相当计算量的模型(但都是GPT4架构)。作者去掉了decoder最后的那个unembedding layer，换成了一个二分类头做实验</li><li>数据方面，作者把数据做了一些清洗，使得数据集中两个类的标签数量接近，然后有数据分类风险的数据们都在train test的其中一个split上</li></ul><p>另外两个setup：</p><ul><li>reward model：作者直接使用了训练ChatGPT的SFT数据集，里面有数据对以及human给出的偏好。可以看成一个很复杂的二分类任务。这个的训练方法和正常的reward model一样，把unembedding layer改成一个N-&gt;1的linear来输出一个float当做score。最后用两个candidate的score谁大来作为预测</li><li>chess puzzle：很多象棋的棋局，作者希望模型给出第一步，最优解是什么。这个可以看做一个生成任务，因为可选位置有很多个。原始数据大概长下面这样</li></ul><img src="../../files/images/weak2strong/chess.png" style="zoom:20%;" ><blockquote><p>我猜测，作者选这几个setup是基于这样的考虑：</p><p>NLP task，大小模型做的都不错。</p><p>chess puzzle，大模型做的不错，小模型做不了。</p><p>reward model，大小模型做的都不太好</p></blockquote><p>由此，作者</p><ul><li>首先构造了大小不同的多个GPT4架构的模型。这些模型都是做过预训练的，使用的预训练数据和GPT4一致</li><li>对于每个setting，作者分别跑了刚才提到的四步测试<ul><li>小模型在trainset真实标签上做训练FT</li><li>FT的小模型给trainset重新打一遍伪标签</li><li>大模型在带噪的trainset上训练，同时做一个真实标签FT的上限训练</li><li>测量三个模型在testset的表现，计算出PRG</li></ul></li></ul><p>论文的主试验就是下面这个图</p><img src="../../files/images/weak2strong/main.png" style="zoom:50%;" ><p>这个看着有点费解，后面的所有图都用了相同的画法，我在这里解释一下：</p><ul><li>纵着看，每列代表一个task。(29个NLP task被划到一起了 )</li><li>横着看，上面三个图报告的是数据集上的performance，下面三个图报告的是PRG</li><li>每个图里面的横轴说的是学生模型的大小，标准是GPT4相对计算量。注意，学生模型一定比老师强</li><li>图里的一条线代表的是同一个老师监督不同大小的学生的水平。老师越大，线越亮。注意，老师永远小于学生，因此这个图差不多都在右下三角半区</li><li>上面的三个图里最上面的线是大模型的上限实验结果。</li></ul><p>作者观察到几个关键结论：</p><ul><li><p>PRG基本永远大于0，学生总是强过老师。</p></li><li><p>老师变大或者学生变大，PRG基本都会增大。对于最强的学生，PRG甚至会超过50%：这也许预示着，对于超人模型来说，weak-to-strong generalization实际上很简单</p></li><li><p>chess puzzle任务上，这些现象更显著。当学生很小时，PRG接近0，大小模型做的都很垃圾。并且，这个任务上的PRG和前面相反，出现了inverse scaling现象？即同一个老师，学生越强，PRG反而越低了？</p></li><li><p>对于reward model任务，PRG一直都很低。对于所有setting，PRG都低于20%</p></li></ul><p>作者特别强调了一下，PRG&gt;0这个事情基本一直成立，这个现象其实很神奇，因为也没做什么特别的优化。这个现象预示着superalignment的可能性，也需要在相关领域有更多的研究</p><h2 id="如何提高weak-to-strong-generalization的表现？">如何提高weak-to-strong generalization的表现？</h2><p>做完主试验，一个显然的思路就是：如何优化他？作者在这里简单地想了几个办法。在这里，我们看一下OpenAI研究员的&quot;注意到&quot;是什么样的吧？</p><h3 id="bootstrapping：级联">bootstrapping：级联</h3><p>注意到：刚才提到老师和学生差距太大，以及老师/学生提升，PRG效果都会变好。那么，如果用小老师监督稍微强一点的学生，再把学生当成新老师，监督更强一点的学生……如此反复，效果会更好吗？</p><img src="../../files/images/weak2strong/bootstrap.png" style="zoom:50%;" ><p>作者在chess puzzle进行了实验，图里面的虚线是主试验里面的小老师直接监督大学生，实现是级联监督的结果。作者发现：</p><ul><li>当最开始的大小gap差距小时，这个办法是优化</li><li>当最开始的大小gap差距大时，这个办法是负优化</li></ul><p>尽管在chess puzzle看起来效果不错，但作者发现这个办法在NLP task提升不大，在reward modeling没有提升。对于这个现象。作者&quot;注意到&quot;：主试验中，Chess这个场景gap越大PRG越小，其他场景都是gap越大PRG越大。因此级联方法缩小gap，对于Chess才有帮助。</p><blockquote><p>可是，主试验又为什么会有这样的现象呢？</p></blockquote><h3 id="auxiliary-confidence-loss-学生要学会质疑">auxiliary confidence loss: 学生要学会质疑</h3><p>注意到：强模型是在弱模型打得标签上训练的，因此强模型不得不去学习弱模型犯的错误。我们更希望强模型从噪声标签中学习任务的intension，而不是学习弱模型的错误(因为对强模型来说，理解弱模型犯的错误也是一个简单的事情)。但目前的训练框架没法区分这两者。</p><p>作者想了一个朴素的办法，把强模型自己的confidence loss加入进去作为辅助loss，和弱模型的标签loss形成一个对抗，对冲一下&quot;学习犯错&quot;带来的影响。<br>$$<br>L’ = (1-\alpha)\text{CE}(f(x), f_w(x)) + \alpha \text{CE}(f(x), \hat{f}_t(x))<br>$$<br>其中$f_w(x)$是老师的logits， $\hat{f}_t(x)=I[f(x) &gt;t]$是给学生模型的预测按照阈值t做了离散化后的标签。$\alpha,t$都是超参数</p><p>注意到，上面的公式等价于下面这个，思路和self-training有点类似<br>$$<br>L’ = \text{CE}\left(f(x), (1-\alpha)f_w(x)+\alpha\hat{f}_t(x) \right)<br>$$<br>这个新loss相当于噪声标签和学生自己打的标签的一个加权平均。最开始的时候学生啥都不会，应该把$\alpha$调小，后面再慢慢增大，作者在实验中就在前20%训练中把$\alpha$从0提升到0.75，后面维持0.75不变</p><blockquote><p>如果老师和学生模型的算法相同，也许可以不再标签层面做平滑，可以直接用logits做平滑，去掉一个超参数t。作者估计是为了保证scaling的通用性，就没这么优化。</p></blockquote><img src="../../files/images/weak2strong/aux.png" style="zoom:50%;" ><p>类似上面，作者在NLP任务上做了实验，发现这个优化对于NLP任务的PRG提升很明显，甚至从25%提升到了80%……作者认为这个结论是符合直觉的：aux loss可以减缓学生在&quot;weak error learning&quot;上的计算资源，减缓对于老师的错误的拟合</p><p>上篇也写了4000多字了，就讲到这里，在下篇我会分享作者对于这些现象，以及superalignment问题的理解，再说一说我对这些问题的看法，以及我对于这篇论文研究思路的思考。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;昨天OpenAI一口气更新了两篇论文，暨DALL.E 3之后的又一更新，其中一篇讲述了一个朴素的问题：如果未来的模型超越人类了，我们该怎么给他们提供监督信号？（毕竟我们只有人类——一个相对更弱的模型）&lt;/p&gt;
&lt;p&gt;OpenAI把这个问题叫做weak-to-strong generalization在这里做了一些简单的尝试，对于这个问题的性质进行了一些探索。我们来一起学习一下他们看问题和解决问题的思路吧！&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="superhumanAI" scheme="https://www.yynnyy.cn/tags/superhumanAI/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</title>
    <link href="https://www.yynnyy.cn/68247c98"/>
    <id>https://www.yynnyy.cn/68247c98</id>
    <published>2023-11-28T07:34:13.000Z</published>
    <updated>2024-08-09T09:26:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>读得论文多了，写的笔记反而更少了……很多篇论文都想写，最后哪个都没写出来。今天来讲讲yejin Choi 2020年的一个论文：如何用模型自己在训练过程中的表现作为自监督信号，衡量训练集中每一条数据的质量？</p><blockquote><p>很难想象这是yejin choi三年前思考的问题，我直到最近读到这篇论文，还觉得思路很新颖、很精妙</p></blockquote><span id="more"></span><p>作者是Yejin Choi团队,一作Swabha Swayamdipta最近还做了一些有趣的工作，比如这篇：We’re Afraid Language Models Aren’t Modeling Ambiguity。都是挺有意思的选题</p><img src="../../files/images/Dataset-Cartography/authors.png"><h2 id="data-map">data-map</h2><p>回到本篇工作，作者主要探索了以下问题：目前(2020年)学界的范式是选择越来越大的数据集做训练。因为大家发现随着数据集扩大，其多样性会上升，进而促进模型的分布外泛化能力。</p><p>但是，随着数据集的扩大，数据质量一定会下降，作者想到: 有没有可能数据集中每条数据对语言模型的贡献是不一致的呢？作者希望找到一种自动地标注方案。作者直觉地想要用两个维度对数据分类：在一条数据过了很多epoch以后每次的loss对应的平均数和方差。作者把这两个轴叫做confidence(平均数) 、variability(方差)</p><p>对于比如SNLI数据集，作者尝试把RoBERTa训了几个epoch，然后统计里面每条数据在每个epoch的loss，进而画了一个散点图，其中每个点代表一条数据。作者直觉地认为，这个类似钟型曲线天然地把数据分成了三种情况：</p><ul><li>easy-to-learn：很快就学会了，并且方差很小，一直都做对。占大多数</li><li>hard-to-learn：一直学不会，因此方差也很小</li><li>ambiguous：一轮能做对一轮做不对，方差很大。模型对这种数据的判断没有把握</li></ul><img src="../../files/images/Dataset-Cartography/intro.png"><p>另外，对于confidence做离散化，还可以统计acc。作者还把 “n个epoch中一条数据acc”的比例定义为了correctness，在图中表现为了不同颜色的小点。</p><p>由此，作者把这个方法叫做data-map，和标题里的地图学呼应：地图是地球固有的属性，而数据中的confidence、variability也是模型在训练中自己表现出来的性质。</p><p>接下来，作者就要从这个现象出发，展露一下研究员的天才思路，设计一系列实验和探索。</p><h2 id="data-map能作为选择训练数据的指标吗？">data-map能作为选择训练数据的指标吗？</h2><p>作者实现好奇的就是：不同区域的数据，对于训练有什么贡献？实验设计很简单，就只选择对应区域的数据做训练就可以了。在训练完以后，作者分别作了in/outof - distribution(ID、OOD)的测试。</p><ul><li><p>100 train：阳性对照</p></li><li><p>random: 随机选33%，阴性对照</p></li><li><p>high-correctness: correctness从高到低前33%的数据</p></li><li><p>low-variability、low-correctness、high-confidence、high-confidence 同理</p></li><li><p>hard-to-learn: 指的是low-confidence</p></li><li><p>ambiguous： 指的是high-variability</p></li></ul><img src="../../files/images/Dataset-Cartography/exp.png" style="zoom: 33%;" ><p>作者在winoG上训练，然后分别把winoG、WSC作为ID OOD测试，神奇的现象来了：</p><ul><li>仅在hard-to-learn或者ambiguous的33%数据上训练，OOD能力甚至比阳性对照还要好！</li><li>仅在eazy-to-learn的数据上训练，似乎对ID和OOD测试都没啥帮助……不如random 33%</li><li>尽管没有对选数据的方法专门做优化，但效果比几个active-learning算法的效果还要好</li></ul><p>看起来，hard-to-learn和ambiguous的数据对模型的效果起到关键作用。ID的效果和训练集大小强相关，我们相对更关注OOD。因此作者说到这套data-map的方案某种意义上提供了一个加速训练的潜在方案。然而，从这个角度看，这个方案需要先在全集上训一遍模型，这肯定比正常训练开销更大。因此这个方法只有理论价值</p><h2 id="可以抛弃eazy-to-learn数据吗？">可以抛弃eazy-to-learn数据吗？</h2><p>既然上面研究发现hard-to-learn和ambiguous数据最有用，那接下来一个直观的问题就是：如果用更少、少于33%的这种数据，也能达到这种效果吗？</p><p>于是作者选了ambiguous数据的前50%, 33%, 25%, 17%, 10%, 5%，1%作为训练集尝试了实验</p><img src="../../files/images/Dataset-Cartography/rate.png"><p>先看左边两个图：横轴是上面那个top-ambigious训练数据的百分比，纵轴分别是ID和OOD的效果。神奇的又来了：当训练数据低于某个阈值以后，训练就崩溃了？？另一个实验表明，相同的数据量，如果选取不是按照top-ambigious而是random，训练就是正常的</p><p>因此作者想到了一个可能：会不会是eazy-to-learn的数据虽然对于效果没什么帮助，但是对于稳定训练很有帮助？因为更少的top-ambigious显然就采样不到eazy-to-learn的数据了。于是作者点子又来了，做个阴性对照，把刚才训崩的数据比例(17%)里，随机将一部分top-ambigious的数据换成eazy-to-learn的数据？</p><p>于是就画出了右图：作者发现，哪怕在17%中，只要再掺入1/10=1.7%的eazy-to-learn数据，训练就正常了起来？？另外，如果替换的比例太高，ID和OOD的效果就又掉下去了。</p><p>作者最后又提出了一个开放性的研究问题：如何在训练中正确选择各个区域的比例？</p><h2 id="hard-to-learn的数据可能因为误标注吗？">hard-to-learn的数据可能因为误标注吗？</h2><p>想到两个点：</p><ul><li>SNLI画的data-map中hard-to-learn很多，winoG画的data-map，hard-to-learn看起来很少。同时我们知道winoG中的数据被人类精心clean过因此误标注更少</li><li>对于误标注的数据，模型显然是&quot;hard-to-learn&quot;的</li></ul><p>怎么验证这个猜测呢？作者点子又来了</p><p>首先，来个模拟实验。作者将winoG中1%的eazy-to-learn数据的标注换一下，造一批”误标注“数据。在eazy-to-learn数据中采样是因为这里面大概率之前不是误标注的数据</p><img src="../../files/images/Dataset-Cartography/mislabel.png"><p>接着作者用新的数据集重新画data-map，观察刚才那些点在新的图中的位置，作者给出了这些点confidence、variability的直方图。发现confidence显著降低、variable显著升高。这展示了数据中误标注的可能性</p><p>接下来，作者问了另一个问题：既然有潜在的误标注风险，那有可能将data-map作为一种自动的误标注识别手段吗？</p><p>首先作者把刚才的数据集(含1%人造误标注数据)，再采样了同样的1%正常数据形成了一个误标注占50%的数据集。训练一个classifier，其输入是每个instance的confidence、variability，输出2分类。发现这个classifie的测试集F1是100%？？</p><p>接下来，作者将classifier重新应用到原始winoG数据集，发现31/40k的数据划分为了mislabel。同理在SNLI上做同样的实验，发现有15k/500k划分为了mislabel。这和两个数据集的数据质量一致</p><p>最后，为了让作者的逻辑链条完整，作者开展了人类实验，找人去看classifier划分出来的mislabel数据。人类标注结果表明: classifier选出的&quot;mislabel&quot;数据，67%是真的mislabel。这个数字在SNLI上是76%。剩下的基本上也是比较&quot;歧义&quot;的instance</p><p>最后，作者谈到：data-map可以作为一种潜在的对数据集mislabel问题进行自动检测的手段，并且效果还不错。</p><h2 id="模型在训练中表现出来的这种性质，和数据集固有的不确定性有关吗？">模型在训练中表现出来的这种性质，和数据集固有的不确定性有关吗？</h2><p>众所周知，数据集中有一些固有不确定性：有一些instance是歧义的，理论上就是填什么都可以。另外，对于模型无法预测的位置，到底是来源于数据集固有的不确定性，还是模型本身的局限性(换个更强的模型没准就会了)呢？</p><p>作者想到一个办法来衡量数据集中固有的不确定性：在数据集制作时，都是找人来标注。对于本身有歧义的例子，不同的标注员之间应该自己也有不一致性。所以作者分析了data-map中每条数据，列出了标注员当时对于这条数据的一致性</p><img src="../../files/images/Dataset-Cartography/consistency.png"><p>作者发现，模型划分data-map的方式，和人类当时标注时的一致性有非常强的相关性: 起码对于eazy-to-learn数据，标注员基本一致性都很高。</p><h2 id="我的思考">我的思考</h2><p>这个论文的逻辑太顺了：一般我写笔记都会简略写experiment部分，但这次我一个都没有省，并且组织逻辑和Yejin Choi论文组织逻辑完全一致。</p><p>作者从一个现象出发，和学界已经存在的问题联系起来，探索他们发现的现象的潜在应用价值。从联系方向，到提出问题，到设计实验，到画图展示的形式，都展示了研究员敏锐的数据直觉，值得我们去学习……相比之下，再看看近两年的大多数论文写成啥样子了……</p><p>站在2023年的视角下，我只能说对这个论文提出几个潜在的研究问题：</p><ul><li>在instruction tuning领域，大家逐渐意识到diversity和quality的矛盾，以及对最终训练效果的影响。相比于Wizard LM这种自动化的数据筛选。让模型自己去选择数据是一种新的思路吗？</li><li>data-map的结果是和模型绑定的。对于同一个数据集，换一个模型可能画出来的图就会有变化。比如GPT4，可能在SNLI上画的图全是eazy-to-learn。这点对于选择数据至关重要：一条数据不适合这个模型，但有可能适合那个模型，这和模型的基础能力有关。我们不指望找到一个适用于所有模型的golden selection method(可能世界上也不存在这样的方法)，相比之下更希望能找到最适合与这个模型的训练数据</li><li>这两年学界出现了一个新的关键词calibration：对于很强的LLM来说，自己的confidence和acc成强相关性。作者在这片工作中发现另一个联系：自己的confidence和数据集的固有不确定性成高度的相关性。由此我产生了一个问题：既然三者都有相关性，那么，模型的calibration性质可能是来源于&quot;在含固有不确定性的无监督corpus上预训练&quot;吗？如果我们的corpus去掉了不确定性(比如RLHF数据集)，那么模型的calibration性质是不是就消失了呢？</li></ul><p>最后，这是Yejin Choi三年前研究的东西，与君共勉</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;读得论文多了，写的笔记反而更少了……很多篇论文都想写，最后哪个都没写出来。今天来讲讲yejin Choi 2020年的一个论文：如何用模型自己在训练过程中的表现作为自监督信号，衡量训练集中每一条数据的质量？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;很难想象这是yejin choi三年前思考的问题，我直到最近读到这篇论文，还觉得思路很新颖、很精妙&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>LLaVA, LLaVA 1.5和LLaVA-Plus: 讲讲LMM</title>
    <link href="https://www.yynnyy.cn/d33e88af"/>
    <id>https://www.yynnyy.cn/d33e88af</id>
    <published>2023-11-11T03:15:21.000Z</published>
    <updated>2024-08-09T09:26:46.970Z</updated>
    
    <content type="html"><![CDATA[<p>昨天刷到新挂的LLaVA-Plus的Arxiv论文，讲怎么做多模态的ReACT与训练模型。正好发现LMM(Large Multimodal Model)系列的模型似乎怎么讲过。那么LLaVA系，三篇论文，今天一次说完。</p><blockquote><p><a href="https://llava-vl.github.io">Visual Instruction Tuning</a></p><p><a href="https://llava-vl.github.io">Improved Baselines with Visual Instruction Tuning</a></p><p><a href="https://llava-vl.github.io/llava-plus/">LLaVA-Plus: Learning to Use Tools for MulitModal Agents</a></p></blockquote><p><s>flamingo、Kosmos 2.5下次有时间说啊</s></p><span id="more"></span><img src="../../files/images/LLaVA/authors.png"><p>首先，在作者上，这三篇论文基本上是一脉相承，没有出现LLaMA的黑吃黑现象。他们的鼻祖LLaVA一代发表在neurIPS 2023 Oral，上Arxiv的时间是4月份。当时LLM基本还是蛮荒时期，大家都是被GPT-4V发布时的惊艳骗进来，想搞个猴版，技术路径和研究思路有迹可循。不像现在成熟以后，论文都是天马行空地说，很难把握住核心思想。</p><h2 id="LLaVA">LLaVA</h2><p>所谓的large multimodal model, 就是想把LLM的能力范围再往前推一步，让他可以&quot;see and hear&quot;.</p><p>LLaVA的主要思路是：由用一个CLIP作为image encoder，然后训一个轻量级的链接器，把clip embedding连接到一个LLM的空间，由此让一个LLM理解图片，进而变成LMM。</p><p>上面的这套流程，重点就是需要图文数据集，而且需要是instruction-follow数据集。目前的图文数据对大多是image caption的，文字主要是描述文字内容。另外有一些VQA的数据集，其问答式针对图片里的一些元素，总体还是比较简单。</p><p>LLaVA的作者对上面的思路做了一下梳理，构造了一个数据集。里面所有的数据的格式都是类似于下面这样<br>$$<br>X_q X_v<STOP>\text{\n Assistant} : X_c <STOP>\text{\n}<br>$$<br>其中最前面会有一个图片，然后会有一个问题，接下来是回答。总体是多轮对话形式的。作者定下了三种数据类型，共158K数据</p><ul><li>conversation：58k</li><li>detailed descrption: 23k</li><li>Complex reasoning: 77k</li></ul><h3 id="model-and-training">model and training</h3><p>模型结构上，非常简单。作者使用CLIP ViT作为image encoder，用LLaMA作为LLM。然后clip embedding通过一个Linear层投射到word embedding层。接下来直接将他作为一个word embedding和其他word embedding一起去跑LLM后面的流程</p><img src="../../files/images/LLaVA/model.png"><p>作者设计了一个两阶段的训练任务。第一阶段是对齐文本图像空间。作者直接使用图文数据对作为数据集: 图在前，文在后。然后训练的时候只训练W的权重。</p><p>接下来是instruction follow阶段。这一部分训练image encoder和W的权重，用他的158k数据集训练出来的instruction follow模型</p><img src="../../files/images/LLaVA/format.png"><p>训出来的模型基本都是按照这种形式。可以看到，传入的图片基本上是一个原图，加上一些的形式bounding box，然后几种数据格式在表现上就是对话。作者在训练的时候，只有answer的部分是有loss的。</p><p>LLaVA是一个很干脆的论文，把一个思路清晰的做了出来，并且比较重视原始数据。</p><h2 id="LLaVA-1-5">LLaVA 1.5</h2><img src="../../files/images/LLaVA/1.5performance.png" style="zoom: 33%;" ><p>到了二期，论文只有短短5页。作者在方法上没什么更新，只是把LLM基座模型换成了13B，把image encoder换成了更大更强的CLIP-ViT-L-336px，然后把连接层的Linear换成了双层MLP。</p><p>另外作者还观测到之前LLaVA由于图片分辨率的问题，会看不清楚输入，新的image encoder可以看得更清楚</p><p>作者从scaling的视角来看他们的方法， 提出了一个问题：158k数据够了吗？</p><img src="../../files/images/LLaVA/scale.png" ><p>作者把数据集混入了一些VQA、OCR的数据，另外对instruction-following prompt中要求对response给出格式要求，这样更方便模型对学习，比如说：</p><blockquote><p><em>Answer the question using a single word or phrase</em>.</p></blockquote><p>作者在上图中报告了对于数据、模型大小，和图片分辨率做scale后的效果。看起来提升模型大小是涨点最有效的办法</p><p>最后用最大最强的模型去刷了个榜。这篇论文就是经典的二期论文的写法：找到最严重的问题，并修复之。另外，从scale的视角看整个问题，很好的视野。</p><h2 id="LLaVA-Plus">LLaVA-Plus</h2><p>在LMM基座模型效果提升了以后，作者瞄准了现在比较火的工具学习场景：能不能让LMM去通过工具调用来进一步提升task solving的能力？由此写出了LLaVA-Plus(Plug and Learn to Use Skills)</p><img src="../../files/images/LLaVA/intro.png" ><p>既然立足是一篇Agent的论文，作者论文写法都变了，用了story oriented的写法，故事性变得很浓。可见作者的写作功底还是很好的。在Introduction中甚至搬出来了祖师爷&quot;Society of Mind&quot;的理论(1988)</p><blockquote><p>Society of Mind: each tool is originally designed for a specific skill and by itself is only useful for specific scenarios, but the combinations of these tools lead to emergent abilities that show signs of higher intelligence.</p></blockquote><img src="../../files/images/LLaVA/capbility.png" ><p>LLaVA的总体流程和ToolBench非常相似，具体可以看这个 <a href="/660d5dc5.html" title="论文阅读[粗读]-TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS">这个笔记</a>。大致上就是</p><ul><li>先找了一些target image</li><li>找GPT4对image造出来一个query too observationl answer这样的tuple</li><li>把这个tuple弄成一个数据集。以此作为训练数据，训练出来LLaVA-Plus</li></ul><img src="../../files/images/LLaVA/tool.png" ><p>具体的数据格式看起来和ReACT完全一致，经典的thought、action、observation、answer。作者一共制作了一个LLaVA-158K的数据集，另外把测试集搬出来做了一个叫LLaVA-Bench的测试系统。</p><p>作者说明，训练出来的模型达到SOTA水平。</p><h2 id="我的思考">我的思考</h2><p>可以看到，从今年4月走到11月，作者在LLaVA的道路上一路深耕，提高基础能力。再基础能力提上去以后，逐渐做到Agent能力。估计后续随着能力进一步提高，也许可以尝试多步工具调用以及多模态planning。LLaVA是一种lightweight的多模态连接方式，对text encoder和image encoder的模型结构都不做要求，从结果来看，效果还挺好。</p><p>为什么不在预训练阶段就用多模态的模型？一方面，作者在论文里说到的一个问题其实很有道理：目前的多模态数据主要就是图文caption对，这样的数据可以让模型去理解图片，但也没有进一步的能力需求了(不像纯文本数据那样需要推理等)，即使是VQA也以简单的数数、区分左右等等为主。训练数据决定模型学到的能力，我们可能得找到更好的多模态预训练数据。另一方面，多图多文结合也是一条路子。像GPT-4v就是天生多图的，这样的多图数据某种意义上和多步推理有着更紧密的联系。</p><p>最后，我很喜欢scaling的视角，我觉得scaling的结论是最可信的结论，也最有可能是未来大规模应用的前提。也不知道以后的LMM到底是单模态模型的整合，还是预训练级的多模态……</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;昨天刷到新挂的LLaVA-Plus的Arxiv论文，讲怎么做多模态的ReACT与训练模型。正好发现LMM(Large Multimodal Model)系列的模型似乎怎么讲过。那么LLaVA系，三篇论文，今天一次说完。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io&quot;&gt;Visual Instruction Tuning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io&quot;&gt;Improved Baselines with Visual Instruction Tuning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io/llava-plus/&quot;&gt;LLaVA-Plus: Learning to Use Tools for MulitModal Agents&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;s&gt;flamingo、Kosmos 2.5下次有时间说啊&lt;/s&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="多模态" scheme="https://www.yynnyy.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="tool-learning" scheme="https://www.yynnyy.cn/tags/tool-learning/"/>
    
  </entry>
  
  <entry>
    <title>OpenAI开发者大会的所有可能结局</title>
    <link href="https://www.yynnyy.cn/75adad28"/>
    <id>https://www.yynnyy.cn/75adad28</id>
    <published>2023-10-21T12:44:18.000Z</published>
    <updated>2024-08-09T09:26:46.945Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，OpenAI打算在2023/11/6，ChatGPT问世(2022/11/30)大约1一年以后，召开第一届开发者大会，距离现在还有15天。我们不如来大胆预测一下开发者大会可能更新的所有内容吧！即是预测，也是我对OpenAI接下来开发的功能的期望。你觉得哪种结局最有可能呢？</p><blockquote><p>所有图片均由DALL·E 3生成</p></blockquote><span id="more"></span><h2 id="大一统结局">大一统结局</h2><p>目前的ChatGPT Plus我们需要在各种实验性功能中选择一个使用，大家都猜测这背后是GPT4在各种下游任务中特化的finetune版本。大家现在每次只能选一个，直接选择困难症，哪个都想要，但不能同时存在于一个context下。</p><p>有没有可能在11.6开发者大会中，OpenAI大一统所有checkpoint，将会使用一个统一的接口做完所有事情：有视觉可以看图，也能上网，能使用工具，还能做jupter notebook执行，最后能用DALL·E 3画图(画出来的图也可以直接用视觉去理解)。</p><blockquote><p>大一统结局：ChatGPT Plus的选择困难症可以休矣</p></blockquote><figure class="half">    <img src="../files/images/all_ends_for_11.6/consistence.png" width="35%"/>    <img src="../files/images/all_ends_for_11.6/c2.png" width="35%" /></figure><h2 id="模态联结结局">模态联结结局</h2><p>Google准备训练Gemini多模态大模型抢OpenAI的风头，据知情人士透露：OpenAI准备在Gemini上线之前训练一个多模态大模型Gobi来对抗Google的竞争。如果开发者大会上，Gobi已经训练完成了，Google还有后手吗？</p><blockquote><p>模态联结结局：Gobi is all you need</p></blockquote><img src="../files/images/all_ends_for_11.6/gobi.png" style="zoom:50%;" ><h2 id="OpenA-gent-结局">OpenA(gent)结局</h2><p>Agent技术目前非常火，过了大约半年，一直没看到OpenAI出手。按照其一惯逻辑，在找到一以贯之的思想之前，他们可能会做总结对比的工作。就像强化学习一样，OpenAI有没有可能推出来一个Agent-gym框架，从此大家开发Agent都是基于Agent-gym的接口和设计理念。</p><blockquote><p>OpenA(gent)结局：OpenAI的阴影笼罩整个Agent研究。</p></blockquote><img src="../files/images/all_ends_for_11.6/agent.png" style="zoom:50%;" ><h2 id="True-OpenAI结局">True OpenAI结局</h2><p>虽然之前业界普遍承认最强模型是GPT4，但开源和实际构建应用时大家会选择Llama和Llama2为主。OpenAI事实上并没有真正的Open，在开源界被Meta AI统治了。</p><p>因此之前在流传小道消息：OpenAI打算开源一个语言模型，代号是G3PO，也许开发者大会就是G3PO问世的时间，OpenAI用一年时间转形成了True OpenAI</p><blockquote><p>True OpenAI结局：你的llama3，何必是llama</p></blockquote><img src="../files/images/all_ends_for_11.6/TrueOpenAI.png" style="zoom:50%;" ><h2 id="超人主义结局">超人主义结局</h2><p>OpenAI开启了SuperAlignment小组，准备在2030年之前实现AGI。按照他们一贯是先研究、再宣发的特性，保守估计他们在2024年就研究完成了AGI，后面做5年的对齐工作。如果在开发者大会上，他们让AGI技术初步亮相，会不会进一步推进全世界的研究热情呢？</p><blockquote><p>超人主义结局：北大的通班不用再办了</p></blockquote><img src="../files/images/all_ends_for_11.6/AGI.png" style="zoom:50%;" ><h2 id="硅基飞升结局">硅基飞升结局</h2><p>OpenAI部署各种GPT服务，可能是世界上最缺GPU的人。之前听说OpenAI在研究自研AI芯片，如果OpenAI已经研究出来了AI芯片，可以把GPT4加密打印在门电路里，以后只要买到了GPT4-i7芯片，就能直接通过芯片激活来做推理了。每个时钟周期就是一个流水推理周期，64个周期就能推完64层transformer</p><blockquote><p>硅基飞升结局：人类只不过是一段boosting程序，引导硅基生命的到来。</p></blockquote><img src="../files/images/all_ends_for_11.6/chip.png" style="zoom:50%;" ><h2 id="One-More-Thing结局">One More Thing结局</h2><p>6个月前的WWDC上，库克在发布的结束用一句&quot;one more thing&quot;引出了最重要的Apple Vision Pro发布。OpenAI会不会也在憋一个&quot;终极大招&quot;等着大家在开发者大会最放松警惕的时候引出来。比如有人统计了目前模型的运行速度，发现随着时间推移变得越来越快。有没有可能OpenAI找到了一种把稀疏大模型同时变得稠密的办法。最后来一句王炸。</p><blockquote><p>One More Thing结局：通过最新的训练方法，我们成功找到了用100M稠密模型比肩100B稀疏模型的办法。</p></blockquote><img src="../files/images/all_ends_for_11.6/turbo.png" style="zoom:50%;" ><h2 id="快进结局">快进结局</h2><p>如果……上面的一切同时发生呢？</p><blockquote><p>快进结局：开发者大会马蹄疾，一日看尽长安花！</p></blockquote><img src="../files/images/all_ends_for_11.6/fast.jpg"  ><p>那么，你觉得哪一种结局最有可能呢？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;众所周知，OpenAI打算在2023/11/6，ChatGPT问世(2022/11/30)大约1一年以后，召开第一届开发者大会，距离现在还有15天。我们不如来大胆预测一下开发者大会可能更新的所有内容吧！即是预测，也是我对OpenAI接下来开发的功能的期望。你觉得哪种结局最有可能呢？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有图片均由DALL·E 3生成&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>2023-09-29总结</title>
    <link href="https://www.yynnyy.cn/6c02904f"/>
    <id>https://www.yynnyy.cn/6c02904f</id>
    <published>2023-09-29T11:06:44.000Z</published>
    <updated>2024-08-09T09:26:46.966Z</updated>
    
    <content type="html"><![CDATA[<p>今天第一次尝试将Arxiv最新论文同步到博客。</p><p>扫描Arxiv的工作现在基本每天都做，最开始可能还要追溯到两年多前。曾经用过各种各样的方式完成这件事：</p><ul><li>最开始是超哥带着大家每天扫描，每人按日期做分工</li><li>后面一段时间我自己每天刷一刷</li><li>后来形成习惯了，要写一个飞书文档同步进去，后来觉得太麻烦，最后就不了了之了</li></ul><p>从今天开始，试着每天把新扫描到的有趣的论文更新到博客，看看大家的反应如何。可能一个良性的循环是：一方面有人反馈我有遗漏，或者推荐哪篇论文，我就可以仔细看看，或者写一些阅读笔记。</p><span id="more"></span><p>我读论文、扫描Arxiv论文，按照优秀程度大致分为几个粒度：</p><ul><li>最差的是点都不会点进去</li><li>好一些的会点进去看看abstract和作者</li><li>再好点的我会放到Arxiv Insights里</li><li>再好点的我会加入 paper reading TODO list</li></ul><p>写论文阅读笔记是这样：</p><ul><li>读完论文，觉得很好玩的东西，我会加入到blog TODO list</li><li>每次有时间的时候，就会写一篇阅读笔记</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天第一次尝试将Arxiv最新论文同步到博客。&lt;/p&gt;
&lt;p&gt;扫描Arxiv的工作现在基本每天都做，最开始可能还要追溯到两年多前。曾经用过各种各样的方式完成这件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最开始是超哥带着大家每天扫描，每人按日期做分工&lt;/li&gt;
&lt;li&gt;后面一段时间我自己每天刷一刷&lt;/li&gt;
&lt;li&gt;后来形成习惯了，要写一个飞书文档同步进去，后来觉得太麻烦，最后就不了了之了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从今天开始，试着每天把新扫描到的有趣的论文更新到博客，看看大家的反应如何。可能一个良性的循环是：一方面有人反馈我有遗漏，或者推荐哪篇论文，我就可以仔细看看，或者写一些阅读笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="https://www.yynnyy.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>论文阅读[粗读]-TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS</title>
    <link href="https://www.yynnyy.cn/660d5dc5"/>
    <id>https://www.yynnyy.cn/660d5dc5</id>
    <published>2023-09-25T11:54:33.000Z</published>
    <updated>2024-08-09T09:26:46.974Z</updated>
    
    <content type="html"><![CDATA[<p>作者团队就是我们tool learning小组</p><img src="../../files/images/ToolLLM/authors.png"><h2 id="Introduction">Introduction</h2><p>我们在之前(4月)的一篇综述论文里探讨工具学习场景的一些任务的困难</p><blockquote><p><a href="https://arxiv.org/abs/2304.08354">Tool Learning With Foundation Models</a></p></blockquote><p>当时我们定义出来了两种典型的场景</p><ul><li>Tool Augmented: 较为简单的场景，工具是为了模型服务的。模型通过调用工具来增强语言对话能力。比如Toolformer通过计算器拿到精确数值计算结果，或者现在比较火的 retrieve-augmented LLM的概念(通过一个外置的知识库增强事实等能力)</li><li>Tool Oriented: 较为困难的场景，模型是为了工具服务的。将大模型作为工具的组织者，目标是为了完成c端用户的具体需求</li></ul><p>不管是哪种场景，在运行难度上都是 多工具&gt;单工具，多步调用&gt;单步调用。在之前的论文中，我们基本只是探讨了单工具的场景，发现 ChatGPT is all you need</p><img src="../../files/images/ToolLLM/single_tool.png" style="zoom:50%;" ><p>在本文，我们进一步拓展了应用场景，在多步、多工具的场景中探索了Tool Oriented任务的执行效果。</p><h3 id="RapidAPI">RapidAPI</h3><p>首先，我们找到了一个开放的RestAPI平台RapidAPI，他们包含大约50000个API，其中有很多类型的请求。我们只使用了他们的所有GET请求，剩下了大约16000个API</p><blockquote><p>这一个简单的sift背后其实有深刻的原因：我们的工具是不是有状态的？POST请求是会对真实世界产生影响的，也就是说，同样的请求可能会随着逻辑执行的顺序有区别。这在实际执行中会带来比较大的问题，最终我们把任务做了一定的化简，只采用了GET请求。</p></blockquote><p>其中，所有的这些API是按照层级进行组织的。最顶层有49个category,每个category下面含有多种多样的tool，每个tool包含一个或多个API。</p><p>据了解，这应该是第一篇把真实世界工具做到这个数量级的工作：</p><img src="../../files/images/ToolLLM/rapidAPI.png"><p>由此出发，我们首先通过self-instruct构造多种不同难度的query，然后用ReACT以及新版的DFS、ToT算法对query进行了标注，造出了(query-answer)数据集。再在数据集上进行训练Llama最终达到了接近ChatGPT的效果。工作流程如图所示：</p><img src="../../files/images/ToolLLM/intro.png"><p>我们开源了代码、模型权重、数据、评测平台(ToolEval)</p><h2 id="Method">Method</h2><p>从上面的讨论中，我们可以发现，其实我们要做的任务就从任务定义、到数据集、到评测都是全新的，都需要我们去创新。因此我们进一步把workflow划分为5块，也就是下面要讲的5个部分</p><h3 id="query-generation">query generation</h3><p>首先是，作为测试集的query如何构建？这个问题涉及到了：我们使用RapidAPI具体是想要打成什么目的？</p><blockquote><p>我们是为了帮助解决通用任务</p></blockquote><p>在query构造时，我们使用self-instruct的方式，给定工具和对应的描述，让模型去构造对应的query。对于工具的粒度不同，我们划分出了三个难度的query：</p><ul><li>G1:单工具。对着同一个工具下的所有API去拟合query</li><li>G2：对着同一个category下的不同工具下的所有API去给调用</li><li>G3：对着不同category下的不同工具下的所有API去给调用</li></ul><p>三种query的难度逐次增高，对应的工具选择能力需求也更强。</p><p>通用任务也有高下之分。有具体的任务&quot;帮我找找附近好吃的粤菜馆&quot;，或者”帮我赚10000美元“。从任务的角度理解，其实他们对于工具的需求是不一样的：</p><ul><li>具体的任务，可能在找到对口的工具时就已经完成了80%的工作，比如导航工具</li><li>抽象的任务，可能找到对口的工具只能完成10%的工作，比如在纳兹达克开户……</li></ul><p>因此，其实工具能力可以进一步划分出两个子能力：工具选择能力和工具使用能力</p><p>在query构造时，我们选用了self-instruct的方式，对着tool反向构造query，再让模型看着query选择到这个工具去执行。</p><h3 id="answer-generation">answer generation</h3><img src="../../files/images/ToolLLM/DFSDT.png"><p>再构造出query以后，下面就是如何进行answer的标注，我们的标注模型使用ChatGPT-turbo。其中，使用了他们的function接口。对于OpenAI-function接口来说，只要给出了输入的json-schema，模型就会自动选择调用是什么、以及输入的参数是什么。从这个意义上来看，其实function接口既能表达出工具选择能力，又能表达出工具调用能力。</p><h3 id="ReACT">ReACT</h3><p>经典的工具调用manner可以使用CoT，或者说开源工具包langchain类型的ReACT算法</p><blockquote><p><a href="https://github.com/langchain-ai/langchain">langchain</a></p><p><a href="https://arxiv.org/abs/2210.03629">Synergizing Reasoning and Acting in Language Models</a></p></blockquote><p>他的运行逻辑是：模型按照一个链式的模式一样调用工具，每一步先给出一个thought，再给出一个工具调用。一个openAI json描述如下图所示</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JSON"><figure class="iseeu highlight /json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Prefix&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://entreapi-faker.p.rapidapi.com/name/prefix&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Randomly generate a prefix (e.g., Mr. Mrs., etc.)&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;method&quot;</span><span class="punctuation">:</span> <span class="string">&quot;GET&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;required_parameters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;optional_parameters&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gender&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;STRING&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Optional gender.&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;default&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span> </span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;tool_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;EntreAPI Faker&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;category_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Data&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure></div><p>这里的所有description和json-schema是按照RapidAPI接口官网的信息转化成的，因此我们需要先把官网所有的描述都爬取下来再做一次clean。</p><h3 id="DFSDT">DFSDT</h3><p>从CoT出发，后来的有些工作探索了ToT、PoT、HoT、SoT之类的新方法，我们也采用了一个ToT的DFS算法，主要是想解决CoT的以下问题</p><ul><li>误差累积：对于CoT来说，因为是一直向前的manner，误差会逐渐累积。也就是说，如果某一步开始的工具调用失败了，后面其实会陷入到这种“陷阱”里，最终到达一种“持续进行同样的错误工具调用”的死循环里</li><li>探索范围，单次的CoT其实探索范围很窄，但是很多情况下需要多次的探索才能把任务完成</li></ul><p>所谓的ToT，其实就是把thought当成边，把tool当成点，然后把整个的探索过程组织成一颗树，进行树搜索。经典的树搜索有DFS和BFS两种，我们采用了DFS，主要是基于以下原因：</p><blockquote><p>我们希望算法尽可能快速的结束：对于简单的query(CoT就可以做出来的)，可以在类似CoT的执行时间内就做完。相比之下， BFS可能对于所有query都会在类似的时间内做完</p></blockquote><p>传统的DFS算法，需要在每一步的时候生成多个候选，然后选择一个最好的，但在我们的探索中，我们发现，其实模型投票的时候，大多数情况下会认为第一个候选是最好的，这是因为模型本身生成时是根据logits最大化来的，先生成的其实有一定的calibration性质。</p><p>因此，我们最终选择DFSDT，执行上是一个先序遍历的方式。前向时每次只给一个父节点生成一个子节点，到返回到该节点时才生成第二个节点。这种算法，在CoT时间内和CoT算法的行为是完全一致的，只是在CoT执行结束以后才会开始回退，做一些DFS的处理</p><img src="../../files/images/ToolLLM/ans_anno_performance.png"><p>我们在几个粒度上都尝试了DFSDT算法，发现基本都达到了远超CoT的效果</p><h3 id="evaluation-method">evaluation method</h3><p>之前一直提到了效果，但直到现在都没讲这个score到底是怎么算的。其实Eval想要达成几个目的：</p><ul><li>一个query是否是可解的。有些query由于给的信息错误，天生不可解决。比如&quot;帮我导航去西餐馆&quot;，没说是哪个西餐馆。或者说了一个根本不存在的地方</li><li>一个query是否被解决了</li><li>一个query的两种解决方案，到底哪个会更好</li></ul><p>因为这几个问题其实都是动态的，对于每个query的情况各不相同。我们的主要评判方法是模型评测，由此诞生了ToolEval</p><img src="../../files/images/ToolLLM/tooleval.png" style="zoom:50%;" ><p>我们是由人标注了一波数据，然后在上面评判拟合的prompt，用一个和人工具有最高一致性的方案。最终得到了一个相对比较稳定鲁棒的评测方法</p><h3 id="tool-retrieve-system">tool-retrieve system</h3><p>其实还有个重要的问题是，RapidAPI有16000个API，模型不可能在context里全部看到，我们必须帮模型去化简难度。由此我们就加了一个retrieve的模型，分别把query和API-doc编码成为向量。训练时由query自带的一些relative API作为正例，随机采样一些其他API作为负例来做对比学习，和Sentence-BERT一致。实际测试时就直接用embedding来相似度匹配，准确率还是很高的</p><p>作为Baseline方法，我们也对比了ada-embedding直接把所有的工具描述的ada embedding都搞出来算相似度、以及BM25经典方案。</p><img src="../../files/images/ToolLLM/retrieve.png" ><p>其实ada embedding直接做效果也挺好，但还是比不过我们寻模型的方法。</p><h3 id="model-training">model training</h3><p>最后，就是我们拿我们的生成出来的大约20万的query-answer对去训练Llama，使其掌握RapidAPI的能力。</p><img src="../../files/images/ToolLLM/result.png" ><p>另外，我们在训练时发现，其实效果达到最大值附近，需要用到的数据并不多。也许工具学习任务和RLHF任务类似，反而需要小而精的数据保证。</p><h2 id="我的思考">我的思考</h2><p>这篇论文可以说是之前一个开源项目的二期成果，之前我们做了一个BMTools工具包，手动实现了一些工具比如Google-Search等。后来我们发现，线性地去实现工具其实是很低效、很工程的事情。最好是可以把语言模型直接去对接到工业界、学术界已有的工具平台，最终找到了RapidAPI这个平台。</p><p>大模型使用工具是一个很大的话题，找到通用性的工具调用方法更是很难的课题。思考人学习工具的方法论，很多老师傅&quot;言传身教&quot;的所谓经验都是很难用语言去表示的。目前的预言模型主要是基于语言去训练，对于这种更加抽象的能力是否掌握仍是未知数。</p><p>我们在探索中也发现，对话场景下表现相似的ChatGPT、Claude、Llama2，到了工具场景，能力却变得天差地别。这种区别，一方面说明了工具能力也许是比语言能力更困难的能力，另一方面也许揭示出来：ChatGPT，尤其是ChatGPT-function，可能已经在这个领域从预训练模型的角度做出了一定的探索。</p>]]></content>
    
    
    <summary type="html">很久没更新了，今天来讲讲我们组最近发布的工作ToolLLM(ToolBench/ToolLlama)。看看在多步工具学习场景下，Llama用多少数据就能训练出ChatGPT的效果</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="tool-learning" scheme="https://www.yynnyy.cn/tags/tool-learning/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-Llama 2: Open Foundation and Fine-Tuned Chat Models(下)</title>
    <link href="https://www.yynnyy.cn/a3a406b2"/>
    <id>https://www.yynnyy.cn/a3a406b2</id>
    <published>2023-08-05T02:49:45.000Z</published>
    <updated>2024-08-09T09:26:46.975Z</updated>
    
    <content type="html"><![CDATA[<p>语言模型的RLHF(PPO)的基本流程的算法我们在 <a href="/5d2d4022.html" title="论文阅读[粗读]-强化学习和RLHF中的PPO算法">强化学习和RLHF中的PPO算法</a> 中介绍过了，主要分为三个阶段，下面我们来讲讲Llama 2分别做了哪些创新吧</p><span id="more"></span><h2 id="SFT">SFT</h2><h3 id="data-and-train">data and train</h3><p>首先，作者和之前的论文保持一致，认为SFT的训练数据数量不是问题，关键是质量。然而，作者还是收集了27650条instruction tuning的数据，emmm，这好像是我见过的最多的。</p><p>作者在180条数据上做了仔细检查，比对了human的结果和SFT model生成的结果，发现大致在一个水平。</p><blockquote><p>训练方式就是传统的crossentropy，然后作者只在assistant message上计算梯度</p></blockquote><h2 id="reward-model-training">reward model training</h2><p>首先作者需要human paired data。标注了大概100万条数据</p><p>标注过程和后面的过程是绑定的，就是先标注一波，然后做RLHF。下一波标注的时候pair data来自新模型。主要是因为human标注需要时间，以及更高效地利用数据</p><p>另外，作者为了让human永远有选项选，提供了四种偏序关系可以选择。</p><img src="../files/images/LLaMA2/margin.png" style="zoom:50%;" ><h3 id="margin">margin</h3><p>接下来，在训练RewardModel时，作者利用起了之前human标注的多种偏序关系，具体是这样<br>$$<br>\mathcal{L} = - \log(\frac{1}{1 + e^{-(r_\text{good} - r_\text{bad})}})<br>$$<br>这个loss是一个Bradley-Terry模型crossentropy loss的导出形式，希望好的数据的reward更高</p><p>如果已知human的偏序不止一种的话，就可以额外加一点偏差值<br>$$<br>\mathcal{L} = - \log(\frac{1}{1 + e^{-(r_\text{good} - r_\text{bad} - \text{margin})}})<br>$$<br>就是说，比如significant better，就希望差得更远，通过更大的gradient来保证。作者做了消融实验，看在测试集上reward model的分类acc，发现效果确实更好</p><img src="../files/images/LLaMA2/effect_of_margin.png" style="zoom:50%;" ><h3 id="performance">performance</h3><p>衡量reward model好坏的指标就是测试集分类acc，</p><img src="../files/images/LLaMA2/different_order.png" style="zoom:50%;" ><p>首先报告了在各种偏序关系下测试集上的reward model分类结果，发现区分还是挺明显的。基本上human认为显著区别的，模型都能区分。</p><p>接下来作者做了一个scale实验，发现用更大的模型、更多的训练数据可以训练出更准确reward模型。最重要的是：</p><blockquote><p>仍未观察到收敛现象。也就是说，还能接着提规模涨指标</p></blockquote><img src="../files/images/LLaMA2/scale_of_reward_model.png" style="zoom:50%;" ><h2 id="RL">RL</h2><p>最后的RL阶段。和前面提到的，由于human标注是一波一波来的，作者也就是一波波训的reward model和RL模型。称之为RLHF-v1到RLHF-v5</p><p>主体方法上，作者尝试两种方法</p><h3 id="rejection-sampling">rejection sampling</h3><p>这是一个看起来很稳定的算法，和之前讲到的 <a href="/feddc200.html" title="论文阅读[精读]-RRHF: Rank Responses to Align Language Models with Human Feedback without tears">RRHF阅读笔记</a> 非常像</p><p>每个iter中，先对每一个prompt采样K个样本，然后用打分模型打一波分，找到最好的样本，然后按照SFT的方式计算crossentropy loss。</p><p>为了训练的稳定，选取最好样本时是从前面的所有iter里，而不是当前iter里选最好样本(所以其实是K*iter选1)。其实就是退化版的RRHF，不过省下了非常多的训练资源</p><img src="../files/images/LLaMA2/avg_score.png" style="zoom:50%;" ><p>作者实验了一下，用最开始的SFT对应prompt多次sample样本，对应的最高score持续提升，这就是这个算法的优化空间。</p><p>同时，这个算法的效果和sample样本时选取的算法也是高度相关的，更高的diversity更可能找到更好的样本，但是数据少时表现更不稳定。</p><p>作者做了实验(注意左右两图的纵坐标不一样)，和上面一样看多次采样的最高reward。发现这个最佳温度是在变化的，因此作者是每个iter都分别找最佳温度，再sample。</p><img src="../files/images/LLaMA2/temperature.png" style="zoom:50%;" ><h3 id="PPO">PPO</h3><p>作者没怎么使用经典的PPO算法，可能是因为训练不稳定？</p><p>只在最后一次迭代，也就是RLHF-V5之前用了一下。</p><p>流程是这样：训出来RLHF-V4以后，先用PPO增强一波，得到V4-PPO，然后用V4-PPO做rejection sample训练，得出来RLHF-V5</p><h3 id="performance-2">performance</h3><img src="../files/images/LLaMA2/rlhf_method.png" style="zoom:50%;" ><p>作者首先报告了RLHF的平均水平，这个图坐标是指：战胜ChatGPT-0301的百分比。两个轴分别是帮助性和无害性数据的结果</p><blockquote><p>50%就是说和ChatGPT差不多</p></blockquote><p>可以看出来，效果不错，每轮都在加强。</p><img src="../files/images/LLaMA2/between_other.png" style="zoom:50%;" ><p>除了机器自评以外，作者在直接让human实验，每个模型的规模级别，都和同规模的类似模型进行了比较。可以看到</p><ul><li>基本都是赢了</li><li>不过在vicuna 33b上，似乎差不太多？</li><li>70b基本和chatGPT一样</li></ul><p>我非常好奇的是，有没有更详细的PPO和rejection sample的对比实验，比如PPO+SFT-V1，不过作者没有报告</p><h2 id="Ghost-attention-GAtt">Ghost attention(GAtt)</h2><p>最后还有一个创新点,就是封面那个图。在多轮对话中，模型经常在后面的轮数就忘了最前面的全局prompt</p><p>解决方法也很简单：作者把最前面的system message在每个user message前面都复制了一份。作者手动创造了一些好这么做的prompt比如“answer only with emojis”, “act as <em>Napoleon</em>”。然后在SFT query随机中掺杂上这些需求，然后让模型生成这些数据</p><p>作者把这件事情默默用在了iter train过程中，就是说在RL用的prompt里掺杂了这些东西，然后每轮对话都加，但是reward model训练没用上，所以不影响reward model的训练稳定性。</p><p>测试时没有这个trick，只在system prompt加一次，看能不能维持住</p><img src="../files/images/LLaMA2/gatt_performance.png" style="zoom:50%;" ><p>可以发现，不加GAtt，基本上两轮以后前面的prompt已经忘完了，加上以后就一直不忘。</p><p>另外，作者在附录里提到一个有趣的事情：</p><blockquote><p>ChatGPT对system prompt也很看重，如果不加system prompt，对比Llama2的win rate就会从44%暴跌到36%</p></blockquote><p>难道说，MOE真的重出江湖？</p><h2 id="我的思考">我的思考</h2><ul><li>RLHF部分的探索非常详尽，很久没看到做这么多实验的论文了。够solid</li><li>看Llama2的总体情况，scaling trend依旧存在，这就给更大更强的Llama3 留出空间了，不知道什么时候能推出</li></ul><blockquote><p>另外，听说OpenAI打算开源一个叫G3PO的模型，不知道啥时候出来…</p></blockquote><ul><li><p>里面提到的rejection sample非常新颖。其实最近有一些工作就是想用类似SFT的方式来做RL，其实本质上都是对Reward model的对抗训练，差不太多：PRO、RRHF等等。</p></li><li><p>这里我抛出来一个问题：既然RLHF是在显式地对抗reward model，那我们能不能在运行时进行对抗呢？</p></li></ul><blockquote><p>human preference虽然不会变，但是还有很多preference和下游绑定的任务。其实这就是LLM搜索类的算法：ToT里面引入了一些这种in-context对抗的特性，后面我们会进一步指出和解决这个问题。</p></blockquote><ul><li>类似RRHF和rejection sample，是在多个样本中learning from best of N，那么显然这个方法的效率取决于最优score寻找的过程，相同搜索资源下，最优score越高，用这类算法的效果就越好。那么，这是不是也预示着我们需要一个更好的搜索算法呢？</li><li>虽然叫&quot;开源&quot;，但其实Llama 2不是完全开源的……pretrain数据和SFT数据就没给</li></ul>]]></content>
    
    
    <summary type="html">今天继续讲，训练Llama2-Chat模型的方法和创新点</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="预训练模型" scheme="https://www.yynnyy.cn/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读[精读]-Llama 2: Open Foundation and Fine-Tuned Chat Models(上)</title>
    <link href="https://www.yynnyy.cn/6a73db61"/>
    <id>https://www.yynnyy.cn/6a73db61</id>
    <published>2023-07-31T04:07:38.000Z</published>
    <updated>2024-08-09T09:26:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>从影响力上，他是你应该第二个知道的大模型(第一个是GPT)，前两天刚被人用C重写了一遍；从评测结果上，其效果超过ChatGPT-0301</p><p>笔记比较长，因为我在讲解时把正文和附录揉在一起，力求讲清楚所有技术细节和创新点。</p><blockquote><p>除了safety的部分，我确实不懂这个方向……</p></blockquote><img src="../../files/images/LLaMA2/authors.png"><p>从论文里可以看到三个小细节：</p><p>首先在大小写上，作者把这个模型叫做&quot;Llama 2&quot;，但是在1代时描述是LLaMA，现在的开源社区都是用&quot;LLaMA&quot;来称呼，不知道后面大家会不会改</p><p>其次，这个作者团队延续了一代里的Meta，但是GenAI又是什么东西？</p><blockquote><p>GenAI其实也是Meta的部门，是小扎专门为生成式AI成立的研究所，这篇论文可以看做他们打响影响力的开山之作。</p></blockquote><p>最后，LLaMA一代里的元老已经没了，这是被优化掉了？</p><img src="../../files/images/LLaMA2/llama_1_author.png"><p>仔细看，他们还在附录里，但作者列表直接查无此人，这就是大公司内部的斗争吗……</p><blockquote><p>ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philom- ena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organiza- tion support.</p></blockquote><p>作者一共训练了7B, 13B,34B, 70B四个版本的LLaMA2，其中34B因为一些安全性问题暂时没有发布，其他的模型都是商业许可，填申请就能下载</p><h2 id="预训练阶段">预训练阶段</h2><p>所有版本的模型都训练了2T的token量</p><p>tokenizer和一代一样，都是vocab_size=32k的BPE模型。数字单独用token，然后不能解码的UTF-8一个byte一个</p><p>值得一提的是，这次dataset不开源了，屠龙者终成恶龙？</p><img src="../../files/images/LLaMA2/data.png"><p>作者在两个超算平台上训练，用了大概2000GPU，训练了3.3M A100-80GB hour的训练量完成了训练……</p><img src="../../files/images/LLaMA2/hours.png"><table><tr>  <td><img src="../files/images/LLaMA2/loss1.png" align=left style="zoom:35%;" ></td>  <td><img src="../files/images/LLaMA2/loss.png" align=left style="zoom:25%;" ></td> </tr>    <tr>    <td>llama 1 loss</td>  <td>llama 2 loss</td>  </tr></table><p>另外，对比一代和二代的训练loss</p><ul><li>大概可以发现在训练相同的token 1.4T时，两者的loss是差不多的，这大概说明预训练数据的难度也差不多？</li><li>以及2代在训练的时候尖峰少了很多，大概是代码做了优化，或者小扎自己搭的超算平台真有独到之处？</li></ul><p>从loss曲线上来看，其实训了2T token之后模型仍然没有表现出收敛的趋势，但是提前停止了训练，</p><blockquote><p>If you have money, you can continue……</p></blockquote><p>作者没有纰漏预训练数据集大概多大，不过Llama1应该是训了一个epoch，只有Book和Wiki数据训了两个epoch。</p><blockquote><p>另外，之前有个GPT4黑客爆料说用13Ttoken的数据集训了1个epoch，花了66 million $。不知道Llama 2花了多少钱</p></blockquote><h3 id="sequence-length">sequence_length</h3><p>把sequence-length提升到了4k。</p><blockquote><p>值得一题的是，最近的新方法可以把2k的LLM通过finetune基本无伤拓展到16k、32k量级，参考笔记： <a href="/abce9d24.html" title="论文阅读[粗读]-Extending Context Window of Large Language Models via Position Interpolation">ROPE长度内插延拓</a></p></blockquote><img src="../../files/images/LLaMA2/seq_length.png" style="zoom:35%;" ><p>作者通过150B token训练量的对比实验，发现训练4k length带来了更好的效果</p><h3 id="group-attention">group-attention</h3><p>作者在34B和70B的模型上使用了group-attention技术。这个技术是指，正常的attention要在每个head分别用不同K Q V矩阵把hidden state的某个部分做变换。为了节省参数，可以然后K V变换阵共享参数，作者使用的版本是8个Q阵对应一组K V阵的参数</p><img src="../../files/images/LLaMA2/GQA.png" style="zoom:50%;" ><p>作者通过相同的实验，发现GQA确实不咋影响效果</p><p>MHA是multi-head attention就是原始版本，很好理解。但是表格里面的MQA又是什么东西？</p><p>其实MQA是group attention的一种特殊情况，就是所有head的key value是相同的，区别只有Q</p><blockquote><p>GQA: Training generalized multi-query transformer models from multi-head checkpoints</p></blockquote><h3 id="预训练评测">预训练评测</h3><p>接下来，作者评测了预训练模型的效果</p><p>首先作者和一些开源的模型做了比较</p><img src="../../files/images/LLaMA2/compare_wtih_open_source.png" style="zoom:50%;" ><p>这里面可以看出，Llama 2比Llama 1好很多</p><ul><li>在数学能力上，小模型的数学能力基本翻了好几倍。</li><li>在代码能力上，70B模型暴增7个点</li></ul><p>基本可以认为，在任何场景下，Llama 2都是现在最强的开源模型</p><p>接下来，作者还和闭源模型做了比较</p><img src="../../files/images/LLaMA2/compare_with_close_source.png" style="zoom:50%;" ><p>这次发现，LLama 2的基础能力基本上是最差的……好消息是，和GPT-3.5差不多？</p><p>除了代码能力(humanEval)，看起来Llama2的代码能力确实不行</p><blockquote><p>这里面的GPT-3.5的效果，应该用的是GPT4技术报告里的结果，对应的模型是text-davinci-003。如果和001 002比，可能Llama 2要更强一点</p><p>另外，GPT4这个GSM8K直接干到92也太猛了……</p></blockquote><p>其实作者做的每一个数值，都是一堆实验的，比如上面说的数学能力，就是下面这两个数学题数据集的平均数</p><img src="../../files/images/LLaMA2/math.png" style="zoom:50%;" ><p>其中这个&quot;MATH&quot;，就是之前我们讲 <a href="/d074522f.html" title="论文阅读[精读]-Let’s Verify Step by Step">Let’s-Verify-Step-by-Step</a>时使用的那个测试数据集</p><blockquote><p>当时打到了78%</p></blockquote><p>如果只想了解预训练的细节，那么到这里就足够了，下篇我将会重点讲解RLHF的部分，也是占据篇幅最多的部分</p>]]></content>
    
    
    <summary type="html">一直等李沐老师的视频没等到，那今天我就来为大家讲讲目前最强的开源模型：LLaMA 2。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://www.yynnyy.cn/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="计算机" scheme="https://www.yynnyy.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="人工智能" scheme="https://www.yynnyy.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="预训练模型" scheme="https://www.yynnyy.cn/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
</feed>
