---
title: 2023-11-08-insights
tags:
  - English
categories: Arxiv-Insights
hidden: true
total: 59
interesting: 1
abbrlink: 8cc1681c
date: 2023-11-08 11:37:40
---

感觉没啥好玩的，不过还是找到了一篇看起来不错的

## [Quantifying Uncertainty in Natural Language Explanations of Large Language Models](https://arxiv.org/pdf/2311.03533.pdf)

哈佛大学的论文，稀客呀。作者思考了这个问题：最近的CoT工作可以让模型引出自己的思考，以及关键的中间token。那么如何判断这些思考是可信的呢？本篇工作提出了两个metric: *Verbalized Uncertainty*，*Probing Uncertainty*来衡量模型自己的不确定度。作者发现这个metric和真实的faithfulness相关性很高。

挺好玩的问题，不一定是最好玩的解法
