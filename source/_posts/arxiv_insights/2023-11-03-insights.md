---
title: 2023-11-03-insights
tags:
  - English
categories: Arxiv-Insights
hidden: true
total: 46
interesting: 1
abbrlink: 742193eb
date: 2023-11-03 13:28:27
---



## [Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://arxiv.org/pdf/2311.00871.pdf)

deepmind出品，我很喜欢的一篇论文。作者思考了in context learning(ICL)这种不需要数据训练就能直接跟根据前缀的task描述就完成任务的现象。作者因此直接在(x,f(x))数据集上做实验，发现对于训练集出现的不同的x，模型总能得到几乎完美的答案。但是，如果f是一个OOD的function，模型一下就炸了。这一定程度解释了ICL的能力来自于训练集。
