---
title: '论文阅读[粗读]-MVP: Multi-task Supervised Pre-training for Natural Language Generation'
tags:
  - 计算机
  - 人工智能
  - 预训练模型
categories: 论文阅读笔记
description: 有监督预训练，在绝大多数任务上击败基线BART，在11个数据集达到SOTA
mathjax: true
abbrlink: 1256e99b
date: 2022-07-01 21:21:52
---

​	今天看了个又臭又长的survey，回到老本行看个好玩的论文洗洗脑。这篇论文是人大做的，一作是个硕一的学生，看看人家w。这篇文章说的贡献是：拓展了有监督预训练到NLG生成任务中，并且击败了基线BART，同时在多个数据集达到了SOTA。

​	正好前两天刚看了GPT3，这个感觉有点“碰瓷”的意思，来读一读。

## Introduction

主要就是表示，本文探索两个问题：

- 有监督预训练如何解决NLG问题
- 有监督预训练能不能繁华到更多任务上



## 有监督预训练？

大家都知道无监督预训练就是收集一些人类语料$Y = x_1,x_2,...x_N$，做一些自回归任务，比如GPT：
$$
\log P(x_1,x_2...x_N) = \log p(x0) \sum_{i=1}^N \log(x_i | x_{i-1})
$$
​	做这种马尔科夫链的自回归，然后期望随着PPL的降低，模型可以“学会怎么说话”，生成流畅的、符合人类说话习惯的语料。对于理解模型，就是可以“理解语句的含义”。

​	所谓有监督预训练，就是说提前收集一些已有任务的数据集。数据集的特点是有标注，比如摘要任务是$(Y,summary)$。如果收集了大量的、来自不同预训练任务的有标注语料的话，就可以定义一种输入方式，可以把不同的语料和标注一起喂到模型里，继续进行auto-regressive的训练。

​	具体到本模型MVP，就是比如 summerization：$(Y,X)$，输入是
$$
summary: \quad Y \quad[sep] \quad X
$$
其他任务也是有一些前缀。

具体来说，作者归纳了4大类NLG任务，收集了一共60GB的有监督语料，对比：

- GPT用570GB无监督语料
- BART用了160GB无监督语料

## 模型结构

​	模型结构是什么呢？作者说和BART是一样的。不仅如此，作者甚至是从BART-large初始化了权重。然后用60GB有监督语料接着训练。

​	说实话，我觉得”用BART初始化“是这个有监督预训练成功的关键因素……

## 评测

作者提到了两种方式：

- full tuning：就是MVP拿过来，做fine-tune
- Parameter-Efficient Tuning Performance：把backbone模型冻结，然后加上soft-prompt进行训练

作者进行了很多任务的评测，结论是：比BART好。然后和SOTA比，也有提升。

重点是作者还做了一个泛化测试：

- 用训练时没见过的paraphrase、style transfer任务做测试，效果也比SOTA好。

## 我的评价

- 我在想一个问题：他找的60GB有监督语料里面有没有后面测试用的数据集呢？
  - 作者说跳过了一些训练任务，专门来评测。但据我所知，很多生成任务的数据集构建都是取材于别的生成任务，也就是说：即使没有标签重合，但很可能有数据重合
  - 好吧无监督预训练也被喷这个事……不追究这个了

- 比BART是不是显而易见的？
  - 用BART参数初始化，当然得被BART好了…不然不是负优化
  - 最后战胜SOTA是很好，但这个是”有监督预训练“的功效吗？我感觉这个方法更像是”大号的fine-tune“
- 不过本篇工作有一个很重要的点：**大规模的搞数据集上的auto-regressive也是有用的**。之前的fine-tune都是对输出算loss，现在这个相当于把loss传播到输入也算。
- 泛化性能为什么也有提升？感觉这个是后面探索空间最大的一个点。我个人观点：
  - 用”超越SOTA“作为”泛化性能强“是否有点不公平。对别的模型来说所有任务都是”泛化“。我觉得更合理的s实验是”新任务中MVP战胜SOTA的幅度和老任务一样“
